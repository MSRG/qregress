/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:04 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:07 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:12 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 17:39:51 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 17:42:03 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 827.6354489326477 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:44:52 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:53 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:57 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:38 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:51 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.2259800434113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:38 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:40 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:46 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:24 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:36 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.2216897010803 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:25 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:15:26 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:12 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:25 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 828.6218891143799 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:13 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:13 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:18 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:56 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:10 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 823.5460135936737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:56 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:57 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:02 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:48:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:58 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 830.6583821773529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:47 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:48 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:54 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:37 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:51 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 831.336578130722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:37 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:41 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:47 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:26 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:18:37 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 828.7501509189606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:28 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:31 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:35 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:13 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:32:28 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 827.8294973373413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:35:16 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:16 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:20 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:59 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:11 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.8165955543518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:58 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:59 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:55:03 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:55 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.5391607284546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:45 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:43 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:48 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:11:28 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:13:41 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 823.8580350875854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:28 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:28 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:22:34 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:12 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:27 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 825.3323702812195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:12 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:33:15 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:36:22 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:00 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:13 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.7564208507538 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:00 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:59 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:50:02 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:54:55 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 821.7492115497589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:57:42 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:43 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:03:50 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:06:27 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:08:39 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 824.9413862228394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:27 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:29 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:32 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:20:11 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:26 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 825.004367351532 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:12 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:12 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:17 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:57 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:11 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 827.6816818714142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:00 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:42:00 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:45:05 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:47:44 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:56 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.8117198944092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:52:42 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:55:43 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:58:46 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:01:25 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:37 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 821.3849720954895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:24 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:25 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:28 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:15:07 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:17:19 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 821.0131211280823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:20:05 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:23:05 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:26:08 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:45 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:59 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 819.0011327266693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:45 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:44 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:48 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:27 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:39 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.6375329494476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:26 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:50:26 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:30 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:08 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:58:20 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.5604796409607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:09 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:09 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:13 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:52 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:06 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 825.2220931053162 seconds. 
Discarding model... 

Training complete taking 20630.137446403503 total seconds. 
Now scoring model... 
Scoring complete taking 0.7564647197723389 seconds. 
Saved predicted values as IQP_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (88.55756300920861,), 'R2_train': 0.5709683285482292, 'MAE_train': 6.822966051777153, 'MSE_test': 102.85588144281007, 'R2_test': 0.3800038871095788, 'MAE_test': 6.734792390087033}. 
Saved model results as IQP_HWE-CNOT_results.json. 
