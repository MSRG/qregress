/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:04 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:07 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:12 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 17:39:51 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 17:42:03 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 827.6354489326477 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:44:52 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:53 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:57 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:38 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:51 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.2259800434113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:38 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:40 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:46 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:24 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:36 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.2216897010803 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:25 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:15:26 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:12 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:25 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 828.6218891143799 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:13 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:13 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:18 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:56 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:10 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 823.5460135936737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:56 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:57 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:02 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 18:48:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:58 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 830.6583821773529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:47 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:48 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:54 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:37 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:51 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 831.336578130722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:37 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:41 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:47 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:26 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:18:37 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 828.7501509189606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:28 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:31 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:35 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:13 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:32:28 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 827.8294973373413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:35:16 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:16 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:20 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:59 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:11 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.8165955543518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:58 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:59 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 19:55:03 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:55 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.5391607284546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:45 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:43 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:48 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:11:28 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:13:41 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 823.8580350875854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:28 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:28 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:22:34 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:12 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:27 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 825.3323702812195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:12 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:33:15 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:36:22 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:00 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:41:13 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 826.7564208507538 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:00 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:59 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 20:50:02 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 20:54:55 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 821.7492115497589 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:57:42 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:43 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:03:50 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:06:27 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:08:39 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 824.9413862228394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:27 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:29 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:32 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:20:11 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:26 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 825.004367351532 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:12 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:12 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:17 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:57 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:11 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 827.6816818714142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:00 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:42:00 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:45:05 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 21:47:44 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:56 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.8117198944092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:52:42 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 21:55:43 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 21:58:46 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:01:25 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:37 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 821.3849720954895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:24 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:25 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:28 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:15:07 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:17:19 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 821.0131211280823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:20:05 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:23:05 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:26:08 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:45 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:59 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 819.0011327266693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:45 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:44 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:48 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:27 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:39 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.6375329494476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:26 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 22:50:26 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:30 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:08 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 22:58:20 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 822.5604796409607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:09 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:09 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:13 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:52 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Sun Mar 24 23:12:06 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 825.2220931053162 seconds. 
Discarding model... 

Training complete taking 20630.137446403503 total seconds. 
Now scoring model... 
Scoring complete taking 0.7564647197723389 seconds. 
Saved predicted values as IQP_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (88.55756300920861,), 'R2_train': 0.5709683285482292, 'MAE_train': 6.822966051777153, 'MSE_test': 102.85588144281007, 'R2_test': 0.3800038871095788, 'MAE_test': 6.734792390087033}. 
Saved model results as IQP_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:52:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:53:16 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 11:56:13 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 11:59:14 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 12:01:52 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 12:04:02 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 807.5170359611511 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:06:44 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 12:09:39 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:40 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 12:15:14 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 12:17:24 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 801.9981017112732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:20:05 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:01 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 12:26:00 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:33 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:43 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 798.4090220928192 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:24 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:18 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 12:39:17 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:52 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 12:44:00 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 798.9148824214935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:46:44 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:41 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:39 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 12:55:13 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:21 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 799.5665459632874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:02 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 13:02:57 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 13:05:57 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 13:08:29 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 13:10:39 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 798.1557364463806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:13:20 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 13:16:14 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 13:19:11 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 13:21:46 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:54 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 795.0531394481659 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:35 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 13:29:30 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:30 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 13:35:05 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:14 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 800.820062160492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:39:57 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 13:42:51 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 13:45:51 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 13:48:25 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 13:50:34 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 801.1727802753448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:53:17 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 13:56:12 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:11 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:47 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 14:03:58 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 801.7316472530365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:06:39 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 14:09:35 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 14:12:35 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 14:15:09 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 14:17:19 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 800.3481848239899 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:20:01 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 14:22:56 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:56 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 14:28:29 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 14:30:39 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 802.4746561050415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:33:22 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 14:36:17 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 14:39:17 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 14:41:51 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 14:44:02 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 801.5696003437042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:46:45 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 14:49:40 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 14:52:38 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 14:55:14 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 14:57:22 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 800.3779766559601 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:00:05 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 15:03:00 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 15:05:59 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 15:08:35 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 15:10:45 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 803.3941237926483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:13:27 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 15:16:22 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:20 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:54 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 15:24:03 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 797.6035850048065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:46 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 15:29:39 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 15:32:39 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 15:35:13 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 15:37:22 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 798.2704985141754 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:40:04 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 15:42:59 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 15:46:01 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 15:48:35 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 15:50:43 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 803.9933273792267 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:53:27 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 15:56:22 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 15:59:20 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 16:01:54 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 16:04:05 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 800.6053009033203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:06:49 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 16:09:45 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 16:12:45 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 16:15:21 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 16:17:30 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 802.2908256053925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:20:11 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 16:23:05 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 16:26:04 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 16:28:37 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 16:30:45 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 798.6951947212219 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:33:29 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 16:36:25 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 16:39:24 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 16:41:58 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 16:44:08 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 802.1617286205292 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:46:52 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 16:49:46 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:47 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 16:55:21 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 16:57:29 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 800.4371874332428 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:00:12 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 17:03:09 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 17:06:08 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 17:08:42 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 17:10:50 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 801.1817655563354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:13:32 2024]  Iteration number: 0 with current cost as 0.27861211454402557 and parameters 
[-2.90318357  2.23743452 -2.12427975  0.68916015  0.03730416 -2.64288372
  3.0493796   2.2948203   1.15870666 -0.97947363  0.49740354  1.21701765
  0.98802034 -1.85288034  1.88457573]. 
Working on 0.4 fold... 
[Thu Apr  4 17:16:29 2024]  Iteration number: 0 with current cost as 0.19570467463141267 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.53557643  0.21247785 -2.54536919
  3.04289727  2.27586665  1.18067664 -0.95931039  0.57569892  1.26597327
  0.9764814  -1.83343259  1.99289985]. 
Working on 0.6 fold... 
[Thu Apr  4 17:19:28 2024]  Iteration number: 0 with current cost as 0.22163888591957953 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.61855871  0.15750945 -2.53506425
  3.04410309  2.27653075  1.1782195  -0.98410154  0.57310958  1.23506997
  0.99875044 -1.83815819  1.90129049]. 
Working on 0.8 fold... 
[Thu Apr  4 17:22:04 2024]  Iteration number: 0 with current cost as 0.21036674104596353 and parameters 
[-2.90318345  2.23743452 -2.12427975  0.63828597  0.19968127 -2.44491877
  3.04274396  2.25802028  1.19110437 -0.93761277  0.6068674   1.30205674
  1.00212128 -1.8283847   1.92497343]. 
Working on 1.0 fold... 
[Thu Apr  4 17:24:14 2024]  Iteration number: 0 with current cost as 0.2264923117528938 and parameters 
[-2.90318345  2.23743464 -2.12427975  0.58882991  0.14629324 -2.58782572
  3.04541245  2.28391688  1.17175569 -0.97767304  0.54697255  1.23470155
  0.98426302 -1.84167175  1.93730373]. 
Training complete taking 804.2894470691681 seconds. 
Discarding model... 

Training complete taking 20021.034554719925 total seconds. 
Now scoring model... 
Scoring complete taking 0.7628130912780762 seconds. 
Saved predicted values as IQP_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (88.55756300920861,), 'R2_train': 0.5709683285482292, 'MAE_train': 6.822966051777153, 'MSE_test': 102.85588144281007, 'R2_test': 0.3800038871095788, 'MAE_test': 6.734792390087033}. 
Saved model results as IQP_HWE-CNOT_results.json. 
