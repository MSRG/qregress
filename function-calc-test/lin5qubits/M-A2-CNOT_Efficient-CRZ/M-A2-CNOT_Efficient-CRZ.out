/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:25 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:50 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:39 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:12 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:27 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:18 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1128.0765450000763 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:38 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:38 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:15 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:39 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:43 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1166.556853055954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:10 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:19 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:06 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:40:25 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:35 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1217.4010655879974 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:00 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:42 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:21 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:21 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1225.7999691963196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:02 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:18 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:57 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:17 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:26 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1215.8580060005188 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:16 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:22 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:18 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:50 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:57 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1226.266944885254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:50 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:05 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:49 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:11 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:13 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1210.8546657562256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:09:49 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:54 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:18:22 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:21:49 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:54 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1182.8942532539368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:29:28 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:23 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:07 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:41:29 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:31 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1169.3535282611847 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:59 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:54:48 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:23 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:45 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:03:34 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1146.6415627002716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:08:01 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:48 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:16:14 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:19:21 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:13 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1106.235135793686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:24 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:03 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:34:33 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:37:47 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:40:36 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1106.4477758407593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:53 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:29 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:56 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:00 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:48 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1089.6917762756348 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:00 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:33 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:10:58 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:14:07 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 22:16:58 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1110.715146780014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:21:31 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:27:15 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:55 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:33:15 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 22:36:13 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1138.6422622203827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:46:11 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:48:37 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:51 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 22:54:41 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1103.1231644153595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:52 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:42 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:20 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:10:38 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 23:13:23 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1124.2098126411438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:17:37 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:35 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:26:02 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:16 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 23:32:23 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1142.648643732071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:36:49 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:42:37 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:45:08 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:21 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:11 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1121.5158083438873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:19 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:11 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:41 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:06:55 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 00:09:42 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1111.1785802841187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:13:59 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:19:43 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:22:11 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:25:27 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 00:28:15 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1126.9672920703888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:32:39 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:14 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:40:41 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:44:02 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:59 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1105.6167283058167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:51:08 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:56:46 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:59:18 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:02:31 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 01:05:20 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1108.1992218494415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:09:33 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:15:08 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:17:40 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:50 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 01:23:36 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1097.584053516388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:27:49 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:33:21 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:35:51 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:38:54 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:50 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1095.3398733139038 seconds. 
Discarding model... 

Training complete taking 28577.81978201866 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2593677043914795 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184541613,), 'R2_train': 0.2834338374757326, 'MAE_train': 10.976254799553638, 'MSE_test': 167.36739709607315, 'R2_test': -0.00885952430293968, 'MAE_test': 10.950798115672914}. 
Saved model results as M-A2-CNOT_Efficient-CRZ_results.json. 
