/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:25 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:50 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:39 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:12 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:27 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:18 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1128.0765450000763 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:38 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:38 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:15 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:39 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:43 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1166.556853055954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:10 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:19 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:06 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:40:25 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:35 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1217.4010655879974 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:00 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:42 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:01:21 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:21 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1225.7999691963196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:02 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:15:18 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:57 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:17 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:26 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1215.8580060005188 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:16 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:22 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:18 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:50 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:57 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1226.266944885254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:50 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:05 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:49 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:11 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:13 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1210.8546657562256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:09:49 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:54 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:18:22 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:21:49 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:54 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1182.8942532539368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:29:28 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:23 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:07 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:41:29 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:31 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1169.3535282611847 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:59 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:54:48 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:23 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:00:45 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:03:34 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1146.6415627002716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:08:01 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:48 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:16:14 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:19:21 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:13 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1106.235135793686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:24 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:03 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:34:33 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:37:47 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:40:36 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1106.4477758407593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:53 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:29 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:56 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:00 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:48 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1089.6917762756348 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:00 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:33 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:10:58 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:14:07 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 22:16:58 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1110.715146780014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:21:31 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:27:15 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:55 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:33:15 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 22:36:13 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1138.6422622203827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:40:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:46:11 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:48:37 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:51 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 22:54:41 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1103.1231644153595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:58:52 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:42 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:20 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:10:38 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 23:13:23 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1124.2098126411438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:17:37 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:35 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:26:02 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:16 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 23:32:23 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1142.648643732071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:36:49 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:42:37 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:45:08 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:48:21 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Sun Mar 24 23:51:11 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1121.5158083438873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:19 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:11 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:41 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:06:55 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 00:09:42 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1111.1785802841187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:13:59 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:19:43 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:22:11 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:25:27 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 00:28:15 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1126.9672920703888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:32:39 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:14 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:40:41 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:44:02 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 00:46:59 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1105.6167283058167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:51:08 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:56:46 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:59:18 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:02:31 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 01:05:20 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1108.1992218494415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:09:33 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:15:08 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:17:40 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:20:50 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 01:23:36 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1097.584053516388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:27:49 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:33:21 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:35:51 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:38:54 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:50 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1095.3398733139038 seconds. 
Discarding model... 

Training complete taking 28577.81978201866 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2593677043914795 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184541613,), 'R2_train': 0.2834338374757326, 'MAE_train': 10.976254799553638, 'MSE_test': 167.36739709607315, 'R2_test': -0.00885952430293968, 'MAE_test': 10.950798115672914}. 
Saved model results as M-A2-CNOT_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:22:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:23:30 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:00 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:27 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:34:32 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 12:37:18 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1077.5218930244446 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:41:27 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:47:00 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:49:29 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:52:38 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:19 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1087.0670228004456 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:28 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:12 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:07:40 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:10:59 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 13:13:46 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1107.3249266147614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:18:06 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:23:52 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:23 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:39 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 13:32:31 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1120.9226648807526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:39 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:42:13 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:42 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:48:10 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 13:51:07 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1125.101270198822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:55:28 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:01:04 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:03:35 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:06:43 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:27 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1090.2289090156555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:13:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:19:19 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:44 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:57 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 14:27:41 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1093.3440582752228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:31:45 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:37:15 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:39:47 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:43:02 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 14:46:00 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1102.5909113883972 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:50:08 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:55:45 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:58:11 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:01:34 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 15:04:28 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1109.683742761612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:08:41 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:14:44 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:17:17 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:20:30 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 15:23:19 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1128.2682864665985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:27:28 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:33:21 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:35:48 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:39:03 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 15:41:46 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1119.167890548706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:46:10 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:51:47 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:54:13 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:57:25 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 16:00:14 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1098.4534058570862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:04:33 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:10:02 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:12:39 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:15:55 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 16:18:43 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1113.4497365951538 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:23:06 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:28:42 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:31:06 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:16 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 16:37:05 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1096.8407883644104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:41:15 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:46:48 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:49:20 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:52:42 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 16:55:26 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1104.3598709106445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:59:45 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:05:24 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:07:47 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:55 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:40 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1095.6546957492828 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:18:03 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:23:35 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:26:03 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:29:12 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 17:31:54 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1090.762609243393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:36:09 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:41:41 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:44:11 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:47:18 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 17:50:09 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1096.3498632907867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:54:27 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:00:05 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:02:35 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:05:42 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 18:08:32 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1105.0139663219452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:12:48 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:18:22 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:20:45 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:23:50 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 18:26:34 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1081.8774042129517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:30:53 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:36:28 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:38:53 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:42:00 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 18:44:53 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1098.5231466293335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:49:12 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:54:44 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:57:11 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:00:16 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 19:03:06 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1088.3432657718658 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:07:19 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:12:58 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:15:27 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:18:36 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 19:21:30 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1106.8640472888947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:25:42 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:31:14 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:33:38 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:36:46 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 19:39:31 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1077.1797909736633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:43:40 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308  0.6027151   1.14432461
  1.31029914 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:49:24 2024]  Iteration number: 0 with current cost as 0.232255101784782 and parameters 
[ 1.31234093  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010938
  3.06858498  2.18960125  1.18551998 -1.06648349  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:51:48 2024]  Iteration number: 0 with current cost as 0.4080225480846814 and parameters 
[-1.00216455  2.23743451 -2.12427951 -0.11653128  0.55388708 -2.77010922
  3.06858473  2.18960145  1.18551998 -1.06648334  0.6027151   1.14432433
  1.31029886 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:54:53 2024]  Iteration number: 0 with current cost as 0.4674262856853504 and parameters 
[-0.3076348   2.23743432 -2.12427964 -0.11653134  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029867 -1.87354711]. 
Working on 1.0 fold... 
[Thu Apr  4 19:57:43 2024]  Iteration number: 0 with current cost as 0.4897581184010036 and parameters 
[ 0.07572621  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468 ]. 
Training complete taking 1092.1324684619904 seconds. 
Discarding model... 

Training complete taking 27507.02743792534 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2335116863250732 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184541613,), 'R2_train': 0.2834338374757326, 'MAE_train': 10.976254799553638, 'MSE_test': 167.36739709607315, 'R2_test': -0.00885952430293968, 'MAE_test': 10.950798115672914}. 
Saved model results as M-A2-CNOT_Efficient-CRZ_results.json. 
