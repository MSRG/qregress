/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:22 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:38:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:39:12 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:06 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.5345809459686 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:41:29 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:43:21 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:10 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:12 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:07 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.2266061306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:47:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:21 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:09 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:07 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.73306369781494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:31 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:12 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:14 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:08 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.3905420303345 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:32 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:25 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:14 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:17 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:11 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.3639407157898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:35 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:27 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:16 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:18 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:10:14 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 362.1480858325958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:38 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:30 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:20 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:24 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:22 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 367.6932680606842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:46 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:37 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:27 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:30 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:27 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 365.8974850177765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:23:51 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:41 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:31 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:33 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:29 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.6444981098175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:52 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:46 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:35 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:37 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:34 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 365.44602489471436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:57 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:49 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:39 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.7965774536133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:42:05 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:57 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:47 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:50 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:45 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 363.1996204853058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:59 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:51 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:56 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:52 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 366.3640820980072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:13 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:02 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:50 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:51 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:46 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 354.42508578300476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:01 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:50 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:03:51 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:46 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.05241441726685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:06:08 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:07:58 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:46 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:48 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:42 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.0770444869995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:05 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:56 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:43 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:46 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:41 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.7467942237854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:04 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:55 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:42 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:47 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:42 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.43423080444336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:24:03 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:52 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:39 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:27:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:28:36 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 355.8958623409271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:59 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:31:50 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:32:38 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:41 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:37 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 361.35468435287476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:02 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:53 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:41 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:42 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:37 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.06744289398193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:00 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:50 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:37 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:38 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:33 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 356.4581971168518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:47:56 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:45 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:34 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:37 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:33 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 358.41599202156067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:54 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:45 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:56:34 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:36 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:58:31 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.3758029937744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:59:54 2024]  Iteration number: 0 with current cost as 0.44731675517515807 and parameters 
[-1.26342681  2.23743432 -2.12427964 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029899 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:44 2024]  Iteration number: 0 with current cost as 0.23225510177981423 and parameters 
[ 1.31234095  2.23743443 -2.12427943 -0.11653103  0.55388688 -2.77010938
  3.06858478  2.18960165  1.18552019 -1.06648329  0.6027149   1.14432445
  1.31029878 -1.8735466   0.7296504   2.88578399 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:33 2024]  Iteration number: 0 with current cost as 0.40802254751369477 and parameters 
[-1.00216455  2.23743464 -2.12427951 -0.1165309   0.55388708 -2.7701091
  3.06858486  2.18960145  1.18552011 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354667  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:36 2024]  Iteration number: 0 with current cost as 0.4674262952273419 and parameters 
[-0.30763464  2.23743432 -2.12427964 -0.11653118  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029883 -1.8735468   0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:04:31 2024]  Iteration number: 0 with current cost as 0.4897581184010037 and parameters 
[ 0.07572621  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.18960145  1.18552014 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 359.7261519432068 seconds. 
Discarding model... 

Training complete taking 9023.468923330307 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9326620101928711 seconds. 
Saved predicted values as M-A2-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (147.9083184644662,), 'R2_train': 0.28343383742580874, 'MAE_train': 10.976254785918496, 'MSE_test': 167.3673969668236, 'R2_test': -0.008859523523847557, 'MAE_test': 10.95079813178329}. 
Saved model results as M-A2-CNOT_Modified-Pauli-CRZ_results.json. 
