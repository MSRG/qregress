/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:09 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:25 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:12 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:18 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:18 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1297.6458127498627 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:45 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:53 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:40 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:50 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:53 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1294.5651540756226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:19 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:31 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:25:19 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:34 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:39 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1301.797666311264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:03 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:12 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:53 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:09 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:23 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1300.1921184062958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:43 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:49 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:32 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:42 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:38 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1279.1301736831665 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:00 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:14 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:25 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:36 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:33 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1320.3501434326172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:01 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:09 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:49 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:51 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1272.214033126831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:14 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:22 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:04 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:11 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:08 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1296.7322447299957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:25:51 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:03 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:54 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:03 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:43:24 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1331.631958246231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:01 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:16 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:12 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:17 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:05:27 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1306.270453453064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:49 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:01 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:48 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:52 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:49 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1292.1469712257385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:31:21 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:34:28 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:10 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:16 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:16 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1301.9231204986572 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:02 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:56:12 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:02:01 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:04 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:00 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1278.8344674110413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:22 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:42 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:35 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:39 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:36 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1296.4154026508331 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:35:58 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:39:06 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:50 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:54 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:52:50 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1277.9686605930328 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:57:15 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:24 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:06 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:10:10 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:07 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1295.342314004898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:18:52 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:22:00 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:27:43 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:31:57 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:35:53 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1281.6481380462646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:40:12 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:24 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:15 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:53:19 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:57:16 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1291.355352640152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:01:43 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:04:53 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:10:43 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:48 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:18:47 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1289.914457321167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:13 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:21 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:32:16 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:36:20 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:40:17 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1283.840635061264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:39 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:47:46 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:53:31 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:36 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:01:33 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1277.5706224441528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:05:57 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:09:04 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:14:45 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:18:49 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:22:46 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1272.1463871002197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:27:08 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:30:20 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:36:04 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:09 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:44:10 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1307.903686761856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:48:55 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:04 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:57:50 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:01:59 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:05:54 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1285.7727642059326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:10:20 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:13:32 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:19:35 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:23:41 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:27:50 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1321.5165226459503 seconds. 
Discarding model... 

Training complete taking 32354.830605506897 total seconds. 
Now scoring model... 
Scoring complete taking 0.9843795299530029 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (140.15644629346352,), 'R2_train': 0.32098905644261844, 'MAE_train': 10.849150011013407, 'MSE_test': 138.09957550362475, 'R2_test': 0.1675614578085971, 'MAE_test': 10.357066222265862}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:06:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:07:10 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:10:20 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:16:06 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:12 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:24:11 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1285.2864949703217 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:28:35 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:31:45 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:33 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:39 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:40 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1306.594654560089 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:25 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:53:35 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:24 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:32 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:42 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1302.7331938743591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:06 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:15:16 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:13 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:25:30 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:29:27 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1318.3331015110016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:08 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:18 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:43:13 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:47:52 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:52:15 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1359.9418015480042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:47 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:00:00 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:05:48 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:58 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:55 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1293.8655560016632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:18:16 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:21:26 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:27:10 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:31:15 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:35:13 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1277.6038415431976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:39:35 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:42:43 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:28 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:52:34 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:56:38 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1283.931717634201 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:00:59 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:04:07 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:09:50 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:13:59 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:18:05 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1289.0760397911072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:22:27 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:25:40 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:35 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:35:41 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:39:39 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1293.1173055171967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:44:01 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:47:09 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:52:51 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:56:57 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:00:58 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1282.631484746933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:05:23 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:08:32 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:14:16 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:18:21 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:22:19 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1277.3987030982971 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:26:41 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:29:49 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:35:30 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:39:36 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:43:38 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1282.3418095111847 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:48:03 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:51:19 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:57:08 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:01:14 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:05:13 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1301.0715532302856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:09:45 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:12:54 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:18:51 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:23:00 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:27:09 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1315.004786491394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:31:40 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:34:49 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:40:33 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:44:46 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:48:55 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1301.3588926792145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:53:21 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:56:36 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:02:27 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:07:00 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:11:36 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1362.5144565105438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:16:02 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:19:15 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:24:58 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:29:11 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:33:22 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1304.521684885025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:37:48 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:41:05 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:46:59 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:51:05 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:55:30 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1323.7868483066559 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:59:52 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:02:59 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:08:48 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:12:57 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:17:02 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1316.1592226028442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:21:48 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:24:57 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:30:47 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:34:57 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:38:55 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1289.3673405647278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:43:18 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:46:26 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:52:08 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:56:13 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:00:33 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1346.362206697464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:05:43 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:08:53 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:14:39 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:18:44 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:22:52 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1289.991616487503 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:27:13 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:30:22 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:36:08 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:40:31 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:44:31 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1304.1182479858398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:48:58 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:52:07 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:57:57 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:02:06 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:06:05 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1289.5872044563293 seconds. 
Discarding model... 

Training complete taking 32596.701629161835 total seconds. 
Now scoring model... 
Scoring complete taking 0.8859875202178955 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (140.15644629346352,), 'R2_train': 0.32098905644261844, 'MAE_train': 10.849150011013407, 'MSE_test': 138.09957550362475, 'R2_test': 0.1675614578085971, 'MAE_test': 10.357066222265862}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRZ_results.json. 
