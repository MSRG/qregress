/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:09 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:25 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:12 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:18 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:18 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1297.6458127498627 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:45 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:53 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:40 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:50 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:53 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1294.5651540756226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:19 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:31 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:25:19 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:34 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:39 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1301.797666311264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:03 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:12 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:53 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:09 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:23 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1300.1921184062958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:43 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:49 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:32 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:42 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:38 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1279.1301736831665 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:00 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:14 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:25 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:36 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:33 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1320.3501434326172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:01 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:09 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:49 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:51 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1272.214033126831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:14 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:22 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:04 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:11 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:21:08 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1296.7322447299957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:25:51 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:03 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:54 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:03 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:43:24 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1331.631958246231 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:01 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:16 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:57:12 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:17 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:05:27 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1306.270453453064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:49 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:13:01 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:48 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:52 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:49 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1292.1469712257385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:31:21 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:34:28 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:10 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:16 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:16 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1301.9231204986572 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:02 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:56:12 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:02:01 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:04 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:10:00 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1278.8344674110413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:22 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:42 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:35 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:39 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:36 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1296.4154026508331 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:35:58 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:39:06 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:50 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:54 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:52:50 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1277.9686605930328 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:57:15 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:24 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:06:06 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:10:10 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:07 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1295.342314004898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:18:52 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:22:00 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:27:43 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:31:57 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:35:53 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1281.6481380462646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:40:12 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:24 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:15 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:53:19 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:57:16 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1291.355352640152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:01:43 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:04:53 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:10:43 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:48 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:18:47 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1289.914457321167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:23:13 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:21 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:32:16 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:36:20 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:40:17 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1283.840635061264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:44:39 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:47:46 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:53:31 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:57:36 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:01:33 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1277.5706224441528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:05:57 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:09:04 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:14:45 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:18:49 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:22:46 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1272.1463871002197 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:27:08 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:30:20 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:36:04 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:09 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:44:10 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1307.903686761856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:48:55 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:04 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:57:50 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:01:59 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:05:54 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1285.7727642059326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:10:20 2024]  Iteration number: 0 with current cost as 0.42560813622455684 and parameters 
[-3.20441607  2.27778271 -2.12151642 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6013697   1.14432445
  1.65238305 -1.8735468   0.7296508   2.8857842  -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:13:32 2024]  Iteration number: 0 with current cost as 0.341392205766047 and parameters 
[-3.27265314  2.18746209 -2.11688009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.55191404  1.14432446
  1.74600828 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:19:35 2024]  Iteration number: 0 with current cost as 0.3703651986770673 and parameters 
[-3.24717237  2.23755702 -2.11854339 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57995114  1.14432445
  1.7076334  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:23:41 2024]  Iteration number: 0 with current cost as 0.37189970123751404 and parameters 
[-3.24989683  2.23817659 -2.11852294 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960146  1.18551998 -1.06648309  0.58034092  1.14432445
  1.71085874 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:27:50 2024]  Iteration number: 0 with current cost as 0.37063608689852096 and parameters 
[-3.25232487  2.22188959 -2.1181758  -0.11653104  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.0664831   0.57123553  1.14432444
  1.71654642 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1321.5165226459503 seconds. 
Discarding model... 

Training complete taking 32354.830605506897 total seconds. 
Now scoring model... 
Scoring complete taking 0.9843795299530029 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (140.15644629346352,), 'R2_train': 0.32098905644261844, 'MAE_train': 10.849150011013407, 'MSE_test': 138.09957550362475, 'R2_test': 0.1675614578085971, 'MAE_test': 10.357066222265862}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRZ_results.json. 
