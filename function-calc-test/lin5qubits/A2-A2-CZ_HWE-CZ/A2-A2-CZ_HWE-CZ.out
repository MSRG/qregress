/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:12 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 17:34:25 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:31 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:06 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:48 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 484.82207679748535 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:41:15 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 17:42:29 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:34 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:09 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:47:51 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 483.9986107349396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:19 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 17:50:33 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:39 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:18 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:56:01 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 488.701664686203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:57:28 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 17:58:42 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:46 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:21 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:05 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 483.91345381736755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:33 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:46 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:50 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:10:25 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:09 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 483.79454493522644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:13:37 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:50 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:16:54 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:18:36 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:20:28 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 500.97946429252625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:57 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:11 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:25:15 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:51 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:34 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 488.12588691711426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:06 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:19 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:33:23 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:59 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:42 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 484.5748586654663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:11 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:26 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:30 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:07 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:44:49 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 486.08883595466614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:46:17 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:30 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:36 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:12 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:01 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 495.433340549469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:31 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:45 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:55 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:31 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:14 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 489.89188742637634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:42 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:55 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:03 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:39 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:26 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 492.559757232666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:53 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:08 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:22 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:59 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:44 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 497.31508469581604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:19:12 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:20:24 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:22:27 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:02 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:25:45 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 480.0675756931305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:27:12 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:28:25 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:28 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:02 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:47 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 482.1961636543274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:35:14 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:26 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:29 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:02 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:45 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 479.2128641605377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:43:14 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:44:27 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:31 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:05 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:49:51 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 486.7062757015228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:51:20 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 19:52:32 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:38 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:13 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:57:54 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 503.31676411628723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:59:43 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:01 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:03:04 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:04:38 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:06:22 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 485.7090992927551 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:07:49 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:09:06 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:11:14 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:12:49 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:14:30 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 488.90278458595276 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:56 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:17:09 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:19:12 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:46 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:22:28 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 478.8455662727356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:23:55 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:25:07 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:27:12 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:46 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:36 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 487.34901881217957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:03 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:33:16 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:22 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:56 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:38:38 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 483.3995921611786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:06 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:41:19 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:43:22 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:57 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:46:38 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 481.38933658599854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:08 2024]  Iteration number: 0 with current cost as 0.3353662828316882 and parameters 
[-3.19224908  2.26499879 -1.67844618 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:20 2024]  Iteration number: 0 with current cost as 0.34044209846462037 and parameters 
[-3.13058441  2.24172342 -1.7598919  -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:51:23 2024]  Iteration number: 0 with current cost as 0.3182829062872693 and parameters 
[-3.15419039  2.24144122 -1.72149435 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:53:00 2024]  Iteration number: 0 with current cost as 0.33075761951788407 and parameters 
[-3.13610208  2.24291804 -1.75190683 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.60271511  1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:54:42 2024]  Iteration number: 0 with current cost as 0.32684848565400965 and parameters 
[-3.16084855  2.24804966 -1.71591661 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508 ]. 
Training complete taking 481.055912733078 seconds. 
Discarding model... 

Training complete taking 12178.35285949707 total seconds. 
Now scoring model... 
Scoring complete taking 1.0896852016448975 seconds. 
Saved predicted values as A2-A2-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (206.77432570092247,), 'R2_train': -0.0017522112657377686, 'MAE_train': 13.567234494378642, 'MSE_test': 218.46282076050187, 'R2_test': -0.31685322980678277, 'MAE_test': 14.664600853424972}. 
Saved model results as A2-A2-CZ_HWE-CZ_results.json. 
