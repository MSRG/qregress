/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:52:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:12 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:29 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:12 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:20 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 780.2293274402618 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:10 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:39 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:22 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:03 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:01 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 759.3571450710297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:18:48 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:23 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:13 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:50 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:47 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 765.7391102313995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:04 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:38 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:21 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:18 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 750.6441059112549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:07 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:23 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:04 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:03 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 767.5650413036346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:54 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:59:31 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:14 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:58 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:07 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 785.0335605144501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:04 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:44 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:15:35 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:31 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:36 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 812.7354912757874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:14 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:23 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:00 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:59 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 793.45077085495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:39 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:59 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:41:32 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:07 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:04 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 726.7187216281891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:49 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:16 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:52 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:34 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:29 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 747.1520884037018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:13 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:33 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:09 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:08:47 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:30 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 718.5512697696686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:13:13 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:33 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:18:09 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:43 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:37 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 725.6607375144958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:25:19 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:42 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:30:18 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:52 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:43 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 725.3930456638336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:37:23 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:39:44 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:42:19 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:44:56 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:47:40 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 722.235951423645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:49:26 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:41 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:19 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:57:00 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 20:59:51 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 726.2210087776184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:01:34 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:02 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:06:32 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:09:01 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:53 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 727.5800178050995 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:38 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:16:05 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:18:59 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:21:44 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:48 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 773.2186090946198 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:39 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:21 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:32:14 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:34:45 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:32 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 761.5646066665649 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:14 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:42 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:18 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:52 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:44 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 736.5916247367859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:51:33 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:53:56 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:25 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:58:56 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:01:40 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 716.0436515808105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:27 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:05:47 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:23 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:11:01 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:13:43 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 719.4752621650696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:25 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:44 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:20:16 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:51 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:41 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 715.4427306652069 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:27:23 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:29:40 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:32:13 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:34:49 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:37:34 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 716.2035784721375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:39:15 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:34 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:11 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:52 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:03 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 749.6646661758423 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:51:58 2024]  Iteration number: 0 with current cost as 0.5266736777098473 and parameters 
[-1.50725454  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855201  -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:54:42 2024]  Iteration number: 0 with current cost as 0.21738920405150636 and parameters 
[ 1.20231152  2.23743447 -2.12427964 -0.11653103  0.55388691 -2.77010914
  3.06858465  2.18960145  1.18551998 -1.06648342]. 
Working on 0.6 fold... 
[Sun Mar 24 22:57:35 2024]  Iteration number: 0 with current cost as 0.21359071537449859 and parameters 
[ 1.24075897  2.23743464 -2.12427943 -0.11653082  0.55388708 -2.77010918
  3.06858477  2.18960145  1.1855202  -1.0664833 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:00:22 2024]  Iteration number: 0 with current cost as 0.21892701612491255 and parameters 
[ 1.17066511  2.23743443 -2.12427943 -0.11653123  0.55388708 -2.77010918
  3.06858477  2.18960145  1.18552019 -1.06648329]. 
Working on 1.0 fold... 
[Sun Mar 24 23:03:35 2024]  Iteration number: 0 with current cost as 0.2150946344684368 and parameters 
[ 1.34838651  2.23743484 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960166  1.18552019 -1.0664835 ]. 
Training complete taking 813.6625854969025 seconds. 
Discarding model... 

Training complete taking 18736.13564157486 total seconds. 
Now scoring model... 
Scoring complete taking 2.2140285968780518 seconds. 
Saved predicted values as A2-A2-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (135.81936823238854,), 'R2_train': 0.3420007440560898, 'MAE_train': 10.744794510050792, 'MSE_test': 137.88459699081528, 'R2_test': 0.1688573082784699, 'MAE_test': 10.617140453758406}. 
Saved model results as A2-A2-CNOT_ESU2_results.json. 
