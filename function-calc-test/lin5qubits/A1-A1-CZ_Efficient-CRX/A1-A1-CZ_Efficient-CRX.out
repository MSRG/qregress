/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:23 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:08 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:32 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 17:40:49 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 17:43:53 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 887.8838858604431 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:46:47 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:58 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:15 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:16 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:06 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 852.1896524429321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:59 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:11 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:23 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:21 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:13 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 850.1245937347412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:30 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:53 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:59 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:05 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 888.8829033374786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:01 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:19 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:43 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:10 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:11 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 905.9902222156525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:06 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:48:18 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:47 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:47 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:42 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 875.4412779808044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:44 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:12 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:32 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:23 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:24 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 880.0785009860992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:21 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:47 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:07 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:23:03 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:26:15 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 893.5512175559998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:21 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:32:55 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:21 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:27 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:41 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 925.2440686225891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:44:48 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:19 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:37 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:55 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:57:04 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 932.4494752883911 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:00:18 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:52 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:22 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:09:41 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:36 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 930.6583008766174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:58 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:25 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:59 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:14 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:26 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 956.1897871494293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:56 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:20 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:37:52 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:41:02 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:11 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 926.2888774871826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:50:27 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:51 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:47 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:52 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 887.4408164024353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:02:01 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:05:25 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:51 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:01 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:07 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 908.9255831241608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:17:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:20:37 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:59 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:09 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:02 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 907.2534217834473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:13 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:36 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:55 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:41:05 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:44:10 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 901.0072939395905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:47:11 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:34 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:53:05 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:24 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:59:36 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 930.9494678974152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:50 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:23 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:11:48 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 22:14:52 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 916.0286800861359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:59 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:21:35 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:24:08 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:25 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:36 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 938.104799747467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:40 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:37:00 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:19 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:17 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:19 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 890.0483450889587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:30 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:47 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:54:03 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:57:07 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:00:03 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 876.5200052261353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:07 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:29 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 23:08:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:56 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:57 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 892.6474661827087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:17:59 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 23:21:30 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:52 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 23:26:59 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:09 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 911.9824233055115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:08 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:42 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 23:39:02 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 23:42:03 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:45:01 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 899.5958061218262 seconds. 
Discarding model... 

Training complete taking 22565.477744817734 total seconds. 
Now scoring model... 
Scoring complete taking 2.06919527053833 seconds. 
Saved predicted values as A1-A1-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (206.9387927548426,), 'R2_train': -0.0025489989442195604, 'MAE_train': 12.577271517852694, 'MSE_test': 191.37228810339, 'R2_test': -0.1535565402258443, 'MAE_test': 12.013220429265761}. 
Saved model results as A1-A1-CZ_Efficient-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:44:33 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 11:47:52 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 11:50:10 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 11:53:10 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 11:56:10 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 879.5712723731995 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:59:08 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 12:02:17 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 12:04:29 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 12:07:14 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 12:10:00 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 825.5754060745239 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:12:45 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 12:15:52 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 12:18:01 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:44 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 12:23:33 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 813.1570982933044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:26:18 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:23 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:31 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 12:34:16 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 12:37:01 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 809.3618779182434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:39:47 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:53 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 12:45:00 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:47 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 12:50:34 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 812.023618221283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:53:19 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:26 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 12:58:33 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 13:01:21 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 13:04:07 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 812.1293108463287 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:06:52 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:59 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:14 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 13:14:58 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 13:17:41 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 817.7841196060181 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:20:28 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 13:23:33 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 13:25:46 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:34 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 13:31:21 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 816.2425138950348 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:05 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:09 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 13:39:20 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 13:42:06 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 13:44:52 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 812.077821969986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:47:38 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 13:50:44 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:53 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 13:55:41 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 13:58:23 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 814.08180975914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:11 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 14:04:24 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:35 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:30 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 14:12:24 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 840.5473117828369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:15:14 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 14:18:25 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 14:20:39 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 14:23:35 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:29 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 842.963317155838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:19 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 14:32:33 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:40 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:30 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:16 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 828.2858741283417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:43:06 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 14:46:16 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:30 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 14:51:22 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 14:54:17 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 841.9760851860046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:57:09 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 15:00:22 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 15:02:36 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 15:05:31 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 15:08:24 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 844.2694380283356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:11:16 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 15:14:29 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 15:16:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 15:19:40 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 15:22:34 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 855.243057012558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:25:28 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 15:28:42 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 15:30:57 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 15:33:48 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 15:36:40 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 844.0828995704651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:39:36 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 15:42:47 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 15:45:03 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 15:47:54 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 15:50:46 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 848.6346459388733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:53:41 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 15:56:50 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 15:59:05 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 16:02:00 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 16:04:49 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 842.2289018630981 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:07:43 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 16:10:55 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 16:13:12 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 16:16:07 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 16:19:01 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 850.618326663971 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:21:57 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 16:25:12 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 16:27:27 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 16:30:22 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 16:33:17 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 854.7633099555969 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:36:13 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 16:39:17 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 16:41:26 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 16:44:18 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 16:47:12 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 840.3156991004944 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:50:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 16:53:19 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 16:55:32 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 16:58:19 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 17:01:06 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 828.2331593036652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:03:53 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 17:06:58 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 17:09:06 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 17:11:50 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 17:14:35 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 807.0218451023102 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:17:21 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Thu Apr  4 17:20:25 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Thu Apr  4 17:22:33 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Thu Apr  4 17:25:19 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:05 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 811.3621301651001 seconds. 
Discarding model... 

Training complete taking 20792.55171108246 total seconds. 
Now scoring model... 
Scoring complete taking 2.0667436122894287 seconds. 
Saved predicted values as A1-A1-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (206.9387927548426,), 'R2_train': -0.0025489989442195604, 'MAE_train': 12.577271517852694, 'MSE_test': 191.37228810339, 'R2_test': -0.1535565402258443, 'MAE_test': 12.013220429265761}. 
Saved model results as A1-A1-CZ_Efficient-CRX_results.json. 
