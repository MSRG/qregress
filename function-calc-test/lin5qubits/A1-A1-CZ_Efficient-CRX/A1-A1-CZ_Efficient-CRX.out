/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:23 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:32:08 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:32 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 17:40:49 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 17:43:53 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 887.8838858604431 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:46:47 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 17:49:58 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:15 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:16 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:06 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 852.1896524429321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:59 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:11 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:23 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:21 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:13 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 850.1245937347412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:15:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:30 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:53 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:59 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:05 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 888.8829033374786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:01 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:19 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:43 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:10 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:11 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 905.9902222156525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:45:06 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 18:48:18 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:47 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:47 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:42 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 875.4412779808044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:44 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:12 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:32 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:23 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:24 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 880.0785009860992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:21 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:17:47 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:07 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:23:03 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:26:15 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 893.5512175559998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:21 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:32:55 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:21 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:27 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:41 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 925.2440686225891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:44:48 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:19 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:37 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:55 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 19:57:04 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 932.4494752883911 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:00:18 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:52 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:22 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:09:41 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:12:36 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 930.6583008766174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:58 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:25 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:59 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:14 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:26 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 956.1897871494293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:56 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:20 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:37:52 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:41:02 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:44:11 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 926.2888774871826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 20:50:27 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:51 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:47 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:52 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 887.4408164024353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:02:01 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:05:25 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:07:51 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:01 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:07 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 908.9255831241608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:17:10 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:20:37 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:59 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:26:09 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:29:02 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 907.2534217834473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:13 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:35:36 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:55 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:41:05 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:44:10 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 901.0072939395905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:47:11 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:34 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 21:53:05 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:24 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 21:59:36 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 930.9494678974152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:50 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:23 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:11:48 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 22:14:52 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 916.0286800861359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:59 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:21:35 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:24:08 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:27:25 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 22:30:36 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 938.104799747467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:40 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:37:00 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:19 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:17 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:19 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 890.0483450889587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:30 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:47 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 22:54:03 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 22:57:07 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:00:03 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 876.5200052261353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:07 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:29 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 23:08:48 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:56 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:57 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 892.6474661827087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:17:59 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 23:21:30 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:52 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 23:26:59 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:30:09 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 911.9824233055115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:08 2024]  Iteration number: 0 with current cost as 0.3700387253127946 and parameters 
[-1.82176683  2.23743458 -2.12427964 -0.11653103  0.55388697 -2.77010903
  3.06858488  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:42 2024]  Iteration number: 0 with current cost as 0.3122912106738731 and parameters 
[-1.54439865  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858488  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029899 -1.87354675]. 
Working on 0.6 fold... 
[Sun Mar 24 23:39:02 2024]  Iteration number: 0 with current cost as 0.3315072465391975 and parameters 
[-1.43602946  2.23743458 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18551998 -1.06648314  0.6027151   1.14432439
  1.31029899 -1.87354674]. 
Working on 0.8 fold... 
[Sun Mar 24 23:42:03 2024]  Iteration number: 0 with current cost as 0.33672965815469186 and parameters 
[-1.41170499  2.23743464 -2.12427958 -0.11653097  0.55388708 -2.77010903
  3.06858492  2.18960151  1.18552004 -1.06648314  0.60271516  1.14432451
  1.31029904 -1.87354668]. 
Working on 1.0 fold... 
[Sun Mar 24 23:45:01 2024]  Iteration number: 0 with current cost as 0.33971858859910875 and parameters 
[-1.4090705   2.23743458 -2.12427964 -0.11653097  0.55388702 -2.77010903
  3.06858492  2.18960145  1.18551993 -1.06648314  0.60271516  1.14432439
  1.31029899 -1.87354674]. 
Training complete taking 899.5958061218262 seconds. 
Discarding model... 

Training complete taking 22565.477744817734 total seconds. 
Now scoring model... 
Scoring complete taking 2.06919527053833 seconds. 
Saved predicted values as A1-A1-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (206.9387927548426,), 'R2_train': -0.0025489989442195604, 'MAE_train': 12.577271517852694, 'MSE_test': 191.37228810339, 'R2_test': -0.1535565402258443, 'MAE_test': 12.013220429265761}. 
Saved model results as A1-A1-CZ_Efficient-CRX_results.json. 
