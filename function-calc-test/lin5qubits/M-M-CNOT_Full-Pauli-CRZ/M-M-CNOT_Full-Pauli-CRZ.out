/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:42 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:43 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:45:02 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:51:38 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:24 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2131.387982606888 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:06:13 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:04 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:20:11 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:47 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:16 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2057.778970718384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:31 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:15 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:22 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:58 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:28 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2053.8273820877075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:45 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:26 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:37 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:35:15 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:39 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2053.7276663780212 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:57 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:43 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:48 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:09:23 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:54 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2051.6438059806824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:23:10 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:54 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:37:05 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:37 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:58 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2052.1750054359436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:57:26 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:13 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:11:16 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:51 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:25:22 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2062.575747489929 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:31:51 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:11 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:42 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:40 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:01:05 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2157.25984787941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:07:40 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:14:27 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:55 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:28 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:35:53 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2066.8408665657043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:42:07 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:49:10 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:56:09 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:02:43 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:10:13 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2056.0989196300507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:25 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:07 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:30:15 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:36:45 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:44:08 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2034.3543434143066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:50:19 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:01 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:04:04 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:10:35 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:18:14 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2046.8896808624268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:24:26 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:31:13 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:38:16 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:44:50 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:52:12 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2035.7152490615845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:58:20 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:05:14 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:12:34 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:19:06 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:26:32 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2066.06108212471 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:46 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:39:29 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:46:34 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:53:22 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:01:14 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2111.3959455490112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:07:59 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:14:40 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:21:44 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:28:18 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:35:44 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2050.072163105011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:09 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:49:28 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:56:44 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:03:28 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:10:59 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2123.2992684841156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:17:33 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:24:14 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:31:17 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:37:47 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:45:30 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2081.037901163101 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:52:18 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:59:01 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:06:08 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:12:41 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:20:06 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2040.3620562553406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:26:19 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:33:02 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:40:05 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:47:13 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:54:59 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2121.3388352394104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:01:34 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:08:16 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:15:19 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:21:54 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:29:19 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2037.2724862098694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:35:31 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:42:13 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:49:18 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:55:50 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:03:23 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2046.6819474697113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:09:38 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:16:39 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:23:45 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:30:14 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:37:50 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2063.8489413261414 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:44:03 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:51:02 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:58:25 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:05:07 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:34 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2082.9345405101776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:18:45 2024]  Iteration number: 0 with current cost as 0.3457870957322319 and parameters 
[-3.39033609  2.29684711 -2.07249262 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648309  0.57665545  1.14432444
  1.87545746 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:25:35 2024]  Iteration number: 0 with current cost as 0.3144298494909901 and parameters 
[-3.42046135  2.23019883 -2.0888314  -0.11653103  0.55388707 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648308  0.56199787  1.14432445
  1.92016779 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:32:39 2024]  Iteration number: 0 with current cost as 0.3187108509347698 and parameters 
[-3.402124    2.27428312 -2.0750858  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.56920868  1.14432446
  1.89118553 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:39:13 2024]  Iteration number: 0 with current cost as 0.30958775976051267 and parameters 
[-3.41450322  2.26103308 -2.07824129 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.566945    1.14432445
  1.90994081 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:46:37 2024]  Iteration number: 0 with current cost as 0.32743738810647155 and parameters 
[-3.41468804  2.25807952 -2.08268589 -0.11653103  0.55388707 -2.77010899
  3.06858498  2.18960145  1.18551998 -1.06648308  0.57053146  1.14432445
  1.91114757 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2043.02849984169 seconds. 
Discarding model... 

Training complete taking 51727.61087322235 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.992729663848877 seconds. 
Saved predicted values as M-M-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (104.65379240573694,), 'R2_train': 0.4929874992728619, 'MAE_train': 9.553770686127613, 'MSE_test': 89.99482475029939, 'R2_test': 0.4575279435385109, 'MAE_test': 8.256924199805956}. 
Saved model results as M-M-CNOT_Full-Pauli-CRZ_results.json. 
