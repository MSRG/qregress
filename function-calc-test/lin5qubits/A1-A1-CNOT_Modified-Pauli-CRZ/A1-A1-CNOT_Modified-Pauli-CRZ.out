/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:50:47 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:51:08 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:45 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:28 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:44 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:43 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.49490880966187 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:26 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:04 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:47 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:03 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:02 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.94523334503174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:44 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:22 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:01:05 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:20 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:03:19 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.6192009449005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:03 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:41 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:05:23 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:39 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:38 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.39620447158813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:21 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:58 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:43 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:10:58 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:57 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.2188003063202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:41 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:19 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:04 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:19 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:19 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 261.67872166633606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:02 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:40 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:22 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:37 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:20:37 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 256.4041254520416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:19 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:57 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:41 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:55 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:55 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.71519446372986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:39 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:26:16 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:58 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:15 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:14 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.47607922554016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:57 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:35 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:18 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:32:34 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:34 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.67366313934326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:34:17 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:55 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:39 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:55 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:55 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 261.43730449676514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:16 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:02 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:17 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:15 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 261.4526631832123 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:42:59 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:37 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:44:20 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:36 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:36 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.1724681854248 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:18 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:55 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:39 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:53 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:53 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.4314205646515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:36 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:14 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:52:57 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:54:13 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:55:12 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.40076994895935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:55 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:32 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:16 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:30 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:29 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.1110372543335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:14 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:51 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:35 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:50 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:03:49 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.7756519317627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:04:32 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:12 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:58 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:15 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:14 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 264.60463523864746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:08:56 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:09:35 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:10:17 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:34 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:12:33 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.7663049697876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:17 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:55 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:39 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:54 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:55 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 262.3702428340912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:17:39 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:18:19 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:02 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:20:17 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:17 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 262.1063220500946 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:59 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:37 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:23:20 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:36 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:25:34 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.18429946899414 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:18 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:54 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:38 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:28:52 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:50 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.65804934501648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:32 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:31:11 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:53 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:08 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:07 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.8531756401062 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:50 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:27 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:10 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:25 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:24 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.1247856616974 seconds. 
Discarding model... 

Training complete taking 6479.073286294937 total seconds. 
Now scoring model... 
Scoring complete taking 0.6922645568847656 seconds. 
Saved predicted values as A1-A1-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (193.24510235504033,), 'R2_train': 0.06379233522234207, 'MAE_train': 12.576626105621372, 'MSE_test': 169.18972248710836, 'R2_test': -0.019844162643641194, 'MAE_test': 11.37099599494738}. 
Saved model results as A1-A1-CNOT_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:26:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:14 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:28:07 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:28:59 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:40 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:31:50 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 328.044527053833 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:32:42 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:33:27 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:34:18 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:35:48 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:37:02 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.0892779827118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:54 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:38:38 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:39:33 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:03 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:42:13 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.2955141067505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:43:08 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:43:54 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:44:44 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:46:14 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:47:28 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.4400017261505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:18 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:04 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:49:56 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:51:26 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:52:36 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 309.50754976272583 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:53:29 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:54:14 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:55:06 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:56:36 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:47 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 308.7595248222351 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:58:38 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:59:23 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:00:16 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:01:46 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:02:59 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 310.3933069705963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:03:49 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:04:34 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:05:26 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:06:55 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:08:06 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 308.6352307796478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:08:57 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:42 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:35 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:12:06 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:13:17 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.08033204078674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:14:08 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:53 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:45 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:15 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:33 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 324.3920612335205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:33 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:20:17 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:09 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:41 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:24:00 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 319.31980657577515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:51 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:25:36 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:32 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:02 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:29:14 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.6345462799072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:06 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:50 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:31:42 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:13 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:34:24 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 309.8677816390991 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:35:15 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:36:01 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:36:52 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:38:22 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:39:34 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 309.785546541214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:40:24 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:41:09 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:42:01 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:43:32 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:44:43 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 309.0532560348511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:45:34 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:19 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:47:10 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:48:54 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:50:06 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 323.44515347480774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:50:58 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:51:43 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:36 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:08 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:18 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.16217947006226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:10 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:56:55 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:57:47 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:59:17 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:00:28 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 308.71574330329895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:19 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:02:05 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:02:58 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:31 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:05:56 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 329.3249635696411 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:06:48 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:34 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:08:24 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:55 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:11:08 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.65382409095764 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:02 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:46 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:13:39 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:15:11 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:16:22 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.42161321640015 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:14 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:18:00 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:18:50 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:20:20 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:21:33 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 310.3284661769867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:22:24 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:23:11 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:24:03 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:41 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:26:55 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 333.04344272613525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:27:56 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:28:42 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:29:34 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:31:05 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:32:20 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.2049045562744 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:33:12 2024]  Iteration number: 0 with current cost as 0.3460325798915368 and parameters 
[ 1.33999862  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.60271495  1.14432429
  1.31029883 -1.8735468   0.72965033  2.88578404 -0.54534351 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:33:57 2024]  Iteration number: 0 with current cost as 0.28853377252097057 and parameters 
[-1.29406983  2.23743473 -2.12427954 -0.11653093  0.55388713 -2.77010897
  3.06858498  2.18960155  1.18552008 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735467   0.7296507   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:50 2024]  Iteration number: 0 with current cost as 0.540885482477226 and parameters 
[ 0.02415263  2.23743473 -2.12427944 -0.11653093  0.55388708 -2.77010897
  3.06858489  2.18960155  1.18552018 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.8735467   0.72965061  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:36:20 2024]  Iteration number: 0 with current cost as 0.5183899726642975 and parameters 
[ 0.00866845  2.23743464 -2.12427944 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648328  0.6027151   1.14432445
  1.31029899 -1.87354661  0.72965061  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:33 2024]  Iteration number: 0 with current cost as 0.32750283975187355 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648337  0.60271496  1.14432445
  1.31029884 -1.87354666  0.72965066  2.88578405 -0.54534321 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 311.79748582839966 seconds. 
Discarding model... 

Training complete taking 7869.397470712662 total seconds. 
Now scoring model... 
Scoring complete taking 0.8512988090515137 seconds. 
Saved predicted values as A1-A1-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (193.24510235504033,), 'R2_train': 0.06379233522234207, 'MAE_train': 12.576626105621372, 'MSE_test': 169.18972248710836, 'R2_test': -0.019844162643641194, 'MAE_test': 11.37099599494738}. 
Saved model results as A1-A1-CNOT_Modified-Pauli-CRZ_results.json. 
