/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:19 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:53 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:29 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:49:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:07 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:53 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:18 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2756.0639305114746 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:50 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:17 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:35:45 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:18 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:03 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:52 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2822.305969953537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:03:51 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:17 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:22:13 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 19:23:43 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:21 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:53 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2724.6622660160065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:15 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:36 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:07:20 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:49 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:19 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:25:41 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2687.9280846118927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:03 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:34 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:53:00 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:45 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:03:22 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:47 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2759.7938237190247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:20:03 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:26:20 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:38:02 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:31 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:23 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2697.998646259308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:05:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:11:49 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:23:32 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:02 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:34:00 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:42:27 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2760.311980485916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:51:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:57:16 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:08:54 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 23:10:23 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:53 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:27:12 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2671.9436931610107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:33 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:41:51 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:53:32 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 23:55:01 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:39 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:12:06 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2695.5999162197113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:20:28 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:49 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:39:11 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 00:40:52 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:49:52 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:58:11 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2766.925829410553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:06:35 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 01:12:58 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:24:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 01:26:07 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:34:39 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:43:01 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2689.1035232543945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:51:26 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 01:57:49 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:09:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 02:11:08 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:19:51 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:28:24 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2721.6060931682587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:36:46 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 02:43:12 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:54:54 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 02:56:23 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:04:54 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:13:15 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2689.8005986213684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:21:36 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 03:27:53 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:39:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 03:41:07 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:50:06 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:58:38 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2724.1264379024506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:07:00 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 04:13:23 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:25:21 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 04:27:12 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:35:56 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:44:21 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2740.3322405815125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:52:40 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 04:59:09 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:10:50 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 05:12:19 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:20:49 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:29:08 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2685.1439073085785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:37:26 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 05:43:42 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:55:22 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 05:56:51 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:05:22 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:13:42 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2675.6288883686066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:22:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 06:28:43 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:40:25 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 06:41:54 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:50:32 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:58:54 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2711.7064080238342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:07:14 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 07:13:28 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:25:06 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 07:26:34 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:35:07 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 07:43:35 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2688.5810582637787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:52:02 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 07:58:22 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:10:03 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 08:11:32 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 08:20:03 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 08:28:25 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2684.180774450302 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:36:46 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 08:43:04 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:54:39 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 08:56:08 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 09:04:36 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:12:57 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2670.4793140888214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:21:16 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 09:27:34 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:39:26 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 09:41:03 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 09:49:34 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:58:13 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2720.711629152298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:06:37 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 10:12:59 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:24:41 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 10:26:10 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 10:34:40 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 10:43:00 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2682.191461086273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:51:19 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 10:57:38 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:09:18 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 11:10:48 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 11:19:19 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 11:27:42 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2684.961494207382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:36:04 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 11:42:34 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:54:16 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 11:55:46 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 12:04:17 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 12:12:34 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2688.922614336014 seconds. 
Discarding model... 

Training complete taking 67801.01289606094 total seconds. 
Now scoring model... 
Scoring complete taking 0.9876642227172852 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (140.15648058384232,), 'R2_train': 0.32098889031724154, 'MAE_train': 10.849158815312844, 'MSE_test': 138.09857983751002, 'R2_test': 0.16756745949865337, 'MAE_test': 10.357025707433404}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRX_results.json. 
