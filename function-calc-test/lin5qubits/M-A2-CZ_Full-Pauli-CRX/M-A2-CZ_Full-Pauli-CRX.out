/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:26 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:09 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:33 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:04 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:17:04 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:32 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3008.277506351471 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:05 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:18 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:55 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:06:49 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:18:17 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2985.9250028133392 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:28:51 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:03 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:43 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:57:33 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:08 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3037.6913797855377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:28 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:31 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:10 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:16 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:59:02 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2998.512003183365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:27 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:31 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:27:52 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:36:54 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:23 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2958.640716791153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:58:45 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:52 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:17:20 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:26:16 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:38:04 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2979.223402738571 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:24 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:56:39 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:30 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:16:19 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:27:37 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2974.910411119461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:01 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:46:15 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:56:34 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:05:22 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:16:47 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2949.485725879669 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:09 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:35:15 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:45:53 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:54:52 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:06:35 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2989.223368406296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:16:58 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:25:01 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:35:22 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:44:19 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:55:45 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2945.058147907257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:06:03 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:14:08 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:24:29 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:33:17 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:45:12 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2978.62930560112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:55:42 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:04:15 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:14:55 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:23:45 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:35:09 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2990.496493577957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:45:33 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:53:36 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:04:07 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:12:58 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:24:20 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2947.3577649593353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:34:40 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:42:46 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:53:13 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:02:19 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:13:44 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2969.2380628585815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:24:09 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:32:16 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:42:50 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:52:02 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:03:23 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2982.3999269008636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:13:51 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:21:56 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:32:37 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:41:24 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:52:47 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2957.18817281723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:03:09 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:11:13 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:21:32 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:30:24 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:41:56 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2948.8510591983795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:52:17 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:00:20 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:10:41 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 08:19:32 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:30:52 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2931.9351963996887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:41:14 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:49:37 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:00:08 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 09:09:11 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:20:31 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2989.9575176239014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:30:59 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:39:27 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:49:48 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 09:58:41 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:10:08 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2971.7534704208374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 10:20:31 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:28:34 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:39:12 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 10:48:01 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:59:31 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2981.2537803649902 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 11:10:12 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 11:18:56 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:29:13 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 11:38:04 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:49:37 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3004.0847845077515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 12:00:16 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:08:19 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 12:18:40 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 12:27:31 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:38:49 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2936.367372274399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 12:49:13 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:57:18 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 13:07:38 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 13:16:25 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 13:27:50 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2944.4620027542114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 13:38:17 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 13:46:22 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 13:56:43 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 14:05:35 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 14:17:16 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2967.627506017685 seconds. 
Discarding model... 

Training complete taking 74328.55252671242 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2828431129455566 seconds. 
Saved predicted values as M-A2-CZ_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (67.2078583958774,), 'R2_train': 0.6744004820990965, 'MAE_train': 5.41681420379596, 'MSE_test': 50.741807752630535, 'R2_test': 0.6941378254081055, 'MAE_test': 5.393488864913566}. 
Saved model results as M-A2-CZ_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:26:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:26:43 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:54 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:45:18 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:09 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:42 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2964.4492077827454 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:16:02 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:24:06 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:34:28 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:43:42 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:12 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2968.390353679657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:05:30 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:13:32 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:23:53 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:51 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:44:10 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2944.529844045639 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:54:36 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:02:43 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:13:23 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:22:23 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:33:43 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2972.195682287216 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:44:07 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:52:16 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:02:43 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:11:38 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:23:06 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2960.0261600017548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:33:27 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:41:47 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:11 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:01:18 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:06 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3003.7611453533173 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:23:31 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:31:42 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:42:04 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:51:10 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:02:46 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2991.9226830005646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:13:28 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:21:38 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:32:06 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:40:59 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:52:33 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2973.716656446457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:02:58 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:11:23 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:21:58 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:30:52 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:42:22 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3006.7194759845734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:53:22 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:01:30 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:11:53 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:20:53 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:32:53 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3033.986324071884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:43:38 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:51:42 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:02:05 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:11:19 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:22:39 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2967.4931242465973 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:33:08 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:41:29 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:52:17 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:01:16 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:12:48 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3013.8104474544525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:23:20 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:31:40 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:42:21 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:51:09 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:02:52 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2998.717693090439 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:13:21 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:21:28 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:31:56 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:40:48 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:52:17 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2973.2401452064514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:02:52 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:10:57 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:21:20 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:30:15 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:42:09 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3010.3790249824524 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:53:01 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:01:21 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:12:00 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:21:05 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:32:26 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2993.4062876701355 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:42:55 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:51:02 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:01:23 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:10:11 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:21:48 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2965.819085597992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:32:21 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:40:27 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:51:04 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:59:55 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:11:22 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2970.432080745697 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:21:51 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:30:12 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:40:41 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:49:58 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:01:30 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3034.1637573242188 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:12:25 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:20:34 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:31:10 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:39:58 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:51:22 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2958.702808380127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:01:44 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:09:57 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:20:21 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:29:09 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:40:36 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2956.374479532242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:51:00 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:59:04 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:09:26 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:18:37 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:30:23 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2999.9881982803345 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:41:18 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 06:49:31 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:59:54 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:08:49 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:20:07 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2966.647902727127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:30:27 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:38:28 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:48:48 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:57:37 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:08:56 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2926.8767635822296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:19:14 2024]  Iteration number: 0 with current cost as 0.1659168966550467 and parameters 
[-2.91942663  2.33475102 -2.11241236 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.66858407  1.14432446
  1.36234155 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 08:27:21 2024]  Iteration number: 0 with current cost as 0.15344091232141763 and parameters 
[-2.92162061  2.34133265 -2.10841239 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551998 -1.06648308  0.6774949   1.14432445
  1.3736274  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:37:50 2024]  Iteration number: 0 with current cost as 0.13520145083349178 and parameters 
[-2.90697141  2.33763223 -2.10811498 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.65375086  1.14432445
  1.34515871 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:46:48 2024]  Iteration number: 0 with current cost as 0.14907708669218173 and parameters 
[-2.91884011  2.33237978 -2.11043142 -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960146  1.18551998 -1.06648308  0.66566934  1.14432445
  1.36202439 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:58:09 2024]  Iteration number: 0 with current cost as 0.1619611391137155 and parameters 
[-2.91178982  2.34126475 -2.10597729 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66047237  1.14432445
  1.35469765 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2986.80607008934 seconds. 
Discarding model... 

Training complete taking 74542.55707383156 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2986276149749756 seconds. 
Saved predicted values as M-A2-CZ_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (67.2078583958774,), 'R2_train': 0.6744004820990965, 'MAE_train': 5.41681420379596, 'MSE_test': 50.741807752630535, 'R2_test': 0.6941378254081055, 'MAE_test': 5.393488864913566}. 
Saved model results as M-A2-CZ_Full-Pauli-CRX_results.json. 
