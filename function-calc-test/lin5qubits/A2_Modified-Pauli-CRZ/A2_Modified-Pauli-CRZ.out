/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:37 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:31:17 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:31:57 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:33:20 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:34:00 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 252.74094080924988 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:49 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:30 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:08 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:33 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:38:12 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 253.2681007385254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:39:04 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:42 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:40:25 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:41:47 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:42:27 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 254.02520775794983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:43:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:09 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:48 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:11 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 264.04038286209106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:47:47 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:48:27 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:49:06 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:30 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:09 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.8005437850952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:00 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:40 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:19 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:43 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 254.63352489471436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:16 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:56 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:34 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:59 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:37 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.351096868515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:46 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:28 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:07 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:32 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:11 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 267.69737124443054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:01 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:40 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:19 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:42 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:21 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 256.04318714141846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:58 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:36 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:12:02 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:41 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 254.8208601474762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:13:33 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:12 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:50 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:16:21 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:01 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.4807333946228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:50 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:18 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:47 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:27 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 266.1870331764221 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:57 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:37 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:05 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:44 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.93559527397156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:35 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:13 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:22 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:02 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.1134696006775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:52 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:38 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:17 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:42 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:22 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 261.3123013973236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:13 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:52 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:32 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:20 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:00 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 275.99897146224976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:49 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:33 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:12 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:38 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:16 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.00190687179565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:07 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:46 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:24 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:53 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:33 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.40513491630554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:23 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:04 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:43 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:13 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:51:51 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.6226952075958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:43 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:22 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:03 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:27 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:06 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 253.56268048286438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:57 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:36 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:15 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:39 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:18 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 253.064875125885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:09 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:47 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:35 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:02 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:41 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 263.11075353622437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:31 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:10 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:48 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:12 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 248.4268147945404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:45 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:23 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:11:03 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:25 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:14 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 265.5717418193817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:05 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:44 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:15:22 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:45 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:23 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 247.81985449790955 seconds. 
Discarding model... 

Training complete taking 6457.036739587784 total seconds. 
Now scoring model... 
Scoring complete taking 0.8369026184082031 seconds. 
Saved predicted values as A2_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.3130300309287,), 'R2_train': -0.2611292653031918, 'MAE_train': 13.7800749603307, 'MSE_test': 122.84974676405518, 'R2_test': 0.25948458761070614, 'MAE_test': 10.149093254668012}. 
Saved model results as A2_Modified-Pauli-CRZ_results.json. 
