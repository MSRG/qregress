/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:07 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:58:23 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:54 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:56 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:28 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 908.427746295929 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:15 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:29 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:16:00 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:18:59 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:29 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 898.8253827095032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:12 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:27 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:00 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:02 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:36 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 905.1575748920441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:21 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:40 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:13 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:13 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:44 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 909.9637422561646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:31 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:51 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:27 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:31 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:06 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 922.0812277793884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:51 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:09 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:42 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:49 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:23 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 916.6317937374115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:07 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:25 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:59 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:35:06 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:42 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 919.3844738006592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:29 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:44:50 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:27 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:50:31 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:07 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 925.3177108764648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:53 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:49 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:52 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:22 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 915.8698425292969 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:12 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:24 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:55 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:59 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:44 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 921.0338923931122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:31 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:30:55 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:30 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:33 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:09 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 928.2621099948883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:04 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:21 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:48:54 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:51:59 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:35 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 925.6317086219788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:25 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:48 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:26 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:07:29 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:12 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 933.6843109130859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:55 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:19:55 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:58 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:34 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 924.8633432388306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:29:21 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:40 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:35:12 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:19 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:41:58 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 922.2483916282654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:46 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:50:31 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:36 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:10 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 913.2417352199554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:02 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:03:28 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:57 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:58 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 22:12:34 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 925.2363147735596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:30 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:18:51 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:29 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:36 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:17 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 943.3141181468964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:11 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:34:38 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:37:20 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:40:39 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:35 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 978.2256977558136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:35 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:10 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:50 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:56 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:00:33 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 957.8736503124237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:33 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:07:08 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:09:44 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 23:13:00 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:16:45 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 971.9936912059784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:19:38 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:26:03 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:24 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:06 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 979.6240491867065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:54 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:18 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:02 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 23:45:16 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:49:16 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 973.7996706962585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:15 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:46 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:22 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Mon Mar 25 00:01:35 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:14 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 953.9699399471283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:08:04 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Mon Mar 25 00:11:39 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Mon Mar 25 00:14:22 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Mon Mar 25 00:17:35 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Mon Mar 25 00:21:17 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 965.7516565322876 seconds. 
Discarding model... 

Training complete taking 23340.414640188217 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.3910632133483887 seconds. 
Saved predicted values as M-A2-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (301.5658324309453,), 'R2_train': -0.4609852478341061, 'MAE_train': 15.604482381115016, 'MSE_test': 238.44666165439634, 'R2_test': -0.437312103923031, 'MAE_test': 13.249463316977838}. 
Saved model results as M-A2-CNOT_ESU2_results.json. 
