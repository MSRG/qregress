/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:19 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:21 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:52 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:21 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:52 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:19 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 675.513471364975 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:35 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 17:45:04 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:31 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:58 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:28 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.6448345184326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:42 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:40 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:00:08 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:35 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 670.3457343578339 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:52 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:20 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:47 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:18 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:13:52 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 671.4242684841156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:05 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:11 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:39 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:04 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 674.6366944313049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:17 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:44 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:13 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:43 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:10 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 669.8299198150635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:27 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:54 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:23 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:54 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:20 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.05623960495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:37 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:08 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:32 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:00 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:27 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 663.6863315105438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:39 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:33 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:00 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:28 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 661.3573424816132 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:39 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:35 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:05 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:34 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.6757249832153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:22:48 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:18 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:48 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:16 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:47 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 671.1939957141876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:00 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:31 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:01 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:30 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:00 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 675.6112630367279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:16 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:50 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:21 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:51 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:21 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 679.442791223526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:35 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:06 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:37 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:07 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:36 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 676.4184441566467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:07:51 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:20 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:14:20 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:49 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 672.66654920578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:05 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:35 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:06 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:39 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:07 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 676.1798753738403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:21 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:50 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:20 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:51 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:23 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 679.0586202144623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:40 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:12 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:46:45 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:48:15 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:46 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 680.869128704071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:52:59 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:55:30 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:02 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:32 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:02 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 677.2972321510315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:17 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:46 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:13 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:10:44 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:13:17 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 674.9389700889587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:33 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:59 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:28 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:21:58 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:24 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 668.1328294277191 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:38 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:04 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:31 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:58 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:24 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 656.7348113059998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:37:35 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:59 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:25 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:43:56 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:21 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 659.0736873149872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:48:34 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:57 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:53:29 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:03 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:30 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 665.152410030365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:59:42 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:09 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:38 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:09 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:41 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 672.3374633789062 seconds. 
Discarding model... 

Training complete taking 16774.27955508232 total seconds. 
Now scoring model... 
Scoring complete taking 1.7246606349945068 seconds. 
Saved predicted values as A1_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.69854454070173,), 'R2_train': -0.0013850769778320782, 'MAE_train': 12.570106826515946, 'MSE_test': 193.76226304659724, 'R2_test': -0.16796286443315656, 'MAE_test': 12.065280461447625}. 
Saved model results as A1_Efficient-CRZ_results.json. 
