/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:19 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:21 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 17:33:52 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:21 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:52 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:19 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 675.513471364975 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:35 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 17:45:04 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:31 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:58 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:28 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.6448345184326 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:42 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:40 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:00:08 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:35 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 670.3457343578339 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:52 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:20 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:09:47 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:18 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:13:52 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 671.4242684841156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:05 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:21:11 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:22:39 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:04 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 674.6366944313049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:17 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:44 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:13 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:43 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:36:10 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 669.8299198150635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:27 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:54 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:23 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:54 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:20 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.05623960495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:37 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:08 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:32 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:00 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:27 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 663.6863315105438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:00:39 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:33 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:00 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:28 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 661.3573424816132 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:11:39 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:35 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:05 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:34 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.6757249832153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:22:48 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:18 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:48 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:16 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:47 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 671.1939957141876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:00 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:31 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:01 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:30 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:00 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 675.6112630367279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:45:16 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:50 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:21 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:51:51 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:21 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 679.442791223526 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:35 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:06 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:37 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:03:07 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:36 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 676.4184441566467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:07:51 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:20 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:14:20 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:49 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 672.66654920578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:05 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:35 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:06 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:39 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:28:07 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 676.1798753738403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:21 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:50 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:20 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:51 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:23 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 679.0586202144623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:40 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:44:12 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:46:45 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:48:15 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:50:46 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 680.869128704071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:52:59 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 20:55:30 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:02 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:59:32 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:02 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 677.2972321510315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:17 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:06:46 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:13 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:10:44 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:13:17 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 674.9389700889587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:33 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:59 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:20:28 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:21:58 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:24:24 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 668.1328294277191 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:26:38 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:29:04 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:31:31 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:58 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:24 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 656.7348113059998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:37:35 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:59 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:25 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:43:56 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:21 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 659.0736873149872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:48:34 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:57 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:53:29 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:03 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:30 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 665.152410030365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:59:42 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:09 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:38 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:09 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:41 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 672.3374633789062 seconds. 
Discarding model... 

Training complete taking 16774.27955508232 total seconds. 
Now scoring model... 
Scoring complete taking 1.7246606349945068 seconds. 
Saved predicted values as A1_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.69854454070173,), 'R2_train': -0.0013850769778320782, 'MAE_train': 12.570106826515946, 'MSE_test': 193.76226304659724, 'R2_test': -0.16796286443315656, 'MAE_test': 12.065280461447625}. 
Saved model results as A1_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:38 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 11:40:05 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:42:30 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:43:57 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:46:29 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 665.7834091186523 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:48:41 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 11:51:06 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:32 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:55:06 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:57:34 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 665.0860171318054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:59:49 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 12:02:16 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:04:43 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:06:10 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:08:34 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 657.8903541564941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:10:44 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 12:13:16 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:15:39 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:17:08 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:19:39 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 665.0393362045288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:21:50 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 12:24:17 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:26:41 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:10 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:38 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 662.25488448143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:32:52 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:19 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:41 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:08 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:41:33 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 653.5795922279358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:43:46 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 12:46:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:48:35 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:50:02 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:52:27 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 653.2502586841583 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:39 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 12:57:05 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:31 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:01:03 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:03:32 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 663.864194393158 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:05:42 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 13:08:14 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:52 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:12:22 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:14:54 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 681.589182138443 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:17:06 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 13:19:44 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:14 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:23:45 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:26:13 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 680.9381158351898 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:28:25 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:50 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:33:19 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:49 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:19 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 664.936262845993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:39:31 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 13:42:00 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:32 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:04 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:48:34 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 674.1823616027832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:50:44 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 13:53:11 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:55:37 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:57:05 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:33 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 664.4239373207092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:48 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 14:04:13 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:40 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:08:06 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:10:34 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 654.8274059295654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:43 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 14:15:10 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:17:35 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:19:02 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:21:30 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 658.7613706588745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:23:44 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 14:26:11 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:28:36 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:30:03 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:32:35 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 666.4721267223358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:34:50 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 14:37:15 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:39:42 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:41:14 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:43:44 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 665.5536241531372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:54 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 14:48:27 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:50:54 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:52:25 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:54:49 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 669.850145816803 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:57:07 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 14:59:33 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:01:58 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:03:29 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:05:56 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 666.7661068439484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:08:11 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 15:10:40 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:13:05 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:14:31 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:16:59 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 659.6721460819244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:19:11 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 15:21:36 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:24:07 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:25:34 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:28:02 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 669.6841628551483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:30:20 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 15:32:49 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:35:15 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:36:43 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:39:09 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 666.0995483398438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:41:26 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 15:43:54 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:46:20 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:47:50 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:50:16 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 661.424887418747 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:52:30 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 15:54:56 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:57:24 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:58:51 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:01:18 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 667.259039402008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:03:39 2024]  Iteration number: 0 with current cost as 0.3801133993395362 and parameters 
[-1.86063585  2.23743461 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858493  2.18960142  1.18552001 -1.06648311  0.6027151   1.14432442
  1.31029896 -1.87354677]. 
Working on 0.4 fold... 
[Thu Apr  4 16:06:05 2024]  Iteration number: 0 with current cost as 0.3402131812549596 and parameters 
[-1.73750487  2.23743459 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18551998 -1.06648319  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:08:38 2024]  Iteration number: 0 with current cost as 0.321498174611998 and parameters 
[-1.59867869  2.2374346  -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858492  2.18960139  1.18551998 -1.06648315  0.60271507  1.14432445
  1.31029892 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:10:04 2024]  Iteration number: 0 with current cost as 0.3568027277129595 and parameters 
[-1.80309343  2.23743458 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858493  2.18960145  1.18552001 -1.06648314  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:12:28 2024]  Iteration number: 0 with current cost as 0.35714830022681604 and parameters 
[-1.79851554  2.23743461 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858493  2.18960145  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Training complete taking 666.878719329834 seconds. 
Discarding model... 

Training complete taking 16626.068245887756 total seconds. 
Now scoring model... 
Scoring complete taking 1.7614552974700928 seconds. 
Saved predicted values as A1_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (206.69854454070173,), 'R2_train': -0.0013850769778320782, 'MAE_train': 12.570106826515946, 'MSE_test': 193.76226304659724, 'R2_test': -0.16796286443315656, 'MAE_test': 12.065280461447625}. 
Saved model results as A1_Efficient-CRZ_results.json. 
