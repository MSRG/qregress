/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:53 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:44 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:45 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:58:09 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:12 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:10 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2756.1103553771973 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:48 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:12 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:22 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:44:28 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:46 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:50 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2802.942234992981 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:03:32 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:27 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:28 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:30:49 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:52 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:49 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2760.202342271805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:32 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:13 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:07:20 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:16:42 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:45 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:26:59 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2792.281240463257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:36:04 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:53 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:52 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:03:01 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 21:04:04 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:59 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2734.421472787857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:21:42 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:21 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:23 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:48:40 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:43 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:34 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2737.419088125229 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:07:15 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:16:46 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:34:57 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 22:36:00 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:52 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2782.4409692287445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:53:38 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:42 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:11:41 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:21:03 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 23:22:06 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:10 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2797.2218906879425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:40:16 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:49:57 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:18 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:07:35 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 00:08:38 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:27 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2779.7030425071716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:26:35 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:36:07 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:13 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:53:51 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 00:54:55 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:20 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2792.7922184467316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:08 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:22:40 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:44 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:39:55 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:59 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:50:19 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2748.16570353508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:58:56 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:08:28 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:16:27 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:25:45 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 02:26:48 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:36:02 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2746.578387260437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:44:43 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:54:31 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:02:46 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:12:01 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 03:13:03 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:21:54 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2749.4769208431244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:30:32 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:40:30 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:48:25 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:57:42 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 03:58:52 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:08:21 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2804.0300006866455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:17:16 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:51 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:35:05 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:44:18 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 04:45:20 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:54:17 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2739.853036403656 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:02:57 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:12:36 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:20:55 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:30:03 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 05:31:16 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:40:07 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2767.268345594406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:49:03 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:58:39 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:06:39 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:16:02 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 06:17:04 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:26:05 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2742.2378334999084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:34:45 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:44:15 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:52:35 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:01:49 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 07:02:53 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:06 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2801.4489369392395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:21:27 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:30:57 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:38:52 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:48:04 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 07:49:06 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:57:56 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2707.662001132965 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:06:34 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:16:05 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:24:05 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:33:15 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 08:34:17 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:43:19 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2733.3784108161926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:52:08 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:02:19 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:10:19 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:19:45 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 09:20:48 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:30:22 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2812.683729171753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:39:02 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:48:35 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:56:30 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:05:41 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 10:06:43 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:15:34 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2712.1982793807983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:24:13 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:33:56 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:42:15 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:51:24 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 10:52:27 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:01:24 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2793.2902245521545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 11:10:46 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 11:20:16 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:28:15 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:37:25 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 11:38:27 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:47:22 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2729.407100200653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:56:16 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:05:47 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 12:13:44 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:22:52 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 12:23:54 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:32:44 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2751.3979909420013 seconds. 
Discarding model... 

Training complete taking 69074.61343407631 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9981949329376221 seconds. 
Saved predicted values as M_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (1.6322922889402167,), 'R2_train': 0.9920920916833604, 'MAE_train': 0.9887138067919501, 'MSE_test': 54.68514368715554, 'R2_test': 0.6703681302100142, 'MAE_test': 2.9729326159566027}. 
Saved model results as M_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:52:51 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:53:21 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:03:10 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:10:45 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:19:31 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:31 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:29:02 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2640.814246892929 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:21 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:46:27 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:54:05 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:02:49 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 13:03:48 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:12:10 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2582.2081928253174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:20:23 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:29:25 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:36:57 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:45:45 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:44 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:03 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2570.964522600174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:03:15 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:28 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:59 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:28:46 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 14:29:51 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:38:16 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2595.022778272629 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:46:29 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:55:42 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:03:21 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:12:05 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 15:13:05 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:21:28 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2591.89159989357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:29:41 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:39:01 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:46:34 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:55:17 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 15:56:16 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:04:39 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2599.333117723465 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:13:01 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:22:02 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:29:51 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:38:32 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 16:39:31 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:47:52 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2583.395318031311 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:56:05 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:05:08 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:12:42 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:21:24 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 17:22:24 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:31:29 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2631.8209805488586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:39:56 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:48:59 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:56:33 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:05:15 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 18:06:15 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:14:36 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2590.9448397159576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:23:07 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:32:09 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:39:41 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:48:23 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 18:49:22 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:57:47 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2571.203536748886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:05:58 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:15:36 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:23:11 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:31:53 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 19:32:53 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:41:51 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2642.7260127067566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:50:01 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:59:31 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:07:04 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:15:46 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 20:16:45 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:25:06 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2597.7461364269257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:33:19 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:42:21 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:49:54 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:58:38 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 20:59:38 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:08:02 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2575.6111829280853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:16:14 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:25:15 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:32:48 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:41:28 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 21:42:27 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:50:51 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2570.4926834106445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:59:05 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:08:07 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:15:42 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:24:25 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 22:25:24 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:33:48 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2574.8286986351013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:42:00 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:51:02 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:58:36 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:07:20 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 23:08:19 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:16:41 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2573.9998207092285 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:24:54 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:33:59 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:41:32 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:50:30 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Thu Apr  4 23:51:30 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:00:15 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2614.253391981125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:08:28 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:17:47 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:25:20 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:34:06 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 00:35:05 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:43:28 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2596.6723964214325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:51:45 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:00:45 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:08:20 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:17:05 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 01:18:05 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:26:29 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2582.522581100464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:34:48 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:44:03 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:52:08 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:01:03 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 02:02:02 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:10:31 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2649.409418821335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:18:57 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:28:13 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:36:02 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:44:41 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 02:45:40 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:54:18 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2613.72243642807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:02:30 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:11:37 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:19:11 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:27:59 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 03:28:58 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:37:24 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2585.4743320941925 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:45:36 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:54:48 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:02:34 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:11:21 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 04:12:20 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:20:43 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2602.780789375305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:28:59 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:38:02 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:45:37 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:55:13 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 04:56:12 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:04:43 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2636.933146238327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:12:56 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:22:07 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:29:38 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:38:21 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Fri Apr  5 05:39:20 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:47:43 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2580.083108663559 seconds. 
Discarding model... 

Training complete taking 64954.856469631195 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0233521461486816 seconds. 
Saved predicted values as M_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (1.6322922889402167,), 'R2_train': 0.9920920916833604, 'MAE_train': 0.9887138067919501, 'MSE_test': 54.68514368715554, 'R2_test': 0.6703681302100142, 'MAE_test': 2.9729326159566027}. 
Saved model results as M_Full-Pauli-CRX_results.json. 
