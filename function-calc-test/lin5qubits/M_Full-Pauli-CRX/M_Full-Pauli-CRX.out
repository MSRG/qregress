/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:53 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:44 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:45 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:58:09 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:12 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:10 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2756.1103553771973 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:48 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:12 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:22 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:44:28 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:46 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:50 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2802.942234992981 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:03:32 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:27 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:28 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:30:49 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:52 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:49 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2760.202342271805 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:32 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:59:13 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:07:20 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:16:42 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:45 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:26:59 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2792.281240463257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:36:04 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:53 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:52 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:03:01 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 21:04:04 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:59 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2734.421472787857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:21:42 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:31:21 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:23 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:48:40 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:43 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:34 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2737.419088125229 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:07:15 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:16:46 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:45 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:34:57 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 22:36:00 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:52 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2782.4409692287445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:53:38 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:03:42 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:11:41 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:21:03 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Sun Mar 24 23:22:06 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:10 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2797.2218906879425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:40:16 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:49:57 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:18 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:07:35 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 00:08:38 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:17:27 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2779.7030425071716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:26:35 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:36:07 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:13 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:53:51 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 00:54:55 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:20 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2792.7922184467316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:08 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:22:40 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:44 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:39:55 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:59 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:50:19 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2748.16570353508 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:58:56 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:08:28 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:16:27 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:25:45 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 02:26:48 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:36:02 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2746.578387260437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:44:43 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:54:31 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:02:46 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:12:01 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 03:13:03 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:21:54 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2749.4769208431244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:30:32 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:40:30 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:48:25 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:57:42 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 03:58:52 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:08:21 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2804.0300006866455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:17:16 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:26:51 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:35:05 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:44:18 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 04:45:20 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:54:17 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2739.853036403656 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 05:02:57 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:12:36 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:20:55 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:30:03 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 05:31:16 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:40:07 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2767.268345594406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:49:03 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:58:39 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:06:39 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:16:02 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 06:17:04 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:26:05 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2742.2378334999084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:34:45 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:44:15 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:52:35 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:01:49 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 07:02:53 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:06 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2801.4489369392395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:21:27 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 07:30:57 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:38:52 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:48:04 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 07:49:06 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:57:56 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2707.662001132965 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:06:34 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 08:16:05 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:24:05 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:33:15 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 08:34:17 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:43:19 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2733.3784108161926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:52:08 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:02:19 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:10:19 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:19:45 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 09:20:48 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:30:22 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2812.683729171753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:39:02 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 09:48:35 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:56:30 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:05:41 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 10:06:43 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:15:34 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2712.1982793807983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:24:13 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 10:33:56 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:42:15 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:51:24 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 10:52:27 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:01:24 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2793.2902245521545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 11:10:46 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 11:20:16 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:28:15 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:37:25 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 11:38:27 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:47:22 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2729.407100200653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:56:16 2024]  Iteration number: 0 with current cost as 0.004270761610613883 and parameters 
[-2.88685726  2.23656072 -2.11859771 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58175965  1.14432445
  1.28447194 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 12:05:47 2024]  Iteration number: 0 with current cost as 0.0037848224110330936 and parameters 
[-2.88572082  2.23714662 -2.11811396 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57987812  1.14432445
  1.28239013 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 12:13:44 2024]  Iteration number: 0 with current cost as 0.003556691733120787 and parameters 
[-2.88601715  2.23777278 -2.11839758 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58038219  1.14432445
  1.28293913 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 12:22:52 2024]  Iteration number: 50 with current cost as 0.002416280434664389 and parameters 
[-3.05326402  1.97273019 -1.58675429 -0.11653043  0.55388768 -2.77010852
  3.06858464  2.18960205  1.18551974 -1.0664826   1.01944062  1.14432442
  1.07281087 -1.87354684  0.72964981  2.88578409 -0.54534442 -0.47522471
 -2.02654287  0.72897388  1.60512685  2.83077005 -1.26456769 -0.25136205]. 
Working on 0.8 fold... 
[Mon Mar 25 12:23:54 2024]  Iteration number: 0 with current cost as 0.0029834408767343635 and parameters 
[-2.88850557  2.23726126 -2.11933009 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.58340001  1.14432445
  1.2868968  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:32:44 2024]  Iteration number: 0 with current cost as 0.0033661010038428586 and parameters 
[-2.88508949  2.23697869 -2.11799837 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.57890737  1.14432445
  1.28133583 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2751.3979909420013 seconds. 
Discarding model... 

Training complete taking 69074.61343407631 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9981949329376221 seconds. 
Saved predicted values as M_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (1.6322922889402167,), 'R2_train': 0.9920920916833604, 'MAE_train': 0.9887138067919501, 'MSE_test': 54.68514368715554, 'R2_test': 0.6703681302100142, 'MAE_test': 2.9729326159566027}. 
Saved model results as M_Full-Pauli-CRX_results.json. 
