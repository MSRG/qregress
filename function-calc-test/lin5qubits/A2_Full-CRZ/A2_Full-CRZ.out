/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:06 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:51 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:30 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:28 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:09 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1808.2012350559235 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:05 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:45 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:19 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:20 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1783.8537147045135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:33:54 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:31 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:13 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:06 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:05 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1816.9683265686035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:04:19 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:09:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:27 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:00 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1921.9330246448517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:35 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:51 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:21 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:34 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2013.3518152236938 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:17 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:32 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:20:46 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:30 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 20:37:52 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2063.967921257019 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:41 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:55:42 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:07:25 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2084.983631849289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:19:26 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:24:29 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:29:52 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2046.5633571147919 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:31 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:03 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:22 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 22:21:21 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2080.5201332569122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:28:08 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:24 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:53 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:50:23 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:58 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2081.2829723358154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:24 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:08:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:23 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:43 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:07 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2112.0835893154144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:20 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:34 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:00:17 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:13 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2029.093832731247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:11:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:16:42 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:21:27 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:31:46 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 00:36:37 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1878.7744324207306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:42:47 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:47:36 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:52:20 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:02:33 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 01:07:18 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1841.0990698337555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:42 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:04 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:18 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:35:21 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 01:40:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2022.020659685135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:29 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:59 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:58:08 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:09:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 02:14:30 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2017.0378334522247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:20:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:25:51 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:30:55 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:42:14 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 02:47:30 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1979.3083140850067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:54:12 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:59:17 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:04:24 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:15:33 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 03:21:08 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2016.2588500976562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:27:56 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:33:09 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:38:21 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:49:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 03:54:56 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2036.6617708206177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:01:40 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:06:54 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:12:12 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:23:39 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2011.7740721702576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:34:50 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:39:34 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:44:26 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:55:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 05:00:29 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1914.838247537613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:07:16 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:12:23 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:17:53 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:28:44 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 05:33:50 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1997.0819623470306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:40:27 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:45:40 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:51:04 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:02:27 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 06:07:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2064.4231395721436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:15:02 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:20:36 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:26:15 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:36:45 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 06:41:29 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1990.6748867034912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:47:38 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:52:20 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:57:06 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:07:25 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:16 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.855363368988 seconds. 
Discarding model... 

Training complete taking 49461.614240169525 total seconds. 
Now scoring model... 
Scoring complete taking 2.2118802070617676 seconds. 
Saved predicted values as A2_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.31303010270346,), 'R2_train': -0.2611292656509163, 'MAE_train': 13.780074942135215, 'MSE_test': 122.84974593975717, 'R2_test': 0.25948459257942114, 'MAE_test': 10.149093205702338}. 
Saved model results as A2_Full-CRZ_results.json. 
