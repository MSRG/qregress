/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:06 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:51 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:30 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:28 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 17:58:09 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1808.2012350559235 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:04:05 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:45 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:19 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:20 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1783.8537147045135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:33:54 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:31 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:13 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:06 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:05 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1816.9683265686035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:04:19 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:09:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:27 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:00 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1921.9330246448517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:35 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:51 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:21 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 20:03:34 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2013.3518152236938 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:17 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:32 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:20:46 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:30 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 20:37:52 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2063.967921257019 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:41 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:58 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:55:42 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:07:25 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 21:12:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2084.983631849289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:19:26 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:24:29 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:29:52 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 21:46:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2046.5633571147919 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:53:31 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:59:03 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:04:22 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 22:21:21 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2080.5201332569122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:28:08 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:24 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:53 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:50:23 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:58 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2081.2829723358154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:24 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:08:38 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:23 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:43 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:07 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2112.0835893154144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:20 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:49 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:34 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:00:17 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:13 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2029.093832731247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:11:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:16:42 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:21:27 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:31:46 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 00:36:37 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1878.7744324207306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:42:47 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:47:36 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:52:20 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:02:33 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 01:07:18 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1841.0990698337555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:42 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:04 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:18 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:35:21 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 01:40:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2022.020659685135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:29 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:59 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:58:08 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:09:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 02:14:30 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2017.0378334522247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:20:51 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:25:51 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:30:55 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:42:14 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 02:47:30 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1979.3083140850067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:54:12 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:59:17 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:04:24 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:15:33 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 03:21:08 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2016.2588500976562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:27:56 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:33:09 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:38:21 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:49:53 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 03:54:56 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2036.6617708206177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:01:40 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:06:54 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:12:12 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:23:39 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2011.7740721702576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:34:50 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:39:34 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:44:26 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:55:08 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 05:00:29 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1914.838247537613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:07:16 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:12:23 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:17:53 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:28:44 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 05:33:50 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1997.0819623470306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:40:27 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:45:40 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:51:04 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:02:27 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 06:07:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2064.4231395721436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 06:15:02 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:20:36 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:26:15 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:36:45 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 06:41:29 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1990.6748867034912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:47:38 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:52:20 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:57:06 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:07:25 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Mon Mar 25 07:12:16 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.855363368988 seconds. 
Discarding model... 

Training complete taking 49461.614240169525 total seconds. 
Now scoring model... 
Scoring complete taking 2.2118802070617676 seconds. 
Saved predicted values as A2_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.31303010270346,), 'R2_train': -0.2611292656509163, 'MAE_train': 13.780074942135215, 'MSE_test': 122.84974593975717, 'R2_test': 0.25948459257942114, 'MAE_test': 10.149093205702338}. 
Saved model results as A2_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:49 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:33:00 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:37:37 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 11:42:25 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 11:52:20 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 11:56:59 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1806.4047620296478 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:02:50 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:07:25 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:11:59 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:21:45 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 12:26:17 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1757.1244518756866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:32:05 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:42 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:15 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:51:01 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:30 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1754.698353767395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:01:24 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:55 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:32 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:20:43 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:25 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1797.1094539165497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:31:25 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:36:02 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:40:48 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:50:52 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:28 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1801.2467489242554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:29 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:06:12 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:10:56 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:20:56 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:32 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1802.2834222316742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:31:23 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:35:56 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:40:34 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:50:32 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 14:55:09 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1781.6763231754303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:01:11 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:54 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:10:34 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:20:38 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 15:25:23 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1813.076359987259 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:31:25 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:36:11 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:00 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:51:02 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 15:55:44 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1821.0746195316315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:01:49 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:06:40 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:11:22 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:21:17 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 16:25:58 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1819.4277591705322 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:32:03 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:36:33 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:41:05 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:50:45 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 16:55:16 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1750.260900735855 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:01:06 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:05:39 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:10:12 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:19:54 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 17:24:31 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1754.7976665496826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:30:20 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:34:50 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:39:21 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:49:02 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 17:53:35 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1743.3653633594513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:59:27 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:04:02 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:08:39 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:18:18 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 18:22:49 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1753.1655735969543 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:28:39 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:33:11 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:37:44 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:47:23 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 18:51:55 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1746.7182447910309 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:57:46 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:02:16 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:06:48 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:16:32 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 19:21:03 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1749.0792140960693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:26:56 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:31:30 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:36:00 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:45:40 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 19:50:13 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1747.1962015628815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:56:02 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:00:32 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:05:10 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:14:57 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 20:19:34 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1763.6743187904358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:25:29 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:30:10 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:34:48 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:44:39 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 20:49:18 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1787.9226093292236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:55:15 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:59:52 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:04:50 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:14:58 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 21:19:43 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1825.0269498825073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:25:46 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:30:27 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:35:04 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:45:03 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 21:50:02 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1832.341385602951 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:56:40 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:01:55 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:06:51 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:17:43 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 22:22:42 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1959.3147265911102 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:29:14 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:34:13 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:39:06 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:49:40 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 22:54:36 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1905.6279544830322 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:00:52 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:05:44 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:10:35 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:20:35 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 23:25:10 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1826.330308675766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:31:02 2024]  Iteration number: 0 with current cost as 0.4549975171908689 and parameters 
[-4.26569993  2.23743474 -2.12427942 -0.11653103  0.55388708 -2.77010887
  3.06858488  2.18960156  1.18552009 -1.06648319  0.60271521  1.14432456
  1.31029888 -1.87354669  0.7296507   2.88578409 -0.54534335 -0.47522475
 -2.02654251  0.7289738   1.60512653  2.83077086 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:35:37 2024]  Iteration number: 0 with current cost as 0.3720458631217649 and parameters 
[-4.45269676  2.23743464 -2.12427901 -0.11653103  0.55388739 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:40:14 2024]  Iteration number: 0 with current cost as 0.4011745332900467 and parameters 
[-4.37377066  2.23743464 -2.12427901 -0.11653071  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522454
 -2.02654303  0.7289737   1.60512664  2.83077044 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:50:01 2024]  Iteration number: 0 with current cost as 0.34700585881787127 and parameters 
[-20.14851509   2.23743464  -2.12427964  -0.11653103   0.55388708
  -2.77011412   3.06857984   2.18960145   1.18551998  -1.06648823
   0.6027151    1.14431931   1.31029384  -1.8735468    0.72964051
   2.88578419  -0.54534335  -0.47522485  -2.0265527    0.7289737
   1.60512149   2.83076078  -1.2645671   -0.25136105  -2.39279218
  -2.27309774   3.1333664    2.54856958  -0.67550787  -2.69002716]. 
Working on 1.0 fold... 
[Thu Apr  4 23:54:39 2024]  Iteration number: 0 with current cost as 0.401961966221339 and parameters 
[-4.43042687  2.23743495 -2.12427901 -0.11653071  0.5538877  -2.77010897
  3.06858498  2.18960177  1.1855203  -1.06648308  0.6027151   1.14432476
  1.31029899 -1.87354617  0.7296508   2.88578419 -0.54534335 -0.47522423
 -2.0265424   0.72897401  1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1767.5606634616852 seconds. 
Discarding model... 

Training complete taking 44866.50541496277 total seconds. 
Now scoring model... 
Scoring complete taking 2.0788490772247314 seconds. 
Saved predicted values as A2_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.31303010270346,), 'R2_train': -0.2611292656509163, 'MAE_train': 13.780074942135215, 'MSE_test': 122.84974593975717, 'R2_test': 0.25948459257942114, 'MAE_test': 10.149093205702338}. 
Saved model results as A2_Full-CRZ_results.json. 
