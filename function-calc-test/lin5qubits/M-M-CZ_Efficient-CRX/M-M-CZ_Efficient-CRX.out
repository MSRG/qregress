/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:51:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:55 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:49 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 18:05:11 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 18:09:45 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:55 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1455.9556863307953 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:07 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:08 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 18:29:20 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:05 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:15 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1470.627094745636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:48 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:47 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:36 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:17 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:26 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1497.1467714309692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:08:40 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:36 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:19 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:02 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:18 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1499.392987728119 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:48 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:44 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:17 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:10 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:36 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1516.2860043048859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:58 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:02:17 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 20:09:54 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 20:14:52 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:17 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1541.2189810276031 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:36 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:28:01 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:31 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 20:40:22 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:45:45 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1525.355950832367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:50:14 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:53:18 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:48 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:40 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:07 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1533.486999988556 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:15:33 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:18:42 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 21:26:30 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 21:31:10 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:19 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1500.575130224228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:40:37 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:43:39 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 21:51:01 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:54 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:01:13 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1501.3785195350647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:05:35 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:33 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 22:15:51 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 22:20:33 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:25:53 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1478.1843247413635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:30:05 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:33:12 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 22:40:27 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 22:45:13 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:15 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1460.024218082428 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:29 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:57:31 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:54 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 23:09:26 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:15:00 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1487.647715330124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:19:19 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:22:18 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 23:29:26 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 23:34:07 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:39:32 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1468.2949936389923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:43:55 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:47:02 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Sun Mar 24 23:54:28 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Sun Mar 24 23:59:18 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:04:37 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1503.7579646110535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:08:56 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:11:52 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 00:18:53 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 00:23:39 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:28:42 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1444.5019235610962 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:33:07 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:36:13 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:34 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 00:49:40 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:55:05 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1588.7064983844757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:59:46 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:02:57 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 01:11:00 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 01:16:11 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:21:54 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1607.0736436843872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:26:21 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:29:21 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 01:37:05 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 01:41:49 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:47:10 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1512.8469452857971 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:51:32 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:54:51 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 02:02:32 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 02:07:29 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:12:55 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1547.923389673233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:17:20 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:20:26 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 02:28:06 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:53 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:38:12 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1515.0016458034515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:32 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:45:28 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 02:52:29 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 02:57:08 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:02:16 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1444.7292275428772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:06:42 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:09:38 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 03:16:36 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 03:21:28 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:26:50 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1471.1125519275665 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:31:06 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:34:16 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 03:41:33 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 03:46:26 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:51:41 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1498.7340924739838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:56:05 2024]  Iteration number: 0 with current cost as 0.5502979015063023 and parameters 
[-1.74536134  2.23743509 -2.12427918 -0.1165308   0.55388753 -2.77010897
  3.06858521  2.1896019   1.18552044 -1.06648308  0.60271555  1.14432445
  1.31029944 -1.8735459 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:59:09 2024]  Iteration number: 0 with current cost as 0.49338824913653323 and parameters 
[-1.50129253  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Mon Mar 25 04:06:31 2024]  Iteration number: 0 with current cost as 0.19763133987422127 and parameters 
[64.41734844  2.23743464 -2.12425925 -0.11653103  0.55389727 -2.77011916
  3.06857479  2.18960145  1.18554037 -1.06648308  0.6027151   1.14433464
  1.31030918 -1.87352642]. 
Working on 0.8 fold... 
[Mon Mar 25 04:11:05 2024]  Iteration number: 0 with current cost as 0.20134480373755742 and parameters 
[ 1.53284566  2.23743464 -2.12427889 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552073 -1.06648383  0.6027151   1.14432595
  1.31029899 -1.8735453 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:16:08 2024]  Iteration number: 0 with current cost as 0.5086763804480221 and parameters 
[-1.6112105   2.23743464 -2.1242793  -0.11653086  0.55388725 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648308  0.6027151   1.14432462
  1.31029915 -1.8735463 ]. 
Training complete taking 1455.7144536972046 seconds. 
Discarding model... 

Training complete taking 37525.678414821625 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.6208386421203613 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (322.73150772633045,), 'R2_train': -0.5635258411026933, 'MAE_train': 16.553766459775527, 'MSE_test': 301.15155936264574, 'R2_test': -0.8152855585564676, 'MAE_test': 16.304477348468524}. 
Saved model results as M-M-CZ_Efficient-CRX_results.json. 
