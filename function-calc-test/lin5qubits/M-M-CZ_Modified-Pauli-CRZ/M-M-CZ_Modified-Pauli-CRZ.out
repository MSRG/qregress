/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:58 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:09 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:20 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:04 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:02 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:38:59 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 569.8514566421509 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:40:36 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:45 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:29 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:26 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:48:25 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.2972173690796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:01 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:12 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:57 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:54 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:57:51 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9429836273193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:59:28 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:36 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:24 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:20 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:17 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.7577078342438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:54 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:02 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:49 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:46 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:42 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 564.8849833011627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:18:19 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:19:27 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:22:13 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:17 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:16 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 574.105190038681 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:01 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:52 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:49 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:35:46 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 570.5653579235077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:24 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:38:31 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:17 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:15 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:10 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 564.1418445110321 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:46:49 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:43 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:39 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:37 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.2634656429291 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:16 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:23 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:00:09 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:06 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:03 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9270906448364 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:40 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:49 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:35 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:32 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:30 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.9111757278442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:07 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:22 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:36 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:40 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:38 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 608.3236541748047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:16 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:24 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:29:18 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:15 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:11 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 573.6140305995941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:49 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:57 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:41 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:40:37 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:33 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 561.8464553356171 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:44:10 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:45:19 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:48:01 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:57 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:54 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 559.8177235126495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:53:33 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:54:40 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:26 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:59:22 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:18 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 563.0157911777496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:02:54 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:04:04 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:06:53 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:08:48 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:44 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 567.7763066291809 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:23 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:13:32 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:16:17 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:12 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:10 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 565.9484887123108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:47 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:22:55 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:25:41 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:27:37 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:42 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 571.5717668533325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:31:20 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:32:28 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:32 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:37:29 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:25 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 583.6083180904388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:02 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:42:09 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:44:53 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:46:49 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:48:44 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 558.5927033424377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:50:21 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:51:30 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:15 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:56:11 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:08 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 563.8993420600891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:44 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:00:53 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:03:38 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:05:36 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:32 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 577.5634868144989 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:23 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:10:34 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:13:19 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:15:15 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:12 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 566.6477990150452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:48 2024]  Iteration number: 0 with current cost as 0.5502979012576306 and parameters 
[-1.74536127  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552044 -1.06648331  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965035  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:19:56 2024]  Iteration number: 0 with current cost as 0.4933882541109627 and parameters 
[-1.50129222  2.23743464 -2.12427964 -0.11653071  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354664  0.72965049  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:40 2024]  Iteration number: 0 with current cost as 0.19763134090932857 and parameters 
[64.41734463  2.23744483 -2.12428983 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18554037 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87353661  0.72963042  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:37 2024]  Iteration number: 0 with current cost as 0.20134480339841582 and parameters 
[ 1.53284567  2.23743464 -2.12427964 -0.11652953  0.55388708 -2.77010897
  3.06858498  2.1896022   1.18552224 -1.06648383  0.60271585  1.14432445
  1.31029899 -1.87354605  0.7296493   2.88578344 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:33 2024]  Iteration number: 0 with current cost as 0.5086763801217657 and parameters 
[-1.61121048  2.23743464 -2.12427964 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552032 -1.06648325  0.60271527  1.14432445
  1.31029899 -1.87354663  0.72965047  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 561.1511700153351 seconds. 
Discarding model... 

Training complete taking 14224.026687860489 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0571906566619873 seconds. 
Saved predicted values as M-M-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (322.73150773416654,), 'R2_train': -0.5635258411406567, 'MAE_train': 16.55376646011674, 'MSE_test': 301.1515605306635, 'R2_test': -0.8152855655970617, 'MAE_test': 16.304477363287884}. 
Saved model results as M-M-CZ_Modified-Pauli-CRZ_results.json. 
