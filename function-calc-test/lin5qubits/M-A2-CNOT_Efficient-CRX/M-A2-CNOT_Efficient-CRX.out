/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:16 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:54 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 17:38:13 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:08 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:44:55 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 17:48:12 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1281.9286816120148 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:21 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:04 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:47 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:06:19 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:37 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1282.7033054828644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:14:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 18:21:11 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:10 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:58 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:16 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1289.2172167301178 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:06 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:35 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:08 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:48:29 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 18:51:22 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1187.2416157722473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:48 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:45 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:15 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:07:41 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:58 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1184.954306602478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:41 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 19:22:04 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:25:00 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:28:26 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:39 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1245.265508890152 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:28 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:00 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:06 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:57 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:36 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1337.2289896011353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:39 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 20:05:50 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:59 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:15 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1356.1739711761475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:18 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:37 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:30:09 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:33:27 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:25 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1178.7879297733307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:40:48 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:40 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:49:12 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:26 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:24 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1134.1988060474396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:43 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 21:05:38 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:08:17 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:11:38 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 21:14:34 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1157.5144186019897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:18:59 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 21:24:53 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:27:24 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:37 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 21:33:35 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1146.5197117328644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:38:08 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 21:44:05 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:43 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:50:01 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 21:53:01 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1153.6685671806335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:57:24 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 22:03:20 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:51 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:09:04 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 22:11:56 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1139.4303472042084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:16:20 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 22:22:13 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:24:47 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:05 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:11 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1155.8406028747559 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:35:35 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:28 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:44:00 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:47:13 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:11 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1140.4523522853851 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:35 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 23:00:26 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:02:59 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:06:19 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 23:09:11 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1140.8918406963348 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:13:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:28 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:05 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:24 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 23:28:17 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1147.7312796115875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:32:53 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:40 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:45 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:39 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Sun Mar 24 23:50:11 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1353.0084207057953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:55:37 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:45 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:04:19 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:07:29 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Mon Mar 25 00:10:31 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1204.575787305832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:15:40 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Mon Mar 25 00:22:28 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:25:11 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:28:33 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Mon Mar 25 00:31:42 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1259.8094718456268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:36:22 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Mon Mar 25 00:42:05 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:37 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:47:58 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Mon Mar 25 00:50:49 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1129.6531248092651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:55:09 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Mon Mar 25 01:01:01 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:33 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:06:48 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Mon Mar 25 01:09:40 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1128.6638507843018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:13:55 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Mon Mar 25 01:19:37 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:22:11 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:25:29 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Mon Mar 25 01:28:23 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1126.9604671001434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:32:46 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Mon Mar 25 01:38:36 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:41:13 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:44:32 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Mon Mar 25 01:47:27 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1140.5864429473877 seconds. 
Discarding model... 

Training complete taking 30003.0084335804 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.347212791442871 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (147.90831846241775,), 'R2_train': 0.2834338374357328, 'MAE_train': 10.976254788628937, 'MSE_test': 167.36739699251638, 'R2_test': -0.008859523678718784, 'MAE_test': 10.950798128580812}. 
Saved model results as M-A2-CNOT_Efficient-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:42:46 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:44:21 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 11:50:20 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:52:56 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:56:16 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 11:59:11 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1161.642575263977 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:03:36 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 12:09:27 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:03 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:15:25 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 12:18:31 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1171.1428129673004 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:23:08 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 12:29:21 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:32:07 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:35:33 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 12:38:47 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1216.1172633171082 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:43:31 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:42 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:25 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:55:53 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 12:59:03 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1216.9337258338928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:03:43 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:51 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:31 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:59 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 13:19:06 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1202.8026766777039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:23:44 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 13:29:59 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:35 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:36:01 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 13:39:08 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1201.8421833515167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:45 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 13:49:54 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:37 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:56:02 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:18 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1210.0537469387054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:02 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 14:10:24 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:13:09 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:16:43 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 14:19:46 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1229.587099313736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:24:34 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 14:31:25 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:08 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:42 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:43 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1252.8206098079681 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:22 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 14:51:41 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:54:21 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:58:13 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 15:01:23 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1254.7194888591766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:06:17 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 15:12:46 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:15:26 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:18:54 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 15:22:01 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1243.2708549499512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:57 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 15:33:46 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:36:43 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:40:17 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 15:43:38 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1305.399828672409 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:48:57 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 15:55:37 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:58:36 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:01:49 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 16:04:48 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1234.3568983078003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:09:15 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 16:15:12 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:18:11 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:21:58 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 16:25:22 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1263.13818359375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:30:42 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 16:37:04 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:39:42 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:43:12 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 16:46:42 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1264.5446200370789 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:51:21 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 16:58:01 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:00:55 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:04:37 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 17:08:01 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1292.1377549171448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:12:57 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 17:19:18 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:22:06 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:25:38 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:42 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1234.6486024856567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:33:32 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 17:39:23 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:42:06 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:45:43 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 17:49:07 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1222.6754157543182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:54:04 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 18:00:23 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:03:22 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:07:08 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 18:10:53 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1313.0106649398804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:15:55 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 18:22:28 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:25:27 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:29:07 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 18:32:30 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1287.3496787548065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:37:23 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 18:43:36 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:46:24 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:49:53 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 18:53:07 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1238.865734577179 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:58:04 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 19:04:36 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:07:35 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:11:23 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 19:14:43 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1298.2065434455872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:19:47 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 19:26:54 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:29:59 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:33:40 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 19:36:58 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1349.5933141708374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:42:05 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 19:48:57 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:51:55 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:55:49 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 19:59:15 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1314.9180128574371 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:04:00 2024]  Iteration number: 0 with current cost as 0.4473167478894924 and parameters 
[-1.26342666  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.06648308  0.60271542  1.14432445
  1.3102993  -1.87354649]. 
Working on 0.4 fold... 
[Thu Apr  4 20:10:46 2024]  Iteration number: 0 with current cost as 0.23225510179034786 and parameters 
[ 1.3123409   2.23743484 -2.12427923 -0.11653083  0.55388708 -2.77010897
  3.06858498  2.18960165  1.18552019 -1.06648288  0.60271571  1.14432465
  1.31029939 -1.8735462 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:13:47 2024]  Iteration number: 0 with current cost as 0.4080225492797647 and parameters 
[-1.00216453  2.23743464 -2.12427951 -0.11653103  0.5538872  -2.77010897
  3.06858498  2.18960145  1.18552024 -1.06648308  0.60271535  1.14432445
  1.31029924 -1.8735463 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:17:36 2024]  Iteration number: 0 with current cost as 0.4674263047693248 and parameters 
[-0.30763448  2.23743479 -2.12427932 -0.11653087  0.55388724 -2.77010897
  3.06858498  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 1.0 fold... 
[Thu Apr  4 20:21:10 2024]  Iteration number: 0 with current cost as 0.4897581349776682 and parameters 
[ 0.07572605  2.23743448 -2.12427964 -0.11653118  0.55388708 -2.77010944
  3.06858483  2.1896013   1.18551983 -1.06648324  0.6027151   1.14432429
  1.31029914 -1.87354664]. 
Training complete taking 1328.0773301124573 seconds. 
Discarding model... 

Training complete taking 31307.857481241226 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.1420438289642334 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (147.90831846241775,), 'R2_train': 0.2834338374357328, 'MAE_train': 10.976254788628937, 'MSE_test': 167.36739699251638, 'R2_test': -0.008859523678718784, 'MAE_test': 10.950798128580812}. 
Saved model results as M-A2-CNOT_Efficient-CRX_results.json. 
