/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:51:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:09 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:11 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:27 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:08:42 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:24 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1645.1138181686401 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:35 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 18:24:39 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:55 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:07 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:07 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1667.1411638259888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:47:23 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:36 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:52 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:08 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:10:09 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1681.1858024597168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:24 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 19:20:29 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:45 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 19:31:59 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:37:36 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1644.2824251651764 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:42:46 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 19:47:46 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:01 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 19:59:17 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:05:08 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1654.1290595531464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:21 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:20 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:36 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:49 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:32:24 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1635.8730907440186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:37:38 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:42:42 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:49:01 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 20:54:13 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:59:49 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1649.6881623268127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:05:08 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 21:10:26 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:16:38 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:21:49 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:27:25 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1647.460372209549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:37 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 21:37:39 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:43:53 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:49:20 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:55:05 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1660.204473733902 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:15 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 22:05:17 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:11:28 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 22:17:00 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:22:45 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1664.4564907550812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:27:58 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 22:32:59 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:44 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 22:45:13 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:46 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1676.6023926734924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:55 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:01:38 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:55 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 23:13:04 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:18:42 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1679.6254920959473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:23:55 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:28:55 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:17 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 23:41:45 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:47:22 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1718.4079141616821 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:33 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:38 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:04:04 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:09:13 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:14:51 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1646.4797194004059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:59 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 00:25:01 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:31:37 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:36:52 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:42:28 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1658.476530790329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:47:38 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 00:52:39 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:59:24 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 01:05:08 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:10:45 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1698.2633244991302 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:15:58 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 01:20:58 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:27:24 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 01:32:33 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:38:09 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1641.1046793460846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:43:19 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 01:48:40 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:54:57 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:00:07 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:05:52 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1667.580280303955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:11:05 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:16:34 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:22:58 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:28:49 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:34:27 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1715.3539237976074 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:39:52 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:45:22 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:51:47 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:57:09 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:02:44 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1700.9603333473206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:08:03 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 03:13:05 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:19:14 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:24:23 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:29:57 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1622.7064607143402 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:35:05 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 03:40:40 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:46:57 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:52:21 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:57:59 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1683.129597902298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:03:08 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 04:08:08 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:14:21 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 04:19:34 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:25:22 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1646.939654827118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:30:38 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 04:35:38 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:41:58 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 04:47:07 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:52:45 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1657.8628981113434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:58:13 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:03:21 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 05:09:32 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 05:14:42 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:20:17 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1633.8865838050842 seconds. 
Discarding model... 

Training complete taking 41596.91614770889 total seconds. 
Now scoring model... 
Scoring complete taking 0.9254577159881592 seconds. 
Saved predicted values as IQP_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (1.5598635980506006,), 'R2_train': 0.992442984382499, 'MAE_train': 0.9120310137789375, 'MSE_test': 1.4367238306514787, 'R2_test': 0.9913396961087128, 'MAE_test': 1.027332912676595}. 
Saved model results as IQP_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:27:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:17 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 12:32:27 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:35 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:38 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:17 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1624.354009628296 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:21 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 12:59:23 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:05:28 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 13:10:31 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:16:02 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1603.1996037960052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:21:06 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:03 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:08 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 13:37:26 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:43:14 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1631.903300523758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:48:16 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 13:53:11 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:15 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 14:04:27 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:56 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1602.9504821300507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:15:00 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 14:20:12 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:26:32 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:15 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:39 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1660.7600469589233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:42:39 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 14:47:30 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:53:31 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 14:59:05 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:04:51 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1630.2397499084473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:09:50 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 15:14:44 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:20:48 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 15:25:51 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:31:19 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1590.0947864055634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:36:20 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 15:41:13 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:47:55 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 15:52:55 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:58:49 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1652.0569367408752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:03:52 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 16:08:43 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:14:46 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 16:19:55 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:25:37 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1605.8590247631073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:30:39 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 16:36:17 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:42:19 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 16:47:24 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:52:56 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1637.8237652778625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:57:55 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:02:47 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:08:46 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 17:13:45 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:19:11 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1575.5854380130768 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:24:12 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:29:05 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:35:05 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 17:40:05 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:45:31 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1579.6203744411469 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:50:31 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:55:23 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:01:22 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 18:06:22 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:11:46 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1574.7315583229065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:16:45 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 18:21:37 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:28:20 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 18:33:20 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:38:44 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1618.8585329055786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:43:44 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 18:48:35 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:54:39 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 18:59:39 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:05:04 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1578.5502231121063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:10:03 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 19:14:54 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:21:00 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 19:26:30 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:31:56 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1616.3856670856476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:36:59 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 19:41:54 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:47:54 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 19:53:05 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:58:35 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1602.4890246391296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:03:42 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 20:08:34 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:14:34 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 20:19:34 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:25:10 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1602.804095506668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:30:26 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 20:35:18 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:41:17 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 20:46:18 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:51:44 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1578.9277231693268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:56:43 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:01:35 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 21:07:37 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 21:12:37 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:18:07 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1582.5956423282623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:23:06 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:27:59 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 21:34:00 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 21:39:00 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:44:29 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1581.6280074119568 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:49:29 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:54:23 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:22 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:05:31 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:10:57 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1589.8134062290192 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:15:58 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:20:49 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:26:48 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:31:48 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:37:14 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1596.789030790329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:42:36 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:47:28 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:53:38 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:58:39 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:04:06 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1591.8648421764374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:09:06 2024]  Iteration number: 0 with current cost as 0.17442738792755585 and parameters 
[-3.07403586  3.12219182 -2.18819407 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.899949    1.14432446
  1.3424685  -1.87354679  0.72965081  2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:14:11 2024]  Iteration number: 0 with current cost as 0.14788407628334665 and parameters 
[-3.00980399  2.99646303 -2.16659244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.90641854  1.14432445
  1.29369378 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 23:20:12 2024]  Iteration number: 0 with current cost as 0.15883234484016473 and parameters 
[-3.03205001  3.05401774 -2.17466601 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648309  0.9109994   1.14432446
  1.30808581 -1.8735468   0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:12 2024]  Iteration number: 0 with current cost as 0.158048241501924 and parameters 
[-3.03507008  3.05234091 -2.1760801  -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.90461388  1.14432445
  1.31254407 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:30:48 2024]  Iteration number: 0 with current cost as 0.16022397086296064 and parameters 
[-3.03024556  3.04643272 -2.17354124 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.91022689  1.14432446
  1.30732548 -1.8735468   0.7296508   2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1607.0125646591187 seconds. 
Discarding model... 

Training complete taking 40116.89909887314 total seconds. 
Now scoring model... 
Scoring complete taking 0.9102988243103027 seconds. 
Saved predicted values as IQP_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (1.5598635980506006,), 'R2_train': 0.992442984382499, 'MAE_train': 0.9120310137789375, 'MSE_test': 1.4367238306514787, 'R2_test': 0.9913396961087128, 'MAE_test': 1.027332912676595}. 
Saved model results as IQP_Full-Pauli-CRZ_results.json. 
