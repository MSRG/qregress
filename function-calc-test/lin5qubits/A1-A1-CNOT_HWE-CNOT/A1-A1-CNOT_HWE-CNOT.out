/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:14 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:25 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:51 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 17:35:28 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:02 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 17:40:22 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 749.123302936554 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:42:54 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 17:45:08 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:44 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:20 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 17:52:51 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 748.1846055984497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:35 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:10 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:02:44 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:10 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 741.3262059688568 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:41 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:55 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:30 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:04 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:26 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 740.9649157524109 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:20:02 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:14 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:53 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:28 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:48 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 734.0252149105072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:32:16 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:28 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:03 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:36 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:41:56 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 736.6628968715668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:33 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:46:46 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:23 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:07 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:30 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 743.2676813602448 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:57:04 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 18:59:15 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:47 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:28 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:06:50 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 741.8426342010498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:18 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:30 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:05 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:40 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:19:01 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 736.6300342082977 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:35 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:46 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:26:20 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:28:55 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:15 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 729.7369315624237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:44 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:57 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:31 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:41:08 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:43:34 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 740.1921317577362 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:46:05 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 19:48:17 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 19:50:51 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:35 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:00 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 743.3097388744354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:29 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:41 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:03:18 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:51 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:11 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 732.4910378456116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:40 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:12:52 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:15:35 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:12 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:32 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 740.2678344249725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:23:02 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:25:13 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:28:07 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:30:42 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:33:03 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 749.9501864910126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:32 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:37:59 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:40:33 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:10 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:45:36 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 753.4868009090424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:48:06 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 20:50:21 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 20:53:01 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:35 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:56 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 741.777538061142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:00:26 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:02:57 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:05:31 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:08:04 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:10:24 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 747.7481105327606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:54 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:15:07 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:17:45 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:20:22 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:43 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 739.4312410354614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:25:13 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:24 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:11 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:32:45 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:35:10 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 747.5410449504852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:37:41 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:55 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:31 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:05 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:47:24 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 735.4478914737701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:49:56 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 21:52:09 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 21:54:43 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 21:57:18 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 21:59:40 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 738.5717318058014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:02:15 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:50 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:25 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 22:10:00 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 22:12:21 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 757.4820418357849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:52 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 22:17:04 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:43 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:17 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 22:24:41 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 736.76451420784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:27:10 2024]  Iteration number: 0 with current cost as 0.17395685233024477 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.03787391  0.58658104 -2.62572058
  3.03473395  2.16439753  1.25823118 -1.06953788  0.64144427  1.15272522
  1.37714082 -1.81484778  0.71559942]. 
Working on 0.4 fold... 
[Sun Mar 24 22:29:22 2024]  Iteration number: 0 with current cost as 0.1335791266521221 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06541265  0.58115123 -2.6667207
  3.04127202  2.17025483  1.24362412 -1.06938598  0.62880195  1.1489569
  1.36295953 -1.82456036  0.72839363]. 
Working on 0.6 fold... 
[Sun Mar 24 22:32:04 2024]  Iteration number: 0 with current cost as 0.14756195771492825 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05535157  0.58317736 -2.65167431
  3.03756892  2.16610963  1.25236878 -1.06966166  0.6367854   1.15111834
  1.36682789 -1.82286503  0.72148814]. 
Working on 0.8 fold... 
[Sun Mar 24 22:35:08 2024]  Iteration number: 0 with current cost as 0.07504599153998551 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.21867354  0.68860824 -2.16209864
  2.91077362  2.08769239  1.51561641 -1.07817476  0.77252731  1.1832251
  1.61004309 -1.60968458  0.66890713]. 
Working on 1.0 fold... 
[Sun Mar 24 22:37:28 2024]  Iteration number: 0 with current cost as 0.1492660672200189 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.05762623  0.58333495 -2.65409866
  3.03843004  2.16713351  1.25030115 -1.06943148  0.63464857  1.15072966
  1.36663952 -1.82225953  0.7242876 ]. 
Training complete taking 768.8213634490967 seconds. 
Discarding model... 

Training complete taking 18575.050572395325 total seconds. 
Now scoring model... 
Scoring complete taking 0.8307538032531738 seconds. 
Saved predicted values as A1-A1-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (27.200879039770523,), 'R2_train': 0.8682208701003133, 'MAE_train': 3.8423229693369594, 'MSE_test': 43.55784236869199, 'R2_test': 0.737441431878658, 'MAE_test': 4.960638105133766}. 
Saved model results as A1-A1-CNOT_HWE-CNOT_results.json. 
