/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:48:24 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:30 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:36 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:04 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 17:56:54 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:28 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 709.0741021633148 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:19 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 18:04:13 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:42 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:08:34 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:20 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 716.8312227725983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:13:14 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:09 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:31 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:22 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 18:23:04 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 705.0661044120789 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:01 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:03 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:29 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:32:27 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 18:35:20 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 733.4417443275452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:37:15 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:18 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:51 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:47 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:34 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 735.5966622829437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:29 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:28 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:56 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 18:56:48 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:27 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 710.7429158687592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:20 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 19:04:23 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:51 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:45 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:23 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 716.0664443969727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:13:13 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:12 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 19:18:39 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:20:30 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:09 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 704.8051178455353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:02 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:53 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:22 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:24 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:02 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 713.7664179801941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:53 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 19:39:45 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:08 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:00 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:39 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 696.7975101470947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:30 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:28 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 19:53:54 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:44 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 19:58:28 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 709.052906036377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:00:18 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:15 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:35 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:31 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:17 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 711.231271982193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:11 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:06 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:32 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:19:28 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 20:22:09 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 709.5277659893036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:00 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 20:27:00 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:23 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:31:13 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 20:33:48 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 698.0990800857544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:39 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 20:38:31 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 20:40:51 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:42:48 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 20:45:23 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 697.6035583019257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:15 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 20:50:13 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:43 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 20:54:34 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 20:57:16 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 710.7699415683746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:08 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 21:02:05 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:27 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:06:25 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 21:09:07 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 711.1391060352325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:05 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 21:14:05 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 21:16:33 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:18:22 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:59 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 715.9232139587402 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:22:53 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 21:25:51 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 21:28:24 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:13 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 21:33:01 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 720.7603397369385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:34:52 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 21:37:47 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 21:40:08 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:42:01 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 21:45:03 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 730.3086133003235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:47:08 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 21:50:20 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:54 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:03 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:57 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 768.2528781890869 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:59:54 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:51 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:20 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 22:07:09 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 22:09:51 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 710.467517375946 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:11:53 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 22:15:06 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 22:17:34 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 22:19:32 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 22:22:26 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 758.818995475769 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:24:26 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 22:27:21 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 22:29:49 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 22:31:41 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 22:34:33 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 723.4605867862701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:36:28 2024]  Iteration number: 0 with current cost as 0.16376805080939577 and parameters 
[-4.75191161  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.1855203  -1.06648324]. 
Working on 0.4 fold... 
[Sun Mar 24 22:39:30 2024]  Iteration number: 0 with current cost as 0.13475420080293288 and parameters 
[-4.70194011  2.23743464 -2.12427921 -0.11653082  0.55388687 -2.77010897
  3.06858477  2.18960166  1.18551998 -1.06648351]. 
Working on 0.6 fold... 
[Sun Mar 24 22:41:58 2024]  Iteration number: 0 with current cost as 0.13247481384283172 and parameters 
[-4.68302493  2.23743496 -2.12427931 -0.11653086  0.55388708 -2.77010897
  3.06858498  2.18960161  1.18552031 -1.06648308]. 
Working on 0.8 fold... 
[Sun Mar 24 22:43:53 2024]  Iteration number: 0 with current cost as 0.1391343694063561 and parameters 
[-4.74156352  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010913
  3.06858498  2.18960161  1.18552014 -1.06648324]. 
Working on 1.0 fold... 
[Sun Mar 24 22:46:36 2024]  Iteration number: 0 with current cost as 0.14087453157632995 and parameters 
[-4.68373083  2.23743503 -2.12427924 -0.11653063  0.55388728 -2.77010897
  3.06858498  2.18960185  1.18552018 -1.06648328]. 
Training complete taking 723.4156844615936 seconds. 
Discarding model... 

Training complete taking 17941.02046442032 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.4715864658355713 seconds. 
Saved predicted values as M-M-CZ_ESU2_predicted_values.csv
Model scores: {'MSE_train': (88.70140548420292,), 'R2_train': 0.570271460032706, 'MAE_train': 6.503037374154746, 'MSE_test': 98.20071409688933, 'R2_test': 0.408064369590885, 'MAE_test': 6.3049761511798055}. 
Saved model results as M-M-CZ_ESU2_results.json. 
