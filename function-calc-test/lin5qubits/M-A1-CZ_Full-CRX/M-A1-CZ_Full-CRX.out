/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:00 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:38:51 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:33 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:15 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:55:57 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:39 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1713.8114485740662 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:23 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:04 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:49 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:35 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:18 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1717.1739716529846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:03 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:41:49 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:47:30 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:12 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:48 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1710.8120477199554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:04:29 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:10 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:15:52 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:30 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:12 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1703.2376594543457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:32:54 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:17 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:50:01 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1710.529131412506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:24 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:07:05 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:12:46 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:18:36 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:18 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1717.9258215427399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:05 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:48 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:30 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:47:15 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:59 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1718.2109291553497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:43 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:04:30 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:10:14 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:16:01 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:21:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1726.1392328739166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:27:30 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:15 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:03 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:44:47 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:50:43 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1740.8994324207306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:56:30 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:02:16 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:07:58 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:13:43 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:19:23 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1719.8662219047546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:13 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:31:01 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:36:47 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:35 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:48:19 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1735.1731595993042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:54:04 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:59:51 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:05:35 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:18 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:02 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1723.8583071231842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:22:49 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:28:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:34:24 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:40:14 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:46:08 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1745.680858373642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:51:59 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:46 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:33 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:09:24 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:15:08 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1739.3424365520477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:20:54 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:45 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:32:28 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:13 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:43:59 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1733.4163126945496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:49:46 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:55:36 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:01:26 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:20 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:13:04 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1742.0111002922058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:18:50 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:31 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:30:15 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:36:05 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:53 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1730.2858335971832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:42 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:53:32 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:15 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:05:10 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:57 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1742.3031151294708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:16:41 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:22:27 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:28:24 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:34:17 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:40:14 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1760.147672176361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:46:08 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:52:06 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:58:00 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:03:52 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:09:53 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1786.9935791492462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:15:55 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:21:54 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:27:51 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:33:37 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:39:25 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1765.2162370681763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:45:12 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:51:03 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:56:50 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:02:33 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:08:17 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1728.7434570789337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:14:07 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:19:52 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:25:33 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:31:16 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:36:57 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1721.5632882118225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:42:44 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:48:32 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:54:26 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:00:10 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:05:55 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1737.5965354442596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:11:40 2024]  Iteration number: 0 with current cost as 0.10822095887456029 and parameters 
[-4.66319339  2.23743464 -2.12427957 -0.11653096  0.55388701 -2.77010904
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432452
  1.31029899 -1.87354673  0.72965073  2.88578412 -0.54534342 -0.47522478
 -2.02654248  0.72897362  1.60512671  2.830771   -1.26456703 -0.25136098
 -2.39279211 -2.27309767  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:17:22 2024]  Iteration number: 0 with current cost as 0.09061534838173133 and parameters 
[-4.6900951   2.23743481 -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432454
  1.31029907 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.7289737   1.60512664  2.83077099 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:23:07 2024]  Iteration number: 0 with current cost as 0.08455690777284282 and parameters 
[-4.66452267  2.23743464 -2.12427949 -0.11653103  0.553887   -2.77010912
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578412 -0.54534343 -0.47522485
 -2.02654248  0.72897362  1.60512656  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:28:49 2024]  Iteration number: 0 with current cost as 0.08874339146370301 and parameters 
[-4.68039068  2.23743464 -2.12427935 -0.11653088  0.55388715 -2.7701089
  3.06858505  2.18960152  1.18552006 -1.06648308  0.60271532  1.1443246
  1.31029913 -1.87354666  0.72965095  2.88578419 -0.54534335 -0.47522471
 -2.0265424   0.7289737   1.60512671  2.83077107 -1.2645671  -0.25136097
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:34:33 2024]  Iteration number: 0 with current cost as 0.09587137153921405 and parameters 
[-4.6746318   2.2374348  -2.12427938 -0.11653086  0.55388716 -2.77010897
  3.06858498  2.18960145  1.18552007 -1.06648308  0.60271527  1.14432453
  1.31029907 -1.87354672  0.72965089  2.88578419 -0.54534335 -0.47522477
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136096
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550779 -2.69002202]. 
Training complete taking 1715.629340171814 seconds. 
Discarding model... 

Training complete taking 43286.567755937576 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.1562013626098633 seconds. 
Saved predicted values as M-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (58.35083788403513,), 'R2_train': 0.7173097739218985, 'MAE_train': 5.013475892245843, 'MSE_test': 77.63284334844319, 'R2_test': 0.5320436669882785, 'MAE_test': 5.13789221227575}. 
Saved model results as M-A1-CZ_Full-CRX_results.json. 
