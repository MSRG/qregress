/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:38 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:41 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:40:51 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:43 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:21 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1477.1156332492828 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:16 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:17 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:05:26 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:10:10 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:51 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.6460330486298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:44 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:10 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:19 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:07 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:40 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1496.4805598258972 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:41 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:42 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:55 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:46 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:21 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1474.5413057804108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:15 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:23 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:33 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:22 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:28:54 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1472.2240748405457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:47 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:46 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:56 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:40 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:11 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1456.5756347179413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:03 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:18 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:29 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:13:18 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:49 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1502.6907165050507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:23:06 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:28:12 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:16 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:01 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:42:36 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1461.170149564743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:31 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:52:33 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:07 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:53 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:25 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1487.6871025562286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:15 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:13 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:17 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:27:02 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:31:32 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1450.5172560214996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:27 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:25 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:31 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:38 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:08 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1474.7324748039246 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:00 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:22 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:11:29 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:18 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:20:53 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1482.5833897590637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:44 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:35:57 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:40:44 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:16 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.7369384765625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:50:12 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:55:12 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:17 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:05:02 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:09:33 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1450.8563158512115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:14:23 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:20 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:24:35 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:22 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:58 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.2552361488342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:54 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:58 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:09 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:53:58 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:58:30 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.265961408615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:03:22 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:08:20 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:13:25 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:18:10 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:22:43 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1452.759045124054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:36 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:32:33 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:37:42 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:34 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:06 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1463.1258373260498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:51:59 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:11 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:02:32 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:15 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:11:52 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1500.240728378296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:16:59 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:22:11 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:27:15 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:32:09 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:36:41 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1473.507125377655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:31 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:30 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:51:36 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:56:27 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:00:58 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1460.2397718429565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:05:53 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:11:04 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:16:25 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:21:23 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:25:53 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1494.8552300930023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:30:48 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:35:47 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:41:10 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:45:55 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:33 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1483.4385809898376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:55:30 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:00:28 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:05:33 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:10:30 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:15:02 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1461.3424079418182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:19:52 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:24:50 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:30:00 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:34:57 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:39:27 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1468.442435503006 seconds. 
Discarding model... 

Training complete taking 36823.03146696091 total seconds. 
Now scoring model... 
Scoring complete taking 0.9038832187652588 seconds. 
Saved predicted values as A1_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (10.999933579939112,), 'R2_train': 0.9467090135580063, 'MAE_train': 2.3939758761749914, 'MSE_test': 11.47043450909876, 'R2_test': 0.9308583553118488, 'MAE_test': 2.338046330040941}. 
Saved model results as A1_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:42 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:14 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:42:26 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:47:46 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:52:42 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:57:25 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1521.4903810024261 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:02:33 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:07:45 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:13:09 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:18:08 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:22:50 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1522.7669410705566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:55 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:33:06 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:22 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:43:19 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:48:08 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1519.2143206596375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:53:14 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:58:35 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:03:54 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:08:51 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:13:30 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1521.61164188385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:18:35 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:24:00 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:29:16 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:15 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:38:54 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1518.0992510318756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:54 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:49:17 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:54:32 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:59:26 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:04:10 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1522.9165875911713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:09:17 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:14:43 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:20:09 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:03 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:29:45 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1530.0418629646301 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:34:47 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:39:55 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:45:10 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:50:03 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:54:45 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1510.7411184310913 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:59:57 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:05:06 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:10:44 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:16:08 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:20:55 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1559.9492835998535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:25:57 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:31:05 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:36:50 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:41:44 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:46:40 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1545.6071190834045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:51:43 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:56:50 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:02:14 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:07:07 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:11:48 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1515.1510136127472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:16:58 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:22:09 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:27:28 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:32:22 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:37:05 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1507.9974038600922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:42:06 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:47:14 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:30 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:57:27 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:02:09 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1505.7693502902985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:07:12 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:12:34 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:18:39 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:23:36 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:15 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1566.562111377716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:33:18 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:38:26 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:43:40 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:48:32 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:53:12 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1511.3756732940674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:58:32 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:03:41 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:08:57 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:14:03 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:18:43 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1515.4888548851013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:23:45 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:28:55 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:34:23 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:39:18 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:43:58 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1515.8082416057587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:49:01 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:54:16 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:59:35 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:04:52 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:09:46 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1554.92764878273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:14:56 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:20:15 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:25:46 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:30:43 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:35:26 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1531.3596014976501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:40:27 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:45:42 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:50:58 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:56:15 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:00:54 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1531.4669153690338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:05:59 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:11:07 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:16:24 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:21:19 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:25:59 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1502.4229681491852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:31:01 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:36:11 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:41:27 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:46:21 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:51:01 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1501.2714023590088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:56:03 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:01:11 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:06:27 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:11:21 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:16:03 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1501.9442064762115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 21:21:05 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:26:16 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:31:35 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:36:29 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:41:10 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1506.597589969635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:46:11 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:51:18 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:56:32 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:01:28 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:06:09 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1500.809181213379 seconds. 
Discarding model... 

Training complete taking 38041.39222121239 total seconds. 
Now scoring model... 
Scoring complete taking 0.926196813583374 seconds. 
Saved predicted values as A1_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (10.999933579939112,), 'R2_train': 0.9467090135580063, 'MAE_train': 2.3939758761749914, 'MSE_test': 11.47043450909876, 'R2_test': 0.9308583553118488, 'MAE_test': 2.338046330040941}. 
Saved model results as A1_Modified-Pauli-CRX_results.json. 
