/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:38 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:41 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:40:51 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:45:43 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:21 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1477.1156332492828 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:16 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:00:17 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:05:26 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:10:10 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:51 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.6460330486298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:19:44 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:10 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:19 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:35:07 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:40 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1496.4805598258972 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:41 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:42 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:55 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:46 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:21 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1474.5413057804108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:15 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:23 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:19:33 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:22 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:28:54 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1472.2240748405457 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:47 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:38:46 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:56 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:48:40 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:11 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1456.5756347179413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:03 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:18 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:29 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:13:18 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:49 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1502.6907165050507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:23:06 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:28:12 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:16 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:38:01 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:42:36 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1461.170149564743 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:47:31 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:52:33 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:58:07 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:02:53 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:25 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1487.6871025562286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:12:15 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:13 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:22:17 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:27:02 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:31:32 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1450.5172560214996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:27 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:41:25 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:46:31 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:38 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:08 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1474.7324748039246 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:00 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:22 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:11:29 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:18 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:20:53 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1482.5833897590637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:25:44 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:35:57 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:40:44 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:16 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.7369384765625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:50:12 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:55:12 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:00:17 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:05:02 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:09:33 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1450.8563158512115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:14:23 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:20 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:24:35 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:22 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:58 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.2552361488342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:54 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:58 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:09 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:53:58 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:58:30 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1469.265961408615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:03:22 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:08:20 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:13:25 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:18:10 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:22:43 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1452.759045124054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:36 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:32:33 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:37:42 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:34 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:06 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1463.1258373260498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:51:59 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:11 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:02:32 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:07:15 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:11:52 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1500.240728378296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:16:59 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:22:11 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:27:15 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:32:09 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:36:41 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1473.507125377655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:41:31 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:46:30 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:51:36 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:56:27 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:00:58 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1460.2397718429565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:05:53 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:11:04 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:16:25 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:21:23 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:25:53 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1494.8552300930023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:30:48 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:35:47 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:41:10 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:45:55 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:33 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1483.4385809898376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:55:30 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:00:28 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:05:33 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:10:30 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:15:02 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1461.3424079418182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:19:52 2024]  Iteration number: 0 with current cost as 0.0583350751646212 and parameters 
[-3.00461987  1.87831988 -2.29897802 -0.11653101  0.55388709 -2.77010894
  3.06858498  2.18960147  1.18552    -1.06648307  1.6756502   1.14432447
  1.31029899 -1.87354678  0.72965079  2.88578421 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:24:50 2024]  Iteration number: 0 with current cost as 0.046200377797600646 and parameters 
[-3.01229     1.84216371 -2.32314824 -0.11653101  0.55388709 -2.77010896
  3.06858498  2.18960147  1.18552    -1.06648308  1.76430881  1.14432446
  1.31029897 -1.87354677  0.72965076  2.88578419 -0.5453433  -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:30:00 2024]  Iteration number: 0 with current cost as 0.0505376879170703 and parameters 
[-3.00848142  1.86909962 -2.30828549 -0.11653101  0.55388711 -2.77010895
  3.06858498  2.18960148  1.18552003 -1.06648306  1.69946248  1.14432448
  1.310299   -1.87354674  0.72965079  2.88578422 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:34:57 2024]  Iteration number: 0 with current cost as 0.049341082880678275 and parameters 
[-3.00914149  1.86561029 -2.31200568 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308  1.7062488   1.14432447
  1.31029899 -1.87354679  0.72965078  2.88578419 -0.54534329 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:39:27 2024]  Iteration number: 0 with current cost as 0.052944774997575626 and parameters 
[-3.00843649  1.86488723 -2.30956611 -0.11653101  0.55388711 -2.77010896
  3.06858498  2.18960148  1.18552001 -1.06648308  1.70742403  1.14432447
  1.31029899 -1.87354677  0.72965079  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1468.442435503006 seconds. 
Discarding model... 

Training complete taking 36823.03146696091 total seconds. 
Now scoring model... 
Scoring complete taking 0.9038832187652588 seconds. 
Saved predicted values as A1_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (10.999933579939112,), 'R2_train': 0.9467090135580063, 'MAE_train': 2.3939758761749914, 'MSE_test': 11.47043450909876, 'R2_test': 0.9308583553118488, 'MAE_test': 2.338046330040941}. 
Saved model results as A1_Modified-Pauli-CRX_results.json. 
