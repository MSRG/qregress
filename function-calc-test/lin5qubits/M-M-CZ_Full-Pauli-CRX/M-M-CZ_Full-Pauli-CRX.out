/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:46:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:46:55 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:21 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:41 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:22 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:19 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2701.4448487758636 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:47 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:21 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:46 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:21 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:11 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2705.1623752117157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:52 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:15 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:30 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 19:42:53 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:46 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2655.263779401779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:08 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:02 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:19:11 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 20:27:36 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:33 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2691.3378574848175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:46:00 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:54:48 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:38 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:00 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:09 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2734.7719995975494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:31:33 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:53 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:01 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:57:25 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:06:17 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.9539136886597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:37 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 22:24:01 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:33:10 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 22:41:33 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:59 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2714.4690659046173 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:52 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:14 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:19:00 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:20 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:36:12 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2683.174477338791 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:45:35 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:54:03 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:08 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:11:31 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:25 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2653.6258430480957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:29:49 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:11 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:47:20 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:55:44 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:43 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2662.902769088745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:12 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 01:22:36 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:44 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:26 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:49:22 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2676.138253211975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:58:48 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:07:14 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:16:22 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:24:43 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:33:39 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2658.5374653339386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:43:07 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:51:30 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:00:37 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:09:01 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:17:59 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2658.7405071258545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:27:25 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 03:35:47 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:44:57 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:53:41 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:02:38 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2674.783267736435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:12:00 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 04:20:24 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:29:33 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 04:38:35 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:47:37 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2701.2645020484924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:57:01 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:06:01 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:15:26 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 05:23:53 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:32:47 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2710.286699295044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:42:11 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:50:34 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:59:41 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 06:08:07 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:17:02 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2655.6363339424133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:26:27 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 06:34:48 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:44:04 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 06:52:31 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:01:25 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2658.539322376251 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:10:46 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 07:19:10 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:28:19 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 07:36:41 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:45:31 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2650.1577858924866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:54:56 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 08:03:18 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:12:25 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 08:20:46 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:29:36 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2648.4401242733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:39:04 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 08:47:42 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:56:59 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 09:05:19 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:14:13 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2669.6727180480957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:23:34 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 09:31:54 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:40:58 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 09:49:21 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:58:14 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.0258009433746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:07:37 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 10:15:57 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:25:07 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 10:33:29 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:42:20 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.3564109802246 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:51:40 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 11:00:02 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:09:11 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 11:17:30 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:26:20 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.6161589622498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:35:44 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 11:44:05 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:53:16 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 12:01:36 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:10:31 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2656.9252212047577 seconds. 
Discarding model... 

Training complete taking 66795.2299683094 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.3153724670410156 seconds. 
Saved predicted values as M-M-CZ_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (73.42853954883535,), 'R2_train': 0.6442633696726336, 'MAE_train': 5.453082203156311, 'MSE_test': 93.05230548263519, 'R2_test': 0.43909801865045717, 'MAE_test': 6.644879519862959}. 
Saved model results as M-M-CZ_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:22:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:34 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 12:31:09 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 12:40:33 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 12:49:03 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:14 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2711.7018671035767 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:07:45 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 13:16:17 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 13:25:33 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:00 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:53 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2672.802553653717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:52:18 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 14:00:45 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:55 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 14:18:51 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 14:27:51 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2704.162291765213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:37:22 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 14:45:47 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 14:54:56 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 15:03:38 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:12:45 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2691.1948416233063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:22:14 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 15:30:50 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 15:40:10 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 15:48:51 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 15:58:20 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2733.6900255680084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:07:47 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 16:16:15 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 16:25:31 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 16:34:05 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 16:43:06 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2731.415483236313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:53:19 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:01:49 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:11:16 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 17:19:49 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 17:28:46 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2703.339476108551 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:38:22 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 17:46:52 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 17:56:13 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 18:04:45 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:13:51 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2700.537659406662 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:23:22 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 18:31:56 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 18:41:13 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 18:49:56 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 18:58:58 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2721.5044808387756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:08:44 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 19:17:12 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 19:26:28 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 19:35:06 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 19:44:05 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2706.2091953754425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:53:50 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 20:02:21 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:12:00 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 20:20:27 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 20:29:44 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2732.3192825317383 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:39:22 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 20:48:10 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 20:57:25 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 21:05:55 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:14:55 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2702.959276676178 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:24:25 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:32:55 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:42:07 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 21:51:01 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:00:03 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2716.4512774944305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:09:42 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:18:08 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:27:37 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:36:09 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:45:06 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2705.11737036705 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:54:47 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:03:23 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:12:38 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:21:25 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:30:41 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2724.728189468384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:40:12 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:48:46 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:58:02 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:06:42 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:15:46 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2704.8187301158905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:25:17 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:33:43 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:42:54 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:51:21 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:00:22 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2674.784783601761 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:09:51 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:18:18 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:27:40 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 01:36:09 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:45:17 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2705.5654492378235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:54:57 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:03:34 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:12:49 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:21:14 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:30:41 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2734.54763007164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:40:31 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:49:12 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:58:26 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 03:07:05 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:16:25 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2727.599630355835 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:25:59 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 03:34:26 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:43:37 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 03:52:05 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:01:31 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2700.171284198761 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:10:59 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:19:45 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:29:10 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:37:39 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:46:40 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2741.985645055771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:56:41 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:05:10 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:14:34 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 05:23:00 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:31:56 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2684.5070111751556 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:41:26 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:49:57 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:59:12 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:07:39 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:16:36 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2684.0376613140106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:26:10 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:34:41 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:44:09 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:52:35 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:01:34 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2693.6700463294983 seconds. 
Discarding model... 

Training complete taking 67709.8235669136 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.1174395084381104 seconds. 
Saved predicted values as M-M-CZ_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (73.42853954883535,), 'R2_train': 0.6442633696726336, 'MAE_train': 5.453082203156311, 'MSE_test': 93.05230548263519, 'R2_test': 0.43909801865045717, 'MAE_test': 6.644879519862959}. 
Saved model results as M-M-CZ_Full-Pauli-CRX_results.json. 
