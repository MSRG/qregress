/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:46:15 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:46:55 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:21 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:04:41 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:22 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:19 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2701.4448487758636 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:47 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:21 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:46 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:21 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:11 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2705.1623752117157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:52 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 19:25:15 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 19:34:30 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 19:42:53 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 19:51:46 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2655.263779401779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:08 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:02 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 20:19:11 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 20:27:36 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 20:36:33 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2691.3378574848175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:46:00 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 20:54:48 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:38 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:13:00 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 21:22:09 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2734.7719995975494 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:31:33 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:53 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:01 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 21:57:25 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:06:17 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.9539136886597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:37 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 22:24:01 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 22:33:10 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 22:41:33 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:59 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2714.4690659046173 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:00:52 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:09:14 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Sun Mar 24 23:19:00 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Sun Mar 24 23:27:20 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Sun Mar 24 23:36:12 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2683.174477338791 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:45:35 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Sun Mar 24 23:54:03 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:03:08 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:11:31 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 00:20:25 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2653.6258430480957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:29:49 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:11 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 00:47:20 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 00:55:44 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:04:43 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2662.902769088745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:14:12 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 01:22:36 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:44 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:26 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 01:49:22 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2676.138253211975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:58:48 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:07:14 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 02:16:22 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 02:24:43 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 02:33:39 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2658.5374653339386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:43:07 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 02:51:30 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:00:37 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:09:01 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 03:17:59 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2658.7405071258545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:27:25 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 03:35:47 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 03:44:57 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 03:53:41 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:02:38 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2674.783267736435 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:12:00 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 04:20:24 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 04:29:33 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 04:38:35 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 04:47:37 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2701.2645020484924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:57:01 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:06:01 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:15:26 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 05:23:53 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 05:32:47 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2710.286699295044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:42:11 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 05:50:34 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 05:59:41 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 06:08:07 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 06:17:02 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2655.6363339424133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:26:27 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 06:34:48 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 06:44:04 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 06:52:31 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:01:25 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2658.539322376251 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:10:46 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 07:19:10 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 07:28:19 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 07:36:41 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 07:45:31 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2650.1577858924866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:54:56 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 08:03:18 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:12:25 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 08:20:46 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 08:29:36 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2648.4401242733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:39:04 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 08:47:42 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 08:56:59 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 09:05:19 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:14:13 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2669.6727180480957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:23:34 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 09:31:54 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 09:40:58 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 09:49:21 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 09:58:14 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.0258009433746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:07:37 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 10:15:57 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 10:25:07 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 10:33:29 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 10:42:20 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.3564109802246 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:51:40 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 11:00:02 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:09:11 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 11:17:30 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 11:26:20 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2643.6161589622498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:35:44 2024]  Iteration number: 0 with current cost as 0.3102124271503005 and parameters 
[-2.85150878  2.557987   -2.0891064  -0.11653103  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.70966995  1.14432445
  1.28744995 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.4 fold... 
[Mon Mar 25 11:44:05 2024]  Iteration number: 0 with current cost as 0.27315481976816774 and parameters 
[-2.90428424  2.50941009 -2.09953476 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.75506209  1.14432445
  1.37241386 -1.87354679  0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136105]. 
Working on 0.6 fold... 
[Mon Mar 25 11:53:16 2024]  Iteration number: 0 with current cost as 0.27316088893030643 and parameters 
[-2.87042427  2.5289404  -2.09778453 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.73806713  1.14432446
  1.32526756 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.8 fold... 
[Mon Mar 25 12:01:36 2024]  Iteration number: 0 with current cost as 0.2818758975752259 and parameters 
[-2.90205749  2.52571456 -2.10293978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.75447203  1.14432445
  1.36065744 -1.8735468   0.7296508   2.88578419 -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Mar 25 12:10:31 2024]  Iteration number: 0 with current cost as 0.28399322332310634 and parameters 
[-2.87591175  2.52530031 -2.092689   -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.72767584  1.14432445
  1.32808861 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2656.9252212047577 seconds. 
Discarding model... 

Training complete taking 66795.2299683094 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.3153724670410156 seconds. 
Saved predicted values as M-M-CZ_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (73.42853954883535,), 'R2_train': 0.6442633696726336, 'MAE_train': 5.453082203156311, 'MSE_test': 93.05230548263519, 'R2_test': 0.43909801865045717, 'MAE_test': 6.644879519862959}. 
Saved model results as M-M-CZ_Full-Pauli-CRX_results.json. 
