/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:35 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:53 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:59:36 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:18 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:14 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 971.6104867458344 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:51 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:11 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:15:49 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:44 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:24:21 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 976.2613265514374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:08 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:37 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:17 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:59 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:41 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 977.2186779975891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:43:30 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:45:55 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:45 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:53:39 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:57:19 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 988.5529184341431 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:54 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:02:13 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:04 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:09:56 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:56 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 1009.0909922122955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:49 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:12 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:22:03 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:26:43 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:21 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 978.7777376174927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:00 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:14 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:00 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:01 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:50 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 980.759224653244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:30 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:39 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:11 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:36 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:13 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 925.2219202518463 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:39 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:06:50 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:09:16 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:13:42 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:17:02 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 882.0069255828857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:19:27 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:41 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:26 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:28:46 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:32:12 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 914.0480644702911 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:46 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:36:56 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:39:22 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:43:40 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:47:22 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 906.0849759578705 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:49:44 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:52:02 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:28 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:58:58 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:02:21 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 906.739990234375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:52 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:07:03 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:09:28 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:14:05 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:49 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 928.6339366436005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:20:26 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:22:47 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:25:33 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:30:12 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:08 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 986.3786745071411 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:36:55 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:10 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:41:53 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:46:20 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:49:42 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 925.4322695732117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:52:12 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:54:21 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:46 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:01:02 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:04:38 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 903.6715564727783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:07:17 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:31 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:12:09 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:16:40 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:20:13 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 925.8850255012512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:22:45 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:25:00 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:27:21 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:31:56 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:35:18 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 904.9487257003784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:53 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:40:07 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:40 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:46:54 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:50:27 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 913.4137244224548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:52:56 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:55:12 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:57:46 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:02:17 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:05:41 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 914.4523885250092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:08:14 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:10:34 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:13:09 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:17:34 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:20:50 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 904.5086123943329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:23:15 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:25:32 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:28:13 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:32:33 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:35:50 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 901.6089315414429 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:38:16 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:40:28 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:56 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:47:20 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:50:52 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 901.3337564468384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:53:23 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:29 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:10 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:32 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:59 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 908.6600561141968 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:08:33 2024]  Iteration number: 0 with current cost as 0.3460325808451481 and parameters 
[ 1.33999893  2.23743464 -2.12427948 -0.11653103  0.55388724 -2.77010913
  3.06858483  2.18960161  1.18552014 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:10:56 2024]  Iteration number: 0 with current cost as 0.28853377599065605 and parameters 
[-1.2940697   2.23743464 -2.12427954 -0.11653098  0.55388718 -2.77010897
  3.06858498  2.1896015   1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029903 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:13:23 2024]  Iteration number: 0 with current cost as 0.5408854824393918 and parameters 
[ 0.02415264  2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010907
  3.06858508  2.18960155  1.18552008 -1.06648318  0.6027153   1.14432455
  1.31029908 -1.8735467 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:17:48 2024]  Iteration number: 0 with current cost as 0.5183899724273537 and parameters 
[ 0.0086685   2.23743473 -2.12427954 -0.11653093  0.55388718 -2.77010897
  3.06858489  2.18960155  1.18552008 -1.06648308  0.6027152   1.14432445
  1.31029908 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:21:19 2024]  Iteration number: 0 with current cost as 0.3275028397518906 and parameters 
[ 1.5760501   2.23743478 -2.12427935 -0.11653088  0.55388722 -2.77010883
  3.06858513  2.1896016   1.18552027 -1.06648308  0.60271539  1.14432445
  1.31029913 -1.8735468 ]. 
Training complete taking 918.1715703010559 seconds. 
Discarding model... 

Training complete taking 23353.473415374756 total seconds. 
Now scoring model... 
Scoring complete taking 2.198018789291382 seconds. 
Saved predicted values as A1-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (193.24510237180948,), 'R2_train': 0.06379233514110116, 'MAE_train': 12.576626096660382, 'MSE_test': 169.18972260164992, 'R2_test': -0.01984416333407646, 'MAE_test': 11.370996005933899}. 
Saved model results as A1-A1-CNOT_Efficient-CRZ_results.json. 
