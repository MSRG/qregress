/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:39:27 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 21:42:01 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 21:45:24 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 21:50:09 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 21:53:06 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1080.609070301056 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:57:25 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 21:59:58 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 22:03:17 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:08:06 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 22:11:06 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1083.9507412910461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:15:27 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 22:18:05 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 22:21:23 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:26:06 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 22:29:07 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1087.530722618103 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:33:43 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 22:36:25 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 22:39:56 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 22:45:02 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 22:48:07 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1140.7559463977814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:52:45 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 22:55:28 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 22:58:56 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:04:01 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 23:07:07 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1144.0683617591858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:11:46 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 23:14:27 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 23:17:47 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:22:42 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 23:25:39 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1102.04882979393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:30:12 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 23:32:46 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 23:36:08 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:40:59 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Thu Apr  4 23:44:02 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1100.8861253261566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:48:33 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Thu Apr  4 23:51:18 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Thu Apr  4 23:54:45 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Thu Apr  4 23:59:33 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 00:02:39 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1134.2856798171997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:07:28 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 00:10:18 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 00:14:03 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:18:54 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 00:22:08 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1153.5971546173096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:26:38 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 00:29:18 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 00:33:01 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:38:12 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 00:41:10 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1148.2561256885529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:45:47 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 00:48:26 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 00:51:59 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 00:56:55 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 00:59:57 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1132.5608177185059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:04:38 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 01:07:22 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 01:10:51 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:16:00 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 01:19:12 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1160.8969190120697 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:24:02 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 01:26:43 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 01:30:21 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:35:18 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 01:38:15 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1134.2210335731506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:42:54 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 01:45:34 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 01:48:52 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 01:53:53 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 01:57:01 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1140.6908984184265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:02:05 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 02:04:56 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 02:08:36 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 02:13:31 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 02:16:44 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1169.2564282417297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:21:30 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 02:24:09 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:46 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 02:32:58 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 02:36:12 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1171.8056609630585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:40:56 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 02:43:36 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 02:47:07 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 02:52:05 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 02:55:11 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1128.0689766407013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:59:46 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 03:02:24 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 03:05:48 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:11:06 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 03:14:11 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1140.549868106842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:18:41 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 03:21:29 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 03:24:58 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:29:59 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 03:33:04 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1136.021463394165 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:37:39 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 03:40:16 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 03:43:32 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 03:48:23 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 03:51:24 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1101.8817265033722 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:55:56 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 03:58:43 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 04:02:07 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 04:07:04 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 04:10:23 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1146.7097790241241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:15:06 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 04:17:50 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 04:21:16 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 04:26:25 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 04:29:26 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1134.7918195724487 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:34:07 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 04:36:45 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 04:40:26 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 04:45:24 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 04:48:22 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1135.8491551876068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:53:02 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 04:55:46 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 04:59:16 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 05:04:20 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 05:07:26 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1140.2806026935577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:12:11 2024]  Iteration number: 0 with current cost as 0.3591286368782617 and parameters 
[-1.52651404  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010891
  3.06858485  2.18960145  1.18551998 -1.06648315  0.60271523  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.4 fold... 
[Fri Apr  5 05:14:53 2024]  Iteration number: 0 with current cost as 0.5920625826664998 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388692 -2.77010913
  3.06858483  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.87354649]. 
Working on 0.6 fold... 
[Fri Apr  5 05:18:21 2024]  Iteration number: 0 with current cost as 0.6045894376248686 and parameters 
[-0.11002086  2.23743479 -2.12427917 -0.11653087  0.55388724 -2.77010882
  3.06858483  2.18960145  1.18552014 -1.06648308  0.60271526  1.14432461
  1.31029914 -1.87354633]. 
Working on 0.8 fold... 
[Fri Apr  5 05:23:31 2024]  Iteration number: 0 with current cost as 0.33550642910249256 and parameters 
[-1.56013631  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858485  2.18960139  1.18551998 -1.06648315  0.6027151   1.14432452
  1.31029899 -1.87354667]. 
Working on 1.0 fold... 
[Fri Apr  5 05:26:42 2024]  Iteration number: 0 with current cost as 0.5977001846160478 and parameters 
[-0.10470943  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648324  0.60271542  1.14432445
  1.31029899 -1.87354649]. 
Training complete taking 1154.1848254203796 seconds. 
Discarding model... 

Training complete taking 28303.759613275528 total seconds. 
Now scoring model... 
Scoring complete taking 2.63588809967041 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.6029836176990397,), 'R2_train': -0.2006823359286103, 'MAE_train': 0.681277914527316, 'MSE_test': 0.6329974030370602, 'R2_test': -0.18051797034402584, 'MAE_test': 0.7433181782329503}. 
Saved model results as M-A2-CNOT_Efficient-CRX_results.json. 
