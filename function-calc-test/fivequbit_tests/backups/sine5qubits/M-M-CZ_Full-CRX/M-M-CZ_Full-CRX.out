/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:43:35 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:53:04 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:05:31 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:15:06 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:25:32 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3171.3493280410767 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:36:36 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:46:21 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:58:38 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:08:22 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:19:15 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3214.3641545772552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:30:12 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:39:50 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:52:16 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:02:11 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:13:03 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3221.5618295669556 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:23:52 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:34:10 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:47:06 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:57:20 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:08:32 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3336.4029586315155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:19:34 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:29:48 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:42:40 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:52:40 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:03:31 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3292.721158027649 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:14:22 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:24:25 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:37:52 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:47:53 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:58:59 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3346.4227471351624 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:10:32 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:21:04 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:34:36 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:44:52 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:56:17 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3436.6576404571533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:07:44 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:18:03 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:30:40 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:40:27 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:51:09 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3281.736389875412 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:02:07 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:11:54 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:24:39 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:34:27 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:45:01 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3227.3252239227295 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:55:43 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:05:17 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:17:45 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:27:21 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:37:48 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3161.506701231003 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:48:12 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:57:41 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:09:58 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:19:36 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:30:18 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3157.3239936828613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:40:58 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:50:24 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:02:39 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:12:07 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:22:29 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3122.3160166740417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:33:00 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:42:16 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:54:27 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:03:57 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:14:30 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3124.6938149929047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:24:58 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:34:35 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:47:19 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:56:40 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:07:08 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3156.2671427726746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:17:28 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:26:37 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:38:50 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:48:22 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:58:37 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3094.017976284027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:09:07 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:18:26 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:31:02 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:40:30 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:51:05 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3137.3610379695892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:01:24 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:10:45 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:23:22 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:32:50 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:43:19 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3139.25994849205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 12:53:49 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:03:05 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:15:17 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:24:46 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:35:21 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3121.1439473629 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 13:45:39 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:54:46 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:07:10 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:16:21 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:26:40 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3068.752761363983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:36:56 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:46:03 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:58:01 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 15:07:15 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:17:40 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3062.9312987327576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 15:28:00 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:37:37 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:49:58 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 15:59:07 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:09:14 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3097.9332654476166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 16:19:42 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:29:01 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:41:21 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 16:50:33 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:00:57 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3099.297741174698 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 17:11:14 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 17:20:28 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 17:32:51 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 17:42:31 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:52:45 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3111.8458404541016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 18:03:03 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 18:12:23 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 18:24:44 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 18:34:02 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 18:44:14 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3079.6999938488007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 18:54:33 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 19:03:52 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 19:15:56 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 19:25:23 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 19:35:44 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3111.227073907852 seconds. 
Discarding model... 

Training complete taking 79374.12060070038 total seconds. 
Now scoring model... 
Scoring complete taking 3.628246307373047 seconds. 
Saved predicted values as M-M-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.259757756224677,), 'R2_train': 0.48276115575151823, 'MAE_train': 0.43308974114164817, 'MSE_test': 0.29390023002032634, 'R2_test': 0.45188637210423943, 'MAE_test': 0.4878511994213725}. 
Saved model results as M-M-CZ_Full-CRX_results.json. 
