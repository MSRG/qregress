/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:38:11 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:46:32 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:55:39 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:03:54 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:13:38 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2614.2027547359467 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:21:44 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:29:49 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:38:59 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:07 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:56:50 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2588.745271921158 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:04:52 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:12:59 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:22:10 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:30:17 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:40:10 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2604.2733228206635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:48:18 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:56:20 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:05:27 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:13:33 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:23:15 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2580.648352622986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:31:19 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:39:23 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:48:43 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:56:46 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:06:27 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2593.94819355011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:14:32 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:22:45 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:31:54 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:40:08 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:49:50 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2600.7967998981476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:57:52 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:05:56 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:15:04 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:23:18 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:33:01 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2603.8075516223907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:41:17 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:49:20 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:58:26 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:06:32 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 03:16:18 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2594.0939977169037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:24:35 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 03:32:50 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:41:54 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:50:05 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:00:04 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2627.4821643829346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:08:19 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:16:39 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:25:46 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:33:53 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:43:37 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2603.917901992798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:51:42 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:59:46 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:08:54 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:16:58 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 05:26:55 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2608.8070952892303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:35:11 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:43:15 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:52:25 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:00:31 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 06:10:21 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2591.8560934066772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:18:24 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:26:28 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:35:41 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:43:44 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 06:53:29 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2607.209616661072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 07:01:49 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:09:50 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:19:00 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:27:05 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 07:36:49 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2583.593796491623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:44:53 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:53:19 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:02:23 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:10:31 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 08:20:15 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2645.5719017982483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:28:59 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 08:37:04 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:46:22 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:54:25 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 09:04:51 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2634.9115149974823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 09:12:55 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 09:20:57 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 09:30:08 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:38:20 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 09:48:00 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2592.2392992973328 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:56:06 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 10:04:14 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:13:17 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 10:21:25 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 10:31:12 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2589.76970744133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:39:16 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 10:47:23 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:56:32 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 11:04:38 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 11:14:26 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2594.4060649871826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 11:22:30 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 11:30:51 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 11:39:57 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 11:48:02 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 11:57:45 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2598.5842895507812 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 12:05:50 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 12:13:53 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 12:22:57 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 12:31:01 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 12:40:45 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2579.487835407257 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:48:49 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 12:56:55 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 13:06:13 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 13:14:15 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 13:24:00 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2596.8360986709595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:32:06 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 13:40:09 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 13:49:16 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 13:57:19 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 14:07:01 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2578.0496294498444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 14:15:04 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 14:23:09 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 14:32:14 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 14:40:19 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 14:50:01 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2578.198438644409 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:58:02 2024]  Iteration number: 0 with current cost as 0.3413961064148254 and parameters 
[-2.8811507   2.30421924 -2.13706508 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.58655068  1.14432445
  1.28209647 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 15:06:07 2024]  Iteration number: 0 with current cost as 0.35738611376642454 and parameters 
[-2.86379471  2.31838398 -2.13138169 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.56813817  1.14432445
  1.25400334 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 15:15:14 2024]  Iteration number: 0 with current cost as 0.36112954659894625 and parameters 
[-2.868544    2.32433378 -2.13381464 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.57295216  1.14432445
  1.26238328 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 15:23:23 2024]  Iteration number: 0 with current cost as 0.32635723610933187 and parameters 
[-2.88067717  2.30502282 -2.13694299 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.5797108   1.14432445
  1.28423171 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 15:33:22 2024]  Iteration number: 0 with current cost as 0.3398287426739196 and parameters 
[-2.87407398  2.30554718 -2.13402237 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.58406251  1.14432444
  1.26896114 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105]. 
Training complete taking 2603.0953459739685 seconds. 
Discarding model... 

Training complete taking 64994.53502678871 total seconds. 
Now scoring model... 
Scoring complete taking 1.1127965450286865 seconds. 
Saved predicted values as A2-A2-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.3354289755094818,), 'R2_train': 0.3320819438018595, 'MAE_train': 0.49800391374530567, 'MSE_test': 0.3888312013673795, 'R2_test': 0.2748434378367054, 'MAE_test': 0.5656113559392691}. 
Saved model results as A2-A2-CNOT_Full-Pauli-CRX_results.json. 
