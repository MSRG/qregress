/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:41 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:37:51 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 21:41:42 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 21:46:05 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 21:50:07 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 21:53:45 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1202.3509922027588 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:57:53 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 22:01:42 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 22:06:06 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 22:10:00 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 22:13:39 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1195.0218336582184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:17:48 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 22:21:46 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 22:26:05 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 22:29:56 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 22:33:41 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1201.6824963092804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:37:50 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 22:41:39 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 22:46:00 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 22:49:52 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 22:53:34 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1197.2958748340607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:57:47 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 23:01:36 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 23:05:53 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 23:09:48 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 23:13:38 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1201.0668489933014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:17:48 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 23:21:42 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 23:26:00 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 23:29:54 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 23:33:45 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1203.7334461212158 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:37:52 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Thu Apr  4 23:41:41 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Thu Apr  4 23:46:04 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Thu Apr  4 23:50:09 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Thu Apr  4 23:53:50 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1214.122330904007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:58:08 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 00:02:00 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 00:06:21 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 00:10:45 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 00:14:45 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1247.7279522418976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:18:54 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 00:22:44 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 00:27:09 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 00:31:12 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 00:34:51 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1208.7214379310608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:39:03 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 00:42:51 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 00:47:11 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 00:51:14 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 00:54:54 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1201.1009304523468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:59:04 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 01:02:58 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 01:07:18 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 01:11:16 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 01:14:55 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1199.2302176952362 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:19:03 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 01:22:54 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 01:27:13 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 01:31:05 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 01:34:45 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1190.614679813385 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:38:54 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 01:42:54 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 01:47:17 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 01:51:10 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 01:55:13 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1226.9437437057495 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:59:22 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 02:03:10 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 02:07:30 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 02:11:28 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 02:15:14 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1199.9506995677948 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:19:22 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 02:23:12 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:33 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 02:31:36 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 02:35:16 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1214.4851324558258 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:39:35 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 02:43:24 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 02:48:17 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 02:52:10 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 02:55:57 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1232.500329732895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:00:07 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 03:03:57 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 03:09:08 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 03:13:03 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 03:16:40 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1242.2987129688263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:20:52 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 03:25:00 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 03:29:20 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 03:33:14 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 03:37:01 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1233.2765893936157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:41:24 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 03:45:14 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 03:49:39 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 03:53:34 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 03:57:11 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1203.1372165679932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:01:28 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 04:05:14 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 04:10:20 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 04:14:11 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 04:17:55 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1247.15829372406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:22:13 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 04:26:04 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 04:30:24 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 04:34:35 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 04:38:12 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1209.9195311069489 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:42:25 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 04:46:13 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 04:50:33 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 04:54:27 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 04:58:04 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1189.954776763916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:02:13 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 05:06:23 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 05:10:42 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 05:14:38 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 05:18:16 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1217.038114309311 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:22:32 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 05:26:28 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 05:30:46 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 05:34:40 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 05:38:17 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1199.9388449192047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:42:32 2024]  Iteration number: 0 with current cost as 0.16098966031066658 and parameters 
[-2.90318344  2.23743464 -2.12427964  0.06099498  0.55040926 -2.56689552
  3.04639808  2.37308031  1.1192782  -1.10298085  0.61268145  1.10313632
  1.28204892 -1.83549018  0.96064393]. 
Working on 0.4 fold... 
[Fri Apr  5 05:46:20 2024]  Iteration number: 0 with current cost as 0.16709218110240914 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.0631934   0.55124184 -2.56298885
  3.03573134  2.37350732  1.13742398 -1.10274687  0.61908289  1.1054218
  1.28402364 -1.83291722  0.96323165]. 
Working on 0.6 fold... 
[Fri Apr  5 05:50:45 2024]  Iteration number: 0 with current cost as 0.16556897989845815 and parameters 
[-2.90318345  2.23743464 -2.12427963  0.06500989  0.55264366 -2.55862755
  3.03620249  2.37292607  1.13694275 -1.10357837  0.62330975  1.10573401
  1.28508739 -1.83115176  0.96598408]. 
Working on 0.8 fold... 
[Fri Apr  5 05:54:40 2024]  Iteration number: 0 with current cost as 0.14999375385283692 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.06334176  0.5481661  -2.56769732
  3.04557883  2.38533515  1.11371143 -1.0993691   0.60272375  1.10440723
  1.27970266 -1.83255629  0.97901657]. 
Working on 1.0 fold... 
[Fri Apr  5 05:58:18 2024]  Iteration number: 0 with current cost as 0.16637623649925443 and parameters 
[-2.90318345  2.23743464 -2.12427964  0.0612129   0.54887074 -2.56908178
  3.04397233  2.36956876  1.12545989 -1.10262501  0.61279651  1.10360424
  1.28719019 -1.83456825  0.94670026]. 
Training complete taking 1216.0309422016144 seconds. 
Discarding model... 

Training complete taking 30295.304020643234 total seconds. 
Now scoring model... 
Scoring complete taking 2.7127292156219482 seconds. 
Saved predicted values as M-A1-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (0.09340546881519234,), 'R2_train': 0.8140077222769452, 'MAE_train': 0.25497817430405945, 'MSE_test': 0.10477451873336094, 'R2_test': 0.804599194869623, 'MAE_test': 0.28807783757189503}. 
Saved model results as M-A1-CNOT_HWE-CNOT_results.json. 
