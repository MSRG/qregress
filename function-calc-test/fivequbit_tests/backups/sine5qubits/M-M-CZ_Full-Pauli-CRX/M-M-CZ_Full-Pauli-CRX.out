/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:45:48 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:46:20 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:54:11 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:02:01 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:09:49 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:18:10 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2412.065614461899 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:26:31 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:34:33 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:42:29 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 22:50:12 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:58:16 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2388.632221221924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:06:19 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:14:11 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:21:59 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Thu Apr  4 23:29:51 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:37:48 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2370.0556309223175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:45:49 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:53:34 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:01:18 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:09:11 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:17:13 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2362.016070127487 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:25:11 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:32:58 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:40:37 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 00:48:33 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:56:32 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2361.184959411621 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:04:32 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:12:13 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:20:01 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 01:27:45 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:35:53 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2368.3349187374115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:44:01 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 01:51:49 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:59:32 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:07:22 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:15:26 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2365.2054805755615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:23:26 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 02:31:14 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:38:57 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 02:46:40 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:54:54 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2366.5990781784058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:02:53 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:10:42 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:18:39 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 03:26:25 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:34:31 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2392.332767724991 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:42:46 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 03:50:29 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:58:18 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:06:09 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:14:12 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2385.577425956726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:22:31 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 04:30:15 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:38:07 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 04:45:53 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:53:50 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2358.527289867401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:01:50 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:09:42 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:17:48 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 05:25:34 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:33:38 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2387.481460094452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:41:37 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 05:49:21 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:57:06 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:04:54 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:12:55 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2355.7230746746063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:20:52 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 06:28:48 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:36:33 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 06:44:26 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:52:29 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2377.230903148651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:00:30 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:08:30 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:16:22 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 07:24:29 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:32:36 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2407.3803946971893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:40:37 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 07:48:21 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:56:04 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 08:03:47 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:11:52 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2357.517230272293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:19:54 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 08:27:36 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:35:29 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 08:43:22 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:51:28 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2371.6493759155273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:59:26 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 09:07:16 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 09:14:58 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 09:22:41 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:30:43 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2352.6282098293304 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:38:39 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 09:46:23 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 09:54:07 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 10:01:50 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 10:09:47 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2349.2250423431396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:17:48 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 10:25:46 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 10:33:49 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 10:41:38 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 10:49:55 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2415.5922362804413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 10:58:03 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 11:06:43 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 11:14:25 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 11:22:05 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 11:30:06 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2410.195864200592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 11:38:14 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 11:45:58 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 11:53:38 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 12:01:22 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 12:09:27 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2368.2172434329987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 12:17:42 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 12:25:34 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 12:33:17 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 12:41:00 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 12:48:58 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2350.823917388916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 12:56:58 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 13:04:43 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 13:12:49 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 13:20:44 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 13:29:01 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2413.8896548748016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 13:37:07 2024]  Iteration number: 0 with current cost as 0.2255400208034188 and parameters 
[-2.87716933  2.5420734  -2.08736075 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75106326  1.14432445
  1.3400019  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 13:44:49 2024]  Iteration number: 0 with current cost as 0.22973756793322567 and parameters 
[-2.84465627  2.55106401 -2.07698739 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.71568143  1.14432445
  1.29034356 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 13:52:31 2024]  Iteration number: 0 with current cost as 0.22764355502794448 and parameters 
[-2.8403936   2.56313354 -2.07686826 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72444507  1.14432445
  1.28908896 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Fri Apr  5 14:00:16 2024]  Iteration number: 0 with current cost as 0.22126600932475277 and parameters 
[-2.87322454  2.5420643  -2.07914454 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.74409862  1.14432445
  1.33777281 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 14:08:28 2024]  Iteration number: 0 with current cost as 0.22342109229393475 and parameters 
[-2.88437592  2.54109739 -2.09254869 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.75529044  1.14432445
  1.34343636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 2359.197562932968 seconds. 
Discarding model... 

Training complete taking 59407.286249399185 total seconds. 
Now scoring model... 
Scoring complete taking 1.387740135192871 seconds. 
Saved predicted values as M-M-CZ_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.0679732649281571,), 'R2_train': 0.8646492274100734, 'MAE_train': 0.22081757278486056, 'MSE_test': 0.07228162202377422, 'R2_test': 0.8651973083692348, 'MAE_test': 0.23172871476344511}. 
Saved model results as M-M-CZ_Full-Pauli-CRX_results.json. 
