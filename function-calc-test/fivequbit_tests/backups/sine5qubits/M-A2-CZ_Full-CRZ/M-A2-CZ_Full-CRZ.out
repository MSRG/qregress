/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:43:42 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:51:21 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:59:10 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:06:46 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:15:21 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2427.931848049164 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:24:02 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:32:01 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:39:47 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:38 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:56:24 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2462.5978746414185 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:05:15 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:13:05 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:21:09 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:29:11 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:38:11 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2513.389241695404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:47:10 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:55:33 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:04:03 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:12:24 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:21:54 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2635.462704896927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:31:15 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:39:45 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:48:05 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:56:18 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:05:24 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2610.7821774482727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:14:39 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:22:54 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:30:49 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:38:47 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:47:47 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2536.9909019470215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:56:56 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:05:06 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:13:02 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:21:04 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:29:59 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2513.431438922882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:38:27 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:45:58 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:53:44 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:01:19 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:10:13 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2419.0376896858215 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:18:37 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:26:17 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:34:01 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:41:47 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:50:17 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2400.313785791397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:58:38 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:06:36 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:14:16 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:21:54 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:30:36 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2431.9549882411957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:39:34 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:47:29 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:55:30 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:03:19 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:12:12 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2485.4004979133606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:20:49 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:28:18 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:36:06 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:43:48 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:52:24 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2410.4878656864166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 06:00:49 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:08:33 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:16:13 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:23:50 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:32:44 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2428.1496381759644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:41:36 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:49:16 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:56:53 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:04:40 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:13:23 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2430.5772330760956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:21:57 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:29:26 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:37:13 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:44:59 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:53:44 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2425.07035946846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:02:24 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:10:00 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:17:49 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:25:39 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:34:22 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2439.317355632782 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:43:06 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:51:05 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:58:41 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:06:37 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:15:34 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2478.8618714809418 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:24:34 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:32:03 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:39:34 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:47:01 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:56:00 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2422.372043609619 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:04:51 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:12:43 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:20:30 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:28:13 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:37:04 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2461.9739751815796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:45:45 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:53:30 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:01:16 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:08:59 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:17:42 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2435.254565000534 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:26:25 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:34:14 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:42:03 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:49:40 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:58:09 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2435.1364121437073 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:06:50 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:14:34 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:22:05 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:29:42 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:38:20 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2397.528371334076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 12:46:49 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:54:06 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:01:30 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:08:45 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:17:19 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2345.5203804969788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 13:25:47 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:33:08 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:40:50 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:48:21 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:56:39 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2359.0521388053894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:05:07 2024]  Iteration number: 0 with current cost as 0.09346452407210135 and parameters 
[-4.66993293  2.2374349  -2.12427924 -0.11653089  0.55388721 -2.77010897
  3.06858525  2.18960172  1.18552025 -1.06648308  0.60271563  1.14432445
  1.31029912 -1.87354654  0.72965094  2.88578433 -0.54534309 -0.47522459
 -2.0265424   0.72897383  1.60512664  2.83077107 -1.26456696 -0.25136091
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:12:33 2024]  Iteration number: 0 with current cost as 0.10149399287826942 and parameters 
[-4.64449707  2.23743464 -2.12427964 -0.11653115  0.55388683 -2.77010922
  3.06858486  2.18960145  1.18551998 -1.06648333  0.6027151   1.14432421
  1.31029874 -1.8735468   0.72965068  2.88578407 -0.54534347 -0.47522473
 -2.02654253  0.72897357  1.60512651  2.83077083 -1.26456722 -0.25136105
 -2.39279218 -2.27309774  3.13337143  2.54856946 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:19:59 2024]  Iteration number: 0 with current cost as 0.09775604804380797 and parameters 
[-4.65004243  2.23743464 -2.12427937 -0.11653103  0.55388695 -2.77010924
  3.06858498  2.18960132  1.18551998 -1.06648335  0.6027151   1.14432432
  1.31029899 -1.8735468   0.72965067  2.88578406 -0.54534348 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077081 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:27:19 2024]  Iteration number: 0 with current cost as 0.09389774027944237 and parameters 
[-4.70877406  2.23743464 -2.1242794  -0.11653103  0.55388708 -2.77010921
  3.06858498  2.18960145  1.1855201  -1.0664832   0.60271522  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:35:48 2024]  Iteration number: 0 with current cost as 0.0941878163243493 and parameters 
[-4.69140476  2.23743464 -2.12427951 -0.11653103  0.55388708 -2.77010922
  3.06858486  2.18960145  1.18551986 -1.06648333  0.60271522  1.14432445
  1.31029886 -1.8735468   0.72965068  2.88578419 -0.54534347 -0.47522485
 -2.02654253  0.7289737   1.60512664  2.83077083 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2346.7320902347565 seconds. 
Discarding model... 

Training complete taking 61253.32807803154 total seconds. 
Now scoring model... 
Scoring complete taking 2.744702100753784 seconds. 
Saved predicted values as M-A2-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.16198587233038672,), 'R2_train': 0.6774479938289837, 'MAE_train': 0.3436058689325775, 'MSE_test': 0.17765403957985074, 'R2_test': 0.6686814428906214, 'MAE_test': 0.37889609969158167}. 
Saved model results as M-A2-CZ_Full-CRZ_results.json. 
