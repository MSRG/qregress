/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:41:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:45:03 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 21:54:30 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:02:06 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Thu Apr  4 22:10:30 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:18:58 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2432.2064385414124 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:25:25 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 22:35:11 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:43:00 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Thu Apr  4 22:51:35 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:00:22 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2497.1089651584625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:07:04 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 23:16:43 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:24:27 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Thu Apr  4 23:33:22 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:42:06 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2497.8005125522614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:48:50 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Thu Apr  4 23:58:29 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:06:13 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 00:15:05 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:23:47 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2499.539362668991 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:30:22 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 00:40:14 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:47:58 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 00:56:47 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:05:16 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2497.0209743976593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:12:11 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 01:21:39 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:29:04 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 01:37:56 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:46:32 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2455.7049827575684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:52:55 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 02:02:11 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:09:33 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 02:17:57 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:26:14 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2388.761009454727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:32:42 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 02:41:55 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:48:58 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 02:57:26 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:05:37 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2353.8922600746155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:11:52 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 03:21:17 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:29:09 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 03:37:44 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:46:38 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2483.066685438156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:53:22 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 04:03:16 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:10:55 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 04:19:31 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:28:10 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2485.744103908539 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:34:41 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 04:44:10 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:51:53 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 05:00:30 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:09:17 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2462.782958507538 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:16:02 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 05:25:40 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:33:47 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 05:42:55 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:52:05 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2570.1609053611755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:58:42 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 06:08:36 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:16:29 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 06:25:08 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:33:48 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2513.6990654468536 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 06:40:50 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 06:50:07 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:57:46 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 07:06:15 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:15:07 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2473.7383749485016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 07:22:00 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 07:31:58 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:39:41 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 07:48:10 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:56:43 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2485.020499229431 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 08:03:19 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 08:12:41 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:20:09 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 08:28:18 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:36:39 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2394.8293681144714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:43:12 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 08:52:10 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:59:18 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 09:07:53 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:16:15 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2385.5210144519806 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:23:08 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 09:32:45 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:40:05 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 09:48:30 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:56:49 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2427.910576581955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 10:03:19 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 10:13:15 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:20:56 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 10:29:31 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:38:00 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2472.301773071289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:44:50 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 10:54:33 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:02:26 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 11:11:11 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:19:45 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2503.632225751877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:26:12 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 11:35:15 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:42:37 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 11:50:40 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:58:52 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2339.015972852707 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:05:11 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 12:14:39 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:22:42 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 12:30:51 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:39:12 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2427.2202112674713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 12:45:35 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 12:54:45 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:02:00 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 13:10:02 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:18:27 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2355.0439348220825 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 13:24:46 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 13:33:42 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:40:50 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 13:48:47 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:56:40 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2300.564623117447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 14:03:11 2024]  Iteration number: 0 with current cost as 0.6016901184184594 and parameters 
[-0.18336986  2.23743448 -2.12427948 -0.11653103  0.55388708 -2.77010929
  3.06858483  2.1896013   1.18551983 -1.06648355  0.6027151   1.14432445
  1.31029867 -1.8735468   0.72965065  2.88578388 -0.54534366 -0.47522485
 -2.02654256  0.7289737   1.60512648  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856958 -0.67550787 -2.69002217]. 
Working on 0.4 fold... 
[Fri Apr  5 14:11:55 2024]  Iteration number: 0 with current cost as 0.3672680875242503 and parameters 
[-1.3683026   2.23743464 -2.12427938 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648321  0.6027151   1.14432445
  1.31029886 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522472
 -2.02654253  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:18:53 2024]  Iteration number: 0 with current cost as 0.3708577333667045 and parameters 
[-1.45052441  2.23743442 -2.12427953 -0.11653103  0.55388697 -2.77010919
  3.06858487  2.18960134  1.18551998 -1.0664833   0.60271499  1.14432434
  1.31029888 -1.87354691  0.72965059  2.88578398 -0.54534357 -0.47522485
 -2.02654262  0.72897359  1.60512653  2.83077085 -1.2645672  -0.25136115
 -2.39279229 -2.27309785  3.13337144  2.54856948 -0.67550798 -2.69002212]. 
Working on 0.8 fold... 
[Fri Apr  5 14:26:58 2024]  Iteration number: 0 with current cost as 0.3404778404115016 and parameters 
[-1.43259807  2.23743464 -2.12427953 -0.11653103  0.55388708 -2.77010908
  3.06858498  2.18960124  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296507   2.88578408 -0.54534357 -0.47522485
 -2.02654251  0.7289737   1.60512653  2.83077085 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:35:02 2024]  Iteration number: 0 with current cost as 0.35470298755274515 and parameters 
[-1.42087438  2.23743464 -2.12427946 -0.11653094  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552016 -1.06648308  0.6027151   1.14432454
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522467
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2283.1200802326202 seconds. 
Discarding model... 

Training complete taking 60985.40747952461 total seconds. 
Now scoring model... 
Scoring complete taking 2.813021183013916 seconds. 
Saved predicted values as M-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.6017804311695842,), 'R2_train': -0.19828650829691252, 'MAE_train': 0.7025604331236417, 'MSE_test': 0.576309197112489, 'R2_test': -0.07479645319492478, 'MAE_test': 0.7124948124116629}. 
Saved model results as M-A1-CNOT_Full-CRX_results.json. 
