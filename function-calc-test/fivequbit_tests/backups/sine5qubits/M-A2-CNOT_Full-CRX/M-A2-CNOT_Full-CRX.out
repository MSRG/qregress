/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:41:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:46:01 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:52:45 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:02:05 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:15:13 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:23:22 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2950.3425211906433 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:35:32 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:42:45 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:52:11 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:05:49 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:14:10 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3066.809998512268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:26:41 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:33:50 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:43:06 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:56:43 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:04:56 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3038.809360265732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:17:16 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:24:44 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:33:51 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:47:29 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:55:58 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3061.755696296692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:08:27 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:15:40 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:24:38 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:37:48 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:46:00 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2988.038771867752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:57:50 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:04:45 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:13:38 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:25:50 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:33:18 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2820.671777009964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:44:54 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:51:49 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:00:21 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:12:51 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:20:32 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2846.309385538101 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:32:08 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:39:01 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:47:26 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:59:45 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:07:17 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2778.974910736084 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:18:31 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:25:14 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:34:06 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:46:13 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:53:47 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2798.5786406993866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:05:15 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:11:56 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:20:27 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:32:35 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:40:24 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2800.4793610572815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 05:51:41 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:58:39 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:07:12 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:19:35 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:27:18 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2798.1689071655273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 06:38:17 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:44:49 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:53:25 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:05:20 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:12:44 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2739.649423599243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:24:06 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:30:54 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:39:37 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:51:56 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:59:23 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2808.7346081733704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:10:55 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:17:38 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:26:05 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:38:20 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:45:50 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2782.652007818222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:57:20 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:04:20 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:12:57 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:25:16 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:32:45 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2810.381954908371 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 09:44:02 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:50:45 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:59:41 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:12:07 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:19:50 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2850.394788503647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 10:31:38 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:38:17 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:46:59 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:59:43 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:07:35 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2861.5221202373505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:19:32 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:26:27 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:35:28 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:48:28 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:56:38 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2934.5901157855988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 12:08:27 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:15:26 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:24:10 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:36:52 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:45:03 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2910.274915456772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 12:56:38 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:02:59 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:11:23 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:23:39 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:31:24 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2758.7980279922485 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 13:42:53 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:49:38 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:57:59 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:10:48 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:18:32 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2838.337976694107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 14:30:02 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:36:42 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:45:17 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:57:43 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:05:31 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2825.9679849147797 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 15:17:13 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:23:54 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:32:13 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 15:44:39 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:52:11 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2796.166940689087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 16:03:45 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:10:13 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:18:59 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 16:31:41 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:39:21 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2832.7979242801666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 16:51:15 2024]  Iteration number: 0 with current cost as 0.35912864343770934 and parameters 
[-1.52651411  2.23743457 -2.1242797  -0.11653109  0.55388701 -2.77010904
  3.06858485  2.18960139  1.18551992 -1.06648322  0.60271504  1.14432445
  1.31029899 -1.8735468   0.72965067  2.88578413 -0.54534342 -0.47522485
 -2.02654247  0.72897363  1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856952 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:58:00 2024]  Iteration number: 0 with current cost as 0.5920625826664997 and parameters 
[-0.22659192  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858483  2.18960145  1.18551998 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578419 -0.54534351 -0.4752247
 -2.02654256  0.7289737   1.60512664  2.83077091 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337139  2.54856943 -0.67550772 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 17:06:29 2024]  Iteration number: 0 with current cost as 0.6045894314765937 and parameters 
[-0.11002133  2.23743432 -2.12427995 -0.11653118  0.55388677 -2.77010929
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578388 -0.54534366 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077091 -1.2645671  -0.2513612
 -2.39279218 -2.2730979   3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 17:18:46 2024]  Iteration number: 0 with current cost as 0.3355064298290499 and parameters 
[-1.56013632  2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432452
  1.31029899 -1.87354673  0.72965074  2.88578419 -0.54534335 -0.47522472
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337161  2.54856958 -0.67550781 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:26:20 2024]  Iteration number: 0 with current cost as 0.5977001784458996 and parameters 
[-0.10470975  2.23743432 -2.12427964 -0.11653118  0.55388692 -2.77010913
  3.06858467  2.18960114  1.18551998 -1.0664834   0.60271479  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578388 -0.54534351 -0.47522485
 -2.02654272  0.72897354  1.60512664  2.83077107 -1.2645671  -0.2513612
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2803.49520111084 seconds. 
Discarding model... 

Training complete taking 71502.70394158363 total seconds. 
Now scoring model... 
Scoring complete taking 2.831130027770996 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.6029836177966987,), 'R2_train': -0.2006823361230725, 'MAE_train': 0.681277914825985, 'MSE_test': 0.6329974013228202, 'R2_test': -0.18051796714702828, 'MAE_test': 0.7433181771052026}. 
Saved model results as M-A2-CNOT_Full-CRX_results.json. 
