/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 22:04:29 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:10:26 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:20:06 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:32:28 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:42:05 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:52:50 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3187.641221523285 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:03:19 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:13:19 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:26:27 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:35:55 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:46:34 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3223.630843400955 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:57:07 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:06:41 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:19:21 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:29:18 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:39:51 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3205.9044094085693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:50:31 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:00:25 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:12:56 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:22:47 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:33:35 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3222.7749588489532 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:44:18 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:54:10 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:06:52 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:16:53 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:27:53 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3265.306962490082 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:39:01 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:48:58 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:01:48 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:11:29 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:22:23 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3258.641989469528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:33:06 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:42:50 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:55:26 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:05:22 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:16:18 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3241.8988189697266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:27:00 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:36:42 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:49:26 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:59:10 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:10:25 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3255.169759750366 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:21:32 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:31:24 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:44:39 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:54:41 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:05:49 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3331.1033811569214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:17:02 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:27:23 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:40:25 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:50:14 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:01:22 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3327.2209811210632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:12:27 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:22:22 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:35:34 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:45:29 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:56:30 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3296.852271795273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:07:04 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:16:39 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:29:29 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:39:26 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:49:49 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3201.3777384757996 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 09:00:43 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:11:02 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:23:48 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:33:31 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:44:20 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3254.573941230774 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:54:48 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:04:37 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:17:22 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:27:32 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:38:45 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3272.459954023361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:49:48 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:59:11 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:11:35 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:21:30 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:32:57 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3262.480040073395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:43:33 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:53:21 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:06:34 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:16:57 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:27:58 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3300.0110232830048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:38:49 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:48:33 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:01:21 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 13:10:56 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:21:31 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3195.672925710678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:31:51 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:41:32 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:54:00 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:03:42 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:14:02 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3144.690942764282 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 14:24:35 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:34:37 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:47:31 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 14:57:29 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:08:08 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3274.553969144821 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 15:19:35 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:29:40 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:42:34 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 15:52:19 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:03:09 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3277.4370896816254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 16:13:32 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:22:55 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:36:15 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 16:46:09 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:57:02 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3257.786137342453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 17:08:11 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 17:18:40 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 17:30:56 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 17:41:03 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:51:51 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3277.4337804317474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 18:02:42 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 18:13:03 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 18:25:55 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 18:35:29 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 18:46:18 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3263.456035375595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 18:57:06 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 19:06:52 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 19:19:06 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 19:28:29 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 19:39:30 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3201.2847139835358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 19:50:35 2024]  Iteration number: 0 with current cost as 0.49806911973533813 and parameters 
[-1.59746768  2.23743428 -2.12427946 -0.1165312   0.5538869  -2.77010915
  3.06858463  2.1896011   1.18551981 -1.06648344  0.60271493  1.14432427
  1.31029881 -1.87354698  0.72965063  2.88578402 -0.54534353 -0.47522468
 -2.02654293  0.72897352  1.60512646  2.83077072 -1.26456727 -0.25136105
 -2.39279218 -2.27309792  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 20:00:48 2024]  Iteration number: 0 with current cost as 0.49900188451561156 and parameters 
[-1.70535717  2.23743439 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858473  2.18960096  1.18551998 -1.06648333  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752246
 -2.02654265  0.7289737   1.60512639  2.83077082 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 20:13:38 2024]  Iteration number: 0 with current cost as 0.15865953808565472 and parameters 
[ 1.60727035  2.23743313 -2.12427888 -0.11653178  0.55388633 -2.77010973
  3.06858498  2.1896007   1.18551998 -1.06648459  0.6027151   1.14432445
  1.31029823 -1.87354605  0.72965005  2.88578344 -0.54534335 -0.4752241
 -2.02654316  0.72897294  1.60512588  2.83077032 -1.26456634 -0.25136105
 -2.39279218 -2.27309699  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 20:23:25 2024]  Iteration number: 0 with current cost as 0.4869172326157124 and parameters 
[-1.62060704  2.23743464 -2.12427915 -0.11653078  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.60271559  1.14432469
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522437
 -2.02654265  0.7289737   1.60512664  2.83077107 -1.2645671  -0.2513608
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 20:34:17 2024]  Iteration number: 0 with current cost as 0.1519136867206793 and parameters 
[ 1.54067429  2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648455  0.6027151   1.14432518
  1.31029972 -1.8735468   0.72965007  2.88578419 -0.54534335 -0.47522338
 -2.02654314  0.7289737   1.60512664  2.83077107 -1.26456636 -0.25136105
 -2.39279218 -2.27309701  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 3281.343430995941 seconds. 
Discarding model... 

Training complete taking 81280.70793247223 total seconds. 
Now scoring model... 
Scoring complete taking 2.785106897354126 seconds. 
Saved predicted values as M-M-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.259757756224677,), 'R2_train': 0.48276115575151823, 'MAE_train': 0.43308974114164817, 'MSE_test': 0.29390023002032634, 'R2_test': 0.45188637210423943, 'MAE_test': 0.4878511994213725}. 
Saved model results as M-M-CZ_Full-CRZ_results.json. 
