/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:33 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:40:35 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:51:33 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:57:31 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:04:01 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 22:10:36 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2204.728318452835 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:17:09 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:27:56 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:33:49 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:40:22 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 22:47:05 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2179.8922412395477 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:53:29 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:04:20 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:10:05 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:16:39 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Thu Apr  4 23:23:42 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2203.578052520752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:30:23 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:41:39 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:47:56 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:54:44 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 00:01:33 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2274.2574067115784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:08:15 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:19:25 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:25:25 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:32:06 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 00:38:46 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2242.629901409149 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:45:38 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:56:57 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:03:04 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:09:57 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 01:16:51 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2271.329113006592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:23:32 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:34:58 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:41:12 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:48:15 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 01:55:13 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2307.147657394409 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 02:02:05 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:13:26 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:19:08 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:25:36 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 02:31:57 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2194.143103122711 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:38:29 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:49:11 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:54:58 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:01:18 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 03:07:40 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2145.7714672088623 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 03:14:06 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:24:42 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:30:25 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:36:50 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 03:43:12 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2129.0798456668854 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:49:38 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:00:20 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:06:08 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:12:33 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 04:18:51 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2134.498026609421 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:25:09 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:35:51 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:41:37 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:47:54 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 04:54:11 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2117.9387402534485 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:00:25 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:11:03 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:16:40 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:23:18 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 05:29:53 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2164.9496335983276 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:36:45 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:47:47 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:53:47 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:00:15 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 06:06:50 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2204.767155647278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:13:22 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:24:32 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:30:25 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:37:07 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 06:43:48 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2220.3162496089935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:50:29 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:01:43 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:07:42 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:14:24 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 07:21:24 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2286.7658739089966 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:28:46 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:40:12 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:46:34 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 07:53:31 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 08:00:22 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2308.5640122890472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:06:59 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:17:34 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:23:13 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 08:29:29 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 08:35:41 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2103.3859448432922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:41:53 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:52:16 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:57:47 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:04:00 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 09:10:17 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2077.1137702465057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 09:16:33 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:26:55 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:32:29 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 09:38:45 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 09:45:03 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2084.2571902275085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 09:51:14 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:02:17 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:08:46 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:15:28 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 10:21:47 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2204.4191019535065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 10:28:07 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:38:24 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:44:03 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 10:50:12 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 10:56:23 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2074.272897005081 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 11:02:27 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:12:48 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:18:19 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 11:24:38 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 11:31:22 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2112.4529778957367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 11:37:49 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:48:55 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:54:39 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:01:22 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 12:08:02 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2203.8405752182007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 12:14:39 2024]  Iteration number: 0 with current cost as 0.6752914842561337 and parameters 
[-0.07783359  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010916
  3.06858479  2.18960126  1.18552008 -1.06648327  0.60271491  1.14432426
  1.31029899 -1.8735468   0.72965061  2.885784   -0.54534335 -0.47522485
 -2.0265425   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:25:31 2024]  Iteration number: 0 with current cost as 0.30564058391695037 and parameters 
[ 1.45292768  2.23743464 -2.12427947 -0.11653103  0.55388708 -2.77010931
  3.06858498  2.18960145  1.18552015 -1.06648308  0.6027151   1.14432428
  1.31029899 -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522468
 -2.0265424   0.72897386  1.60512664  2.83077107 -1.2645671  -0.25136088
 -2.39279201 -2.27309774  3.13337155  2.54856975 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:31:35 2024]  Iteration number: 0 with current cost as 0.31224371193036243 and parameters 
[ 1.38271467  2.23743464 -2.12427948 -0.11653103  0.55388692 -2.77010929
  3.06858467  2.18960145  1.18552014 -1.06648324  0.60271479  1.14432429
  1.31029899 -1.87354664  0.7296508   2.88578404 -0.54534335 -0.4752247
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456694 -0.25136105
 -2.39279218 -2.27309759  3.13337155  2.54856958 -0.67550772 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 12:38:14 2024]  Iteration number: 0 with current cost as 0.2985578379552156 and parameters 
[ 1.33710438  2.23743464 -2.12427964 -0.11653134  0.55388677 -2.77010929
  3.06858467  2.1896013   1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029883 -1.8735468   0.72965049  2.88578388 -0.54534351 -0.47522485
 -2.02654256  0.72897354  1.60512632  2.83077076 -1.26456725 -0.2513612
 -2.39279234 -2.2730979   3.13337124  2.54856943 -0.67550803 -2.69002233]. 
Working on 1.0 fold... 
[Fri Apr  5 12:45:12 2024]  Iteration number: 0 with current cost as 0.3032998345453765 and parameters 
[ 1.35806187  2.23743448 -2.12427964 -0.11653103  0.55388693 -2.77010928
  3.06858483  2.1896013   1.18551998 -1.06648324  0.60271495  1.1443243
  1.31029899 -1.8735468   0.7296505   2.88578389 -0.5453435  -0.47522485
 -2.02654256  0.7289737   1.60512664  2.83077092 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.1333714   2.54856958 -0.67550787 -2.69002217]. 
Training complete taking 2230.203233242035 seconds. 
Discarding model... 

Training complete taking 54680.303158283234 total seconds. 
Now scoring model... 
Scoring complete taking 2.3183672428131104 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.5000432171860141,), 'R2_train': 0.004296235497681211, 'MAE_train': 0.6162022730962173, 'MSE_test': 0.540605205554071, 'R2_test': -0.008209760349904949, 'MAE_test': 0.6648572309218637}. 
Saved model results as A1-A1-CNOT_Full-CRX_results.json. 
