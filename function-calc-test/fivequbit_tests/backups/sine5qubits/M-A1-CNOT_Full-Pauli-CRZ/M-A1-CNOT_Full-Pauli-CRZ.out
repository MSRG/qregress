/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:39 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:37:58 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 21:43:47 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 21:49:06 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:54:15 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 21:59:53 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1665.4404275417328 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:05:44 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:11:22 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:16:39 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:21:51 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:27:19 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1643.054498910904 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:33:06 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 22:38:55 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 22:44:17 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:49:28 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 22:54:57 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1662.8640944957733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:00:53 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:06:40 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:11:58 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:17:43 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:23:13 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1690.0458207130432 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:29:00 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Thu Apr  4 23:34:37 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Apr  4 23:39:55 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:45:02 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Apr  4 23:51:00 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1666.4063715934753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:56:44 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:02:22 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:07:39 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:13:01 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:18:33 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1661.3871603012085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:24:27 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:30:04 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 00:36:06 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:41:18 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 00:46:49 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1722.5004477500916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:53:10 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 00:59:07 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:04:29 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:09:38 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:15:17 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1673.2486517429352 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:21:03 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:26:44 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:32:12 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:37:27 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 01:42:57 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1670.9196150302887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:48:52 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 01:54:37 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 01:59:54 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:05:04 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:10:31 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1644.6621985435486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:16:17 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:21:54 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:13 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:32:31 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 02:37:56 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1666.4789159297943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:44:05 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 02:49:43 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 02:55:07 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:00:28 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:06:03 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1677.6460146903992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:12:03 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 03:17:40 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:23:08 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:28:24 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 03:33:54 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1666.3658561706543 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 03:39:52 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 03:45:32 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 03:50:50 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:56:12 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:01:41 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1679.4841566085815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:07:49 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:13:24 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:18:42 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:23:50 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:29:17 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1636.6521625518799 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:35:04 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 04:40:50 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 04:46:27 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:52:05 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 04:57:32 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1695.3596036434174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:03:21 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:08:56 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:14:16 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:19:24 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:24:54 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1639.6381769180298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:30:39 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 05:36:23 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 05:41:38 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:46:47 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 05:52:48 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1674.8737878799438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:58:34 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:04:13 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:09:57 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:15:06 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:20:32 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1661.9796133041382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:26:17 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:31:54 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 06:37:11 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 06:42:20 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 06:47:49 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1639.3362386226654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 06:53:38 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 06:59:14 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:04:32 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:09:39 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:15:07 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1637.898191690445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 07:20:55 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:26:41 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:32:00 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 07:37:07 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 07:42:37 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1669.4654269218445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 07:48:42 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 07:54:19 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 07:59:37 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:04:46 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:10:13 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1636.3400225639343 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 08:16:00 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 08:21:37 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:26:56 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 08:32:24 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 08:38:03 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1672.1699130535126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 08:43:52 2024]  Iteration number: 0 with current cost as 0.3308624650688673 and parameters 
[-3.27356161  2.36511364 -2.11428175 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.65821622  1.14432446
  1.71740718 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.4 fold... 
[Fri Apr  5 08:49:29 2024]  Iteration number: 0 with current cost as 0.3312435044368033 and parameters 
[-3.20560415  2.38846446 -2.11358789 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.66697503  1.14432445
  1.63503906 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Fri Apr  5 08:54:50 2024]  Iteration number: 0 with current cost as 0.3363892071334943 and parameters 
[-3.22281284  2.39490522 -2.11020869 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960144  1.18551998 -1.06648308  0.66674223  1.14432445
  1.65365438 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522486
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 09:00:00 2024]  Iteration number: 0 with current cost as 0.31045327265781175 and parameters 
[-3.21578076  2.35391706 -2.12130939 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551999 -1.06648308  0.66152921  1.14432445
  1.6541775  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Fri Apr  5 09:05:49 2024]  Iteration number: 0 with current cost as 0.3284795815822913 and parameters 
[-3.26063834  2.38119586 -2.11226128 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.66216381  1.14432445
  1.69890273 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Training complete taking 1663.595561504364 seconds. 
Discarding model... 

Training complete taking 41617.81439828873 total seconds. 
Now scoring model... 
Scoring complete taking 0.954110860824585 seconds. 
Saved predicted values as M-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.3195778704997462,), 'R2_train': 0.3636452255088567, 'MAE_train': 0.49031707693221876, 'MSE_test': 0.37583758774290493, 'R2_test': 0.29907607182509643, 'MAE_test': 0.5396119729425752}. 
Saved model results as M-A1-CNOT_Full-Pauli-CRZ_results.json. 
