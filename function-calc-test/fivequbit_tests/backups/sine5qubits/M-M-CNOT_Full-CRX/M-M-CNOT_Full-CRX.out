/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:31 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:44:36 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:53:39 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:03:41 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Thu Apr  4 22:16:39 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:27:02 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3386.1317467689514 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:40:54 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:50:06 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:00:12 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Thu Apr  4 23:13:00 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:23:17 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3382.1729369163513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:37:29 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:46:49 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:56:53 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 00:10:03 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:20:17 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3440.3149750232697 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:34:41 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:43:54 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:54:33 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 01:08:06 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:18:31 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3485.4190816879272 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:33:02 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:42:09 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:51:41 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 02:04:07 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:13:46 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3279.206915616989 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:27:09 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:35:59 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:45:52 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 02:58:48 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:08:37 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3300.858694791794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:22:01 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 03:30:17 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:39:57 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 03:53:14 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:03:20 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3305.274262905121 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:17:41 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 04:27:09 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:36:56 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 04:50:12 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:00:24 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3415.9019677639008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:14:32 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 05:23:30 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:33:24 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 05:46:02 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:55:52 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3340.5600090026855 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:10:26 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 06:19:41 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:30:14 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 06:43:25 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:53:10 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3421.504054069519 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 07:06:57 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 07:15:54 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 07:25:40 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 07:38:19 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 07:48:18 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3302.306906938553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 08:02:01 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 08:10:41 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 08:20:06 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 08:32:55 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 08:42:29 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3253.4099671840668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 08:56:08 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:04:46 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 09:14:23 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 09:27:11 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 09:36:54 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3269.858958005905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 09:50:44 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 09:59:32 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 10:09:24 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 10:22:06 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 10:32:03 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3312.932954311371 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 10:45:55 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 10:54:41 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:04:27 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 11:17:32 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 11:27:08 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3268.396948814392 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 11:40:23 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 11:49:32 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 11:59:11 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 12:11:41 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 12:21:56 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3320.4770991802216 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 12:35:42 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 12:44:32 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 12:54:15 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 13:06:45 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 13:16:11 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3238.120645046234 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 13:29:23 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 13:38:23 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 13:48:01 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 14:00:51 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 14:10:29 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3265.869292974472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 14:24:08 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 14:33:11 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 14:42:52 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 14:55:29 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 15:05:11 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3290.450993537903 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 15:19:05 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 15:27:58 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 15:37:59 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 15:50:59 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:01:16 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3339.0655534267426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 16:14:44 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 16:23:42 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 16:33:53 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 16:46:47 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 16:57:01 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3394.2601687908173 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 17:11:25 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 17:20:07 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 17:29:38 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 17:42:08 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 17:52:04 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3269.592253923416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 18:05:59 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 18:14:31 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 18:24:29 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 18:37:03 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 18:46:49 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3285.3859734535217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 19:00:22 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 19:08:52 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 19:18:45 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 19:31:06 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 19:40:46 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3237.2420496940613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 19:54:34 2024]  Iteration number: 0 with current cost as 0.24221701830883216 and parameters 
[ 1.36565611  2.23743464 -2.12427931 -0.11653086  0.55388724 -2.77010881
  3.06858498  2.18960162  1.18551998 -1.06648308  0.6027151   1.14432478
  1.31029915 -1.8735468   0.7296508   2.88578403 -0.54534319 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077091 -1.26456693 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 20:03:05 2024]  Iteration number: 0 with current cost as 0.3708304824455289 and parameters 
[11.25917721  2.23743464 -2.124279   -0.11653039  0.55388835 -2.77010834
  3.06858435  2.18960209  1.18552125 -1.06648245  0.60271574  1.14432508
  1.31030025 -1.87354617  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 20:12:14 2024]  Iteration number: 0 with current cost as 0.3769550224216388 and parameters 
[11.00344079  2.237434   -2.124279   -0.11653103  0.55388708 -2.77010897
  3.06858435  2.18960209  1.18552062 -1.06648308  0.6027151   1.14432572
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654304  0.7289737   1.60512664  2.8307698  -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002265]. 
Working on 0.8 fold... 
[Fri Apr  5 20:24:49 2024]  Iteration number: 0 with current cost as 0.2493114851340123 and parameters 
[ 1.31247515  2.23743464 -2.12427947 -0.11653069  0.55388708 -2.77010897
  3.06858498  2.18960179  1.18552032 -1.06648308  0.60271544  1.14432478
  1.31029915 -1.87354647  0.7296508   2.88578419 -0.54534318 -0.47522452
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 20:34:10 2024]  Iteration number: 0 with current cost as 0.6176603482739749 and parameters 
[12.35305744  2.23743464 -2.12427837 -0.11652976  0.55388708 -2.77010897
  3.06858498  2.18960272  1.18552125 -1.06648308  0.60271574  1.14432572
  1.31030025 -1.87354553  0.7296508   2.88578419 -0.54534272 -0.47522422
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456646 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550724 -2.69002202]. 
Training complete taking 3176.0553040504456 seconds. 
Discarding model... 

Training complete taking 82980.77034783363 total seconds. 
Now scoring model... 
Scoring complete taking 2.730661153793335 seconds. 
Saved predicted values as M-M-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (0.4071508973366257,), 'R2_train': 0.18926671282540874, 'MAE_train': 0.5538781925598177, 'MSE_test': 0.41711844645070667, 'R2_test': 0.22208871721356505, 'MAE_test': 0.571355506959325}. 
Saved model results as M-M-CNOT_Full-CRX_results.json. 
