/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/sine5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/sine5qubits/sine_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/sine5qubits/sine_train.bin 
 at time Thu Apr  4 21:37:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:39:58 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:43:43 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:48:26 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:52:10 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:56:14 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1219.682380914688 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:00:21 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:04:08 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:09:02 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:12:48 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:16:47 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1234.1180012226105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:20:58 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:24:50 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:29:42 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:33:40 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:37:55 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1272.9109365940094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:42:10 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:45:55 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:50:40 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:54:26 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:58:52 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1261.719990491867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:03:10 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:07:03 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:12:16 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:16:08 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:20:26 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1297.193123102188 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:24:55 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:28:45 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:33:43 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:37:29 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:41:48 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1278.5418555736542 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:46:06 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:49:53 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:54:45 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:58:35 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:03:03 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1271.8490028381348 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:07:25 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:11:30 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:16:44 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:20:45 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:25:13 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1332.3759765625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:29:27 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:33:28 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:38:34 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:42:37 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:46:59 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1302.2264740467072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:51:22 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:55:31 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:00:31 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:04:33 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:08:47 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1308.1284520626068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 01:13:01 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:17:06 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:22:21 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:26:18 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:30:46 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1318.7064173221588 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 01:35:07 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:39:00 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:44:10 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:48:08 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:52:31 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1303.9099373817444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:56:43 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:00:40 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:05:58 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:09:48 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:14:05 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1298.5915751457214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:18:25 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:22:27 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:27:34 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:31:25 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:35:50 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1296.5862548351288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:40:09 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 02:44:13 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 02:49:28 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 02:53:22 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 02:57:40 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1318.743432044983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:01:57 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:06:08 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:11:18 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:15:16 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:19:47 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1326.565292596817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 03:24:09 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:28:06 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:33:07 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:37:09 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 03:41:25 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1302.4290103912354 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:45:52 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 03:49:46 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 03:54:46 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 03:58:48 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:03:06 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1297.205931186676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:07:27 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:11:17 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:16:14 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:20:08 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:24:23 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1273.5640857219696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 04:28:31 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:32:18 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:37:31 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 04:41:30 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 04:45:57 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1301.187875032425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 04:50:17 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 04:54:14 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 04:59:12 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 05:03:04 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 05:07:37 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1291.0437169075012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 05:12:02 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:15:59 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 05:21:06 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 05:24:53 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 05:29:10 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1300.9647932052612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 05:33:36 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:37:30 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 05:42:41 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 05:46:32 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 05:50:46 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1288.7794351577759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:55:03 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 05:58:52 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 06:03:56 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 06:07:53 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 06:12:02 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1275.1147637367249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:16:12 2024]  Iteration number: 0 with current cost as 0.49806911951662386 and parameters 
[-1.59746767  2.23743464 -2.12427928 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Fri Apr  5 06:20:05 2024]  Iteration number: 0 with current cost as 0.4990018842107823 and parameters 
[-1.70535713  2.23743464 -2.12427939 -0.11653078  0.55388708 -2.77010922
  3.06858498  2.18960145  1.18552023 -1.06648308  0.60271535  1.1443247
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Fri Apr  5 06:24:58 2024]  Iteration number: 0 with current cost as 0.15865953809563307 and parameters 
[ 1.60727036  2.23743464 -2.12427888 -0.11653103  0.55388633 -2.77010897
  3.06858423  2.18960221  1.18552074 -1.06648384  0.60271586  1.1443252
  1.31029974 -1.8735468 ]. 
Working on 0.8 fold... 
[Fri Apr  5 06:28:50 2024]  Iteration number: 0 with current cost as 0.48691723275544996 and parameters 
[-1.62060706  2.23743439 -2.12427939 -0.11653078  0.55388732 -2.77010873
  3.06858523  2.1896017   1.18552023 -1.06648333  0.60271535  1.14432469
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Fri Apr  5 06:33:07 2024]  Iteration number: 0 with current cost as 0.15191368628200047 and parameters 
[ 1.5406743   2.23743464 -2.12427817 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552145 -1.06648308  0.60271584  1.14432518
  1.31029899 -1.8735468 ]. 
Training complete taking 1262.8027791976929 seconds. 
Discarding model... 

Training complete taking 32234.942154169083 total seconds. 
Now scoring model... 
Scoring complete taking 3.2057807445526123 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (0.259757756193231,), 'R2_train': 0.4827611558141346, 'MAE_train': 0.4330897409675714, 'MSE_test': 0.29390023043107255, 'R2_test': 0.4518863713382122, 'MAE_test': 0.48785119962045165}. 
Saved model results as M-M-CZ_Efficient-CRZ_results.json. 
