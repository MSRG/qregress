/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:03 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:37 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:31:17 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:31:57 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:33:20 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:34:00 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 252.74094080924988 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:49 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:30 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:36:08 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:37:33 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:38:12 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 253.2681007385254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:39:04 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:42 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:40:25 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:41:47 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:42:27 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 254.02520775794983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:43:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:09 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:44:48 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:46:11 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 264.04038286209106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:47:47 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:48:27 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:49:06 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:30 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:09 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.8005437850952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:00 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:40 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:19 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:43 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:23 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 254.63352489471436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:16 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:56 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:34 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:59 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:37 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.351096868515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:46 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:28 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:07 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:32 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:11 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 267.69737124443054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:01 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:05:40 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:06:19 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:07:42 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:21 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 256.04318714141846 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:09:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:58 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:36 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:12:02 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:12:41 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 254.8208601474762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:13:33 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:14:12 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:50 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:16:21 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:01 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.4807333946228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:50 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:32 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:18 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:47 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:27 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 266.1870331764221 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:22:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:57 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:37 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:05 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:44 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.93559527397156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:35 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:13 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:52 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:22 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:02 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 257.1134696006775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:30:52 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:31:38 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:32:17 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:33:42 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:34:22 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 261.3123013973236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:13 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:52 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:32 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:20 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:00 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 275.99897146224976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:49 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:33 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:41:12 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:38 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:43:16 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 258.00190687179565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:07 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:46 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:24 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:53 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:33 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.40513491630554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:23 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:04 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:43 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:13 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:51:51 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 259.6226952075958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:43 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:22 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:54:03 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:27 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:06 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 253.56268048286438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:57 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:57:36 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:58:15 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:39 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:18 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 253.064875125885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:09 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:47 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:02:35 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:02 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:41 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 263.11075353622437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:31 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:06:10 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:48 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:12 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 248.4268147945404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:45 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:23 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:11:03 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:25 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:14 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 265.5717418193817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:14:05 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:44 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:15:22 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:16:45 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:23 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 247.81985449790955 seconds. 
Discarding model... 

Training complete taking 6457.036739587784 total seconds. 
Now scoring model... 
Scoring complete taking 0.8369026184082031 seconds. 
Saved predicted values as A2_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.3130300309287,), 'R2_train': -0.2611292653031918, 'MAE_train': 13.7800749603307, 'MSE_test': 122.84974676405518, 'R2_test': 0.25948458761070614, 'MAE_test': 10.149093254668012}. 
Saved model results as A2_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:30:36 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:31:11 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:31:49 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:32:27 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:33:50 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:34:28 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 249.05279231071472 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:35:18 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:35:56 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:36:34 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:37:58 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:38:36 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 247.98032999038696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:39:26 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:40:04 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:44 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:42:12 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:42:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 252.5900342464447 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:40 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:44:18 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:44:57 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:46:22 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:47:01 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 252.37087988853455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 11:47:51 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:48:29 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:49:08 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:50:31 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:51:11 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 248.44292569160461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:51:59 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:52:38 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:18 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:54:44 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:55:22 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 251.1019458770752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:56:12 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 11:56:50 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 11:57:29 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 11:58:52 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:59:30 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 250.81478214263916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:00:21 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:01:00 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:01:38 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:03:01 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:03:39 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 247.45067882537842 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:04:29 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:05:07 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:05:46 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:07:07 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:07:46 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 246.22069025039673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:08:35 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:09:15 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:10:00 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:11:31 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:12:11 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 265.166951417923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:13:01 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:13:45 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:14:23 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:15:47 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:16:26 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.90039229393005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:17:17 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:17:55 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:18:33 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:19:56 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:20:34 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 248.5416100025177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:21:25 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:22:03 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:22:43 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:24:04 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:24:48 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 251.91101264953613 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:25:37 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:26:18 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:26:56 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:28:19 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:28:57 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 250.94256043434143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:48 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:26 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:06 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:32:36 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:33:20 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 260.9395225048065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:34:22 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:00 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:35:38 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:37:01 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:37:39 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 260.22982478141785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:38:28 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:39:07 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:39:46 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:08 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:41:49 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 248.70506191253662 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:42:37 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:43:17 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:43:56 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:45:19 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:45:57 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 249.5045816898346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:46:47 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:47:25 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:48:03 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:49:26 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:50:04 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 247.22826218605042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:54 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:32 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:45 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:07 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:54:50 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 284.24855160713196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:55:38 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:56:18 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:57 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:19 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:57 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 249.2078354358673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:57 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:00:37 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:01:17 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:02:38 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:03:27 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 267.7727928161621 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:04:23 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:01 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:05:40 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:07:03 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:07:41 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 255.62358450889587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:08:30 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:08 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:09:48 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:11:09 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:11:49 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 246.43729066848755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:37 2024]  Iteration number: 0 with current cost as 0.4549975176651845 and parameters 
[-4.26569995  2.23743453 -2.12427964 -0.11653113  0.55388708 -2.77010908
  3.06858488  2.18960145  1.18551998 -1.0664833   0.6027151   1.14432445
  1.31029888 -1.87354648  0.72965059  2.88578409 -0.54534346 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:13:16 2024]  Iteration number: 0 with current cost as 0.37204585612792307 and parameters 
[-4.45269738  2.23743432 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578357 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:13:57 2024]  Iteration number: 0 with current cost as 0.40117453142917275 and parameters 
[-4.37377097  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.60271479  1.14432414
  1.31029899 -1.8735468   0.72965018  2.88578388 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:15:20 2024]  Iteration number: 0 with current cost as 0.3470058588267788 and parameters 
[-20.14851508   2.23743464  -2.12426934  -0.11653103   0.55388708
  -2.77010897   3.06858498   2.1896066    1.18552513  -1.06648308
   0.6027151    1.14432445   1.31029899  -1.87353651   0.72964566
   2.88577905  -0.54534335  -0.47522485  -2.0265424    0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:58 2024]  Iteration number: 0 with current cost as 0.4019619828723463 and parameters 
[-4.43042718  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965018  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 248.87669324874878 seconds. 
Discarding model... 

Training complete taking 6337.262987613678 total seconds. 
Now scoring model... 
Scoring complete taking 0.8048045635223389 seconds. 
Saved predicted values as A2_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (260.3130300309287,), 'R2_train': -0.2611292653031918, 'MAE_train': 13.7800749603307, 'MSE_test': 122.84974676405518, 'R2_test': 0.25948458761070614, 'MAE_test': 10.149093254668012}. 
Saved model results as A2_Modified-Pauli-CRZ_results.json. 
