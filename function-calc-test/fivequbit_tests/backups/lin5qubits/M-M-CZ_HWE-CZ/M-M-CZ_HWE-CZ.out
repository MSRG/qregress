/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:16 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:18 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:19 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:20 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:22 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 307.6757526397705 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:40:23 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:23 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:26 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:28 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:33 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 311.3866913318634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:34 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:35 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:34 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:48:36 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:49:40 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 306.3723373413086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:50:40 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:51:40 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:52:43 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:53:42 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:54:42 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 303.0949184894562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:43 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:56:46 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:57:46 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:47 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:47 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 303.96282029151917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:48 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:01:51 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:02:53 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:54 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:55 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 308.1325144767761 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:56 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:06:55 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:07:56 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:08:56 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:09:57 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 301.5490860939026 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:58 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:11:59 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:13:03 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:04 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:15:06 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 309.70178627967834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:06 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:07 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:06 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:19:06 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:20:05 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 299.4780464172363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:06 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:05 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:06 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:24:06 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:25:06 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 299.4145815372467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:26:05 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:27:05 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:28:05 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:08 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:30:06 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 304.99219489097595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:11 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:32:14 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:33:14 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:14 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:35:16 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 306.35345244407654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:17 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:18 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:21 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:22 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:24 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 308.82243752479553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:41:27 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:29 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:35 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:44:36 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:39 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 314.86921095848083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:46:40 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:47:42 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:45 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:49 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:50:52 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 312.4578149318695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:51:54 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:52:56 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:58 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:55:01 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:56:04 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 313.37516927719116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:57:07 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:10 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:12 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:16 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:19 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 314.42074251174927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:21 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:22 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:24 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:05:28 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:06:32 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 313.50697588920593 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:35 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:08:37 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:09:39 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:10:44 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:11:49 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 316.4757068157196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:12:52 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:13:54 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:56 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:15:58 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:17:01 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 316.60505056381226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:18:08 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:12 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:17 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:22 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:22:27 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 324.33166694641113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:34 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:24:39 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:25:43 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:26:48 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:55 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 326.80300211906433 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:29:00 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:30:03 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:08 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:14 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:33:19 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 325.74585247039795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:34:25 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:35:28 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:35 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:37:47 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:59 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 336.8454303741455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:40:03 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:06 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:42:10 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:43:13 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:17 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 321.82405042648315 seconds. 
Discarding model... 

Training complete taking 7808.199333429337 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.1671569347381592 seconds. 
Saved predicted values as M-M-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (79.39266763449696,), 'R2_train': 0.6153691707539284, 'MAE_train': 5.876818643951188, 'MSE_test': 92.18149258458158, 'R2_test': 0.4443471167503872, 'MAE_test': 6.031775482207715}. 
Saved model results as M-M-CZ_HWE-CZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:21:36 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:21:51 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:00 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:24:08 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:25:16 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:26:23 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 340.98820328712463 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:27:33 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:28:41 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:29:50 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:59 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:32:08 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 344.3064544200897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:33:16 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:23 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:35:33 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:36:41 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:37:48 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 340.93867111206055 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:38:57 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:40:06 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:41:14 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:42:22 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:34 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 343.92446517944336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:44:42 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:45:51 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:47:00 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:48:08 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:16 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 343.72161531448364 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:24 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:34 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:42 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:50 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:54:59 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 342.08057141304016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:56:11 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:57:20 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:58:28 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:59:37 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:00:46 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 347.6717600822449 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:01:54 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:03:23 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:33 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:05:41 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:06:48 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 362.6035101413727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:07:57 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:06 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:10:14 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:11:23 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:12:32 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 343.46264123916626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:13:41 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:48 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:56 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:10 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:17 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 344.0860710144043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:24 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:20:33 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:40 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:47 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:23:55 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 338.2967541217804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:25:04 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:26:10 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:27:17 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:28:25 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:29:37 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 341.137770652771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:45 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:31:53 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:33:00 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:11 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:19 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 342.30163645744324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:26 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:37:35 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:38:43 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:39:50 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:40:57 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 338.7275116443634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:42:06 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:43:34 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:44:42 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:45:52 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:46:59 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 361.5965554714203 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:48:07 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:49:15 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:50:22 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:51:29 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:52:41 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 343.1186258792877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:53:49 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:55:00 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:56:19 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:57:25 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:58:34 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 367.7191686630249 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:59:57 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:01:05 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:02:13 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:03:23 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:04:42 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 352.13622069358826 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:05:49 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:08 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:08:26 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:33 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:10:56 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 374.78894543647766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:05 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:13:12 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:14:18 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:15:25 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:16:32 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 336.3746304512024 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:41 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:18:48 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:55 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:21:02 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:22:31 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 357.69499492645264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:23:39 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:24:48 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:55 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:27:10 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:28:17 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 346.90818071365356 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:25 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:30:32 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:31:41 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:32:52 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:34:00 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 342.5961449146271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:09 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:36:16 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:37:34 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:38:42 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:39:50 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 349.93270921707153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:40:57 2024]  Iteration number: 0 with current cost as 0.2099640271719388 and parameters 
[-3.03776317  1.91932187 -1.65680678 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:42:05 2024]  Iteration number: 0 with current cost as 0.17973407905363686 and parameters 
[-3.07107061  1.97835155 -1.64930028 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:43:14 2024]  Iteration number: 0 with current cost as 0.17869443231924204 and parameters 
[-3.06912855  1.96398118 -1.64115529 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:44:21 2024]  Iteration number: 0 with current cost as 0.18725564795840005 and parameters 
[-3.05983954  1.97283196 -1.66312854 -0.11653103  0.55388708 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:45:29 2024]  Iteration number: 0 with current cost as 0.1847733858270345 and parameters 
[-3.07030053  1.95934946 -1.63562244 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60271511  1.14432445
  1.31029898 -1.8735468   0.7296508 ]. 
Training complete taking 339.10958671569824 seconds. 
Discarding model... 

Training complete taking 8686.224355220795 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 0.9992432594299316 seconds. 
Saved predicted values as M-M-CZ_HWE-CZ_predicted_values.csv
Model scores: {'MSE_train': (79.39266763449696,), 'R2_train': 0.6153691707539284, 'MAE_train': 5.876818643951188, 'MSE_test': 92.18149258458158, 'R2_test': 0.4443471167503872, 'MAE_test': 6.031775482207715}. 
Saved model results as M-M-CZ_HWE-CZ_results.json. 
