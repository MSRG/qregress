/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:09 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:25 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 17:35:28 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:53 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 17:47:54 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 17:53:32 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1666.8887584209442 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:58:10 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 18:03:12 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:38 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:38 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:13 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1661.8314752578735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:53 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:54 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:27 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:26 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:06 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1675.5170803070068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:53:47 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:48 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:10 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:11 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:48 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1660.0252714157104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:29 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:24 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 19:33:41 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:38 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 19:44:10 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.274522781372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:50 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 19:53:49 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 20:01:08 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 20:06:06 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 20:11:38 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.5358793735504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:16:14 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 20:21:20 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 20:29:03 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 20:34:00 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:33 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1676.5228946208954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:44:10 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 20:49:08 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 20:56:24 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 21:01:23 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 21:07:09 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1655.1232287883759 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:45 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 21:16:43 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 21:24:03 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:02 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:37 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1644.432956457138 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:39:11 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 21:44:21 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 21:51:46 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 21:56:45 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 22:02:16 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1662.0172367095947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:52 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 22:11:56 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 22:19:12 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:10 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:43 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1647.1617381572723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:34:19 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 22:39:17 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 22:46:35 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:29 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:01 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1644.008466720581 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:01:45 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:47 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 23:14:09 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 23:19:13 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:44 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1654.1702947616577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:29:19 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Sun Mar 24 23:34:17 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Sun Mar 24 23:41:35 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:35 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Sun Mar 24 23:52:12 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1662.5414462089539 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:57:00 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 00:01:57 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 00:09:17 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:20 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 00:19:58 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1653.4236710071564 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:24:33 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 00:29:30 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 00:36:51 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 00:42:03 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 00:47:39 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1661.9783356189728 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:52:17 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 00:57:14 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 01:04:31 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:28 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 01:15:03 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.3387598991394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:19:39 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 01:24:37 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 01:31:55 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 01:36:51 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 01:42:24 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1643.4728498458862 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:02 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 01:52:01 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 01:59:27 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 02:04:25 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:01 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1655.3939633369446 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:37 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 02:19:47 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 02:27:09 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:17 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 02:37:52 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1673.992380619049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:42:32 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 02:47:29 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 02:54:52 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 02:59:50 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 03:05:23 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1644.1162133216858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:09:57 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 03:14:52 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 03:22:11 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 03:27:12 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 03:33:02 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1666.3738770484924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 03:37:42 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 03:42:43 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 03:50:07 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 03:55:10 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 04:00:45 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1660.9125747680664 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:24 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 04:10:21 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 04:17:39 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 04:22:36 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:08 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1642.319568157196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:32:45 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:43 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Mon Mar 25 04:45:02 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Mon Mar 25 04:49:59 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Mon Mar 25 04:55:30 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1638.4801180362701 seconds. 
Discarding model... 

Training complete taking 41380.85525012016 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0283408164978027 seconds. 
Saved predicted values as M-M-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (97.64789816067427,), 'R2_train': 0.5269287056005588, 'MAE_train': 8.422954687714341, 'MSE_test': 108.54136299524139, 'R2_test': 0.34573286232255607, 'MAE_test': 8.789224388295931}. 
Saved model results as M-M-CZ_HWE-CNOT_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:40:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:41:11 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 11:46:06 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 11:53:24 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 11:58:16 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 12:03:50 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1630.0978693962097 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:08:20 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 12:13:13 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 12:20:26 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 12:25:19 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 12:30:45 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1618.9813723564148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:19 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 12:40:16 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 12:47:30 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 12:52:28 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 12:58:20 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1652.587960243225 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:02:52 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 13:07:47 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:56 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 13:19:49 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:20 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1626.0807464122772 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:29:58 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 13:34:52 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 13:42:04 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:58 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 13:52:26 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1617.6638872623444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:56 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 14:01:49 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 14:09:16 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 14:14:14 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 14:20:11 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1696.779095172882 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:25:11 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 14:30:29 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 14:37:50 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 14:42:46 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 14:48:41 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1704.0436570644379 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:53:36 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 14:58:31 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 15:05:42 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 15:10:36 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 15:16:11 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1626.7604067325592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:20:42 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 15:25:35 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 15:33:14 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 15:38:12 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 15:43:46 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1655.479084968567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:48:17 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 15:53:09 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 16:00:55 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 16:05:50 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 16:11:39 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1679.2987446784973 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:16:17 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 16:21:08 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 16:28:21 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 16:33:38 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 16:39:18 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1675.2198657989502 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:44:12 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 16:49:05 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 16:56:17 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 17:01:08 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 17:06:36 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1615.677169561386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:11:09 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 17:16:03 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 17:23:21 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 17:28:43 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 17:34:25 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1669.3810427188873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:38:57 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 17:43:52 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 17:51:07 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 17:56:00 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 18:01:27 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1619.9450833797455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:05:57 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 18:10:49 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 18:18:07 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 18:23:01 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 18:28:29 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1623.4055304527283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:33:00 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 18:38:37 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 18:45:56 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 18:51:17 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 18:56:48 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1734.5110626220703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:01:55 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 19:06:49 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 19:14:24 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 19:19:17 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 19:24:47 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1650.6503412723541 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:29:26 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 19:34:20 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 19:41:30 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 19:46:23 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 19:51:49 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1614.837218761444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:56:22 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 20:01:16 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 20:08:32 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 20:13:25 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 20:18:51 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1621.4642443656921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:23:23 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 20:28:44 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 20:35:57 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 20:40:50 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 20:46:26 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1654.545270204544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:50:56 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 20:55:53 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 21:03:26 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 21:08:20 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 21:13:48 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1649.1688213348389 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:18:27 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 21:23:18 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 21:30:35 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 21:35:30 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 21:40:58 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1628.7248413562775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:45:34 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:34 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 21:57:55 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 22:02:53 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 22:08:21 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1646.7573716640472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:13:01 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 22:18:10 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 22:25:30 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 22:30:21 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 22:35:50 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1639.1927807331085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:40:22 2024]  Iteration number: 0 with current cost as 0.3146732937015906 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.0674937   0.5171293  -2.77080606
  3.11210368  2.17782927  1.11720004 -1.04875873  0.63013156  1.17441178
  1.37249098 -1.82619188  0.69058388]. 
Working on 0.4 fold... 
[Thu Apr  4 22:45:15 2024]  Iteration number: 0 with current cost as 0.27798865544178286 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.06139447  0.52239779 -2.75527058
  3.13713613  2.16336675  1.08228166 -1.04553501  0.61246842  1.17280251
  1.39547379 -1.81170691  0.66535382]. 
Working on 0.6 fold... 
[Thu Apr  4 22:52:30 2024]  Iteration number: 0 with current cost as 0.27910512596608716 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0684442   0.51714257 -2.7719026
  3.11820058  2.16938726  1.11149699 -1.05310007  0.62181522  1.16654172
  1.37747142 -1.82277001  0.68612935]. 
Working on 0.8 fold... 
[Thu Apr  4 22:57:23 2024]  Iteration number: 0 with current cost as 0.2815775128148261 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.06580698  0.51761745 -2.76804785
  3.13071785  2.15976409  1.09539819 -1.04697907  0.6148955   1.17180843
  1.38243477 -1.82024811  0.67851028]. 
Working on 1.0 fold... 
[Thu Apr  4 23:02:49 2024]  Iteration number: 0 with current cost as 0.29022848949002056 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.07079025  0.52509422 -2.76203737
  3.1239439   2.18098624  1.09499017 -1.04804443  0.62837354  1.17472921
  1.37708008 -1.82523398  0.67862102]. 
Training complete taking 1619.6439785957336 seconds. 
Discarding model... 

Training complete taking 41170.89956879616 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0801331996917725 seconds. 
Saved predicted values as M-M-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (97.64789816067427,), 'R2_train': 0.5269287056005588, 'MAE_train': 8.422954687714341, 'MSE_test': 108.54136299524139, 'R2_test': 0.34573286232255607, 'MAE_test': 8.789224388295931}. 
Saved model results as M-M-CZ_HWE-CNOT_results.json. 
