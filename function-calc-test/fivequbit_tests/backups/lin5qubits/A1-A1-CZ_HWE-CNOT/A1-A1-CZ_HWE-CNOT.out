/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:52:01 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:52:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 17:57:24 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:35 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 18:04:27 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:40 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1130.9416408538818 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:03 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 18:16:08 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 18:16:23 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:45 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 18:23:15 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:36 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1133.622608423233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:29:58 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 18:35:06 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:17 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:38:38 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 18:42:06 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:21 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1122.4118280410767 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:39 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 18:53:43 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 18:53:55 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:14 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:00:57 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 19:04:17 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1138.6179132461548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:38 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 19:12:39 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:51 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:12 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:37 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:04 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1139.0177710056305 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:48 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 19:31:53 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 19:32:04 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:24 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:53 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 19:42:25 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1162.7524764537811 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:46:01 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 19:51:04 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 19:51:15 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:38 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 19:58:10 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:32 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1138.6961674690247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:59 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 20:10:34 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 20:10:45 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 20:14:10 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:36 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:50 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1154.153415441513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 20:29:27 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:39 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:00 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:30 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:08 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1154.7573540210724 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:29 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 20:48:32 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 20:48:43 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:04 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 20:55:30 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:44 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1117.761477470398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:02:06 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 21:07:08 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 21:07:19 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 21:10:39 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 21:14:08 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 21:17:27 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1125.9417488574982 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:20:51 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 21:26:48 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 21:27:00 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 21:30:19 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 21:33:58 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 21:37:14 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1188.3268644809723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:40:40 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 21:45:42 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 21:45:53 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 21:49:41 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:23 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:51 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1171.4669888019562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 22:05:13 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 22:05:24 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:43 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 22:12:08 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 22:15:22 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1110.486501455307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:18:42 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 22:23:44 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 22:23:55 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 22:27:15 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 22:30:41 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 22:33:55 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1114.1079804897308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:37:15 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 22:42:16 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 22:42:27 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 22:45:47 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 22:49:13 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 22:52:28 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1113.2823169231415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:55:48 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:00:51 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:01:02 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 23:04:23 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 23:07:49 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 23:11:03 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1115.7810082435608 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:14:24 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:19:28 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:41 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 23:23:00 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 23:26:25 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 23:29:40 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1118.5329885482788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:33:03 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:38:04 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:38:17 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Sun Mar 24 23:41:34 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Sun Mar 24 23:45:00 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Sun Mar 24 23:48:14 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1110.4852755069733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:51:33 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Sun Mar 24 23:56:36 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Sun Mar 24 23:56:48 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:00:09 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:35 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 00:06:58 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1123.8364670276642 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:10:18 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 00:15:25 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 00:15:36 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:19:16 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 00:22:42 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 00:26:20 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1161.9018802642822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:29:39 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 00:34:48 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 00:34:59 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:38:19 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 00:41:51 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 00:45:05 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1134.2586116790771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:48:33 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 00:53:37 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 00:53:50 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 00:57:12 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 01:00:40 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 01:03:56 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1126.4500615596771 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:07:21 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 01:12:36 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 01:12:49 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 01:16:07 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 01:19:39 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 01:22:59 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1139.1768617630005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:26:20 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Mon Mar 25 01:31:23 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Mon Mar 25 01:31:34 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Mon Mar 25 01:34:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Mon Mar 25 01:38:26 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Mon Mar 25 01:41:39 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1122.4403219223022 seconds. 
Discarding model... 

Training complete taking 28369.209784030914 total seconds. 
Now scoring model... 
Scoring complete taking 0.9097073078155518 seconds. 
Saved predicted values as A1-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (92.77359297339554,), 'R2_train': 0.5505430783385121, 'MAE_train': 7.815642370651912, 'MSE_test': 108.54235460391446, 'R2_test': 0.34572688509000316, 'MAE_test': 8.255832775750633}. 
Saved model results as A1-A1-CZ_HWE-CNOT_results.json. 
/home/gjones/scratch/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:07 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:19 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 12:34:17 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:28 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:42 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 12:41:04 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 12:44:13 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1101.1850411891937 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:47:39 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 12:52:33 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 12:52:44 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 12:55:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 12:59:17 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 13:02:27 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1082.297524690628 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:05:41 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 13:10:34 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 13:10:45 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 13:13:59 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:17 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 13:20:26 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1078.2132308483124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:23:39 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 13:28:33 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 13:28:44 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 13:31:59 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 13:35:18 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 13:38:33 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1088.7698817253113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:41:48 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 13:46:50 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 13:47:01 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 13:50:15 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:40 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 13:56:49 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1094.1990339756012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:02 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 14:04:56 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:07 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 14:08:35 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 14:11:53 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 14:15:02 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1094.7905008792877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:18:17 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 14:23:13 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 14:23:26 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 14:26:41 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 14:30:00 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 14:33:08 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1084.9996058940887 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:36:22 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 14:41:17 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 14:41:29 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 14:44:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 14:48:17 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 14:51:27 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1098.876779794693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:54:41 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 14:59:34 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 14:59:45 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 15:02:59 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 15:06:38 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 15:09:48 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1101.4700253009796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:13:03 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 15:18:31 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 15:18:42 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 15:22:09 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 15:25:28 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 15:28:37 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1128.390564441681 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:31:51 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 15:36:45 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 15:36:56 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 15:40:10 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 15:43:31 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 15:46:38 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1080.7293953895569 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:49:51 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 15:55:19 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 15:55:31 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 15:58:45 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 16:02:04 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 16:05:13 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1115.9608891010284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:08:28 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 16:13:24 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 16:13:36 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 16:16:50 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 16:20:08 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 16:23:15 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1080.9681656360626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:26:29 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 16:31:23 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 16:31:35 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 16:34:50 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 16:38:09 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 16:41:21 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1085.8618178367615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:44:37 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 16:49:28 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 16:49:40 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:54 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 16:56:14 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 16:59:22 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1082.132083415985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:02:38 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 17:07:31 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 17:07:43 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 17:10:58 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 17:14:18 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 17:17:26 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1083.398656129837 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:20:41 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 17:25:36 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 17:25:47 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 17:29:01 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 17:32:21 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 17:35:30 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1084.296674489975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:38:45 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 17:43:40 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 17:43:51 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 17:47:05 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 17:50:22 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 17:53:31 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1081.228393793106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:56:47 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 18:01:41 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 18:01:52 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 18:05:07 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 18:08:26 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 18:11:35 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1085.0202457904816 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:14:50 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 18:19:43 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 18:19:54 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 18:23:09 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 18:26:29 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 18:29:39 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1083.3332304954529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:32:54 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 18:37:49 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 18:38:00 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 18:41:14 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 18:44:33 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 18:47:43 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1083.3184854984283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:50:57 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 18:55:53 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 18:56:04 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 18:59:34 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 19:02:56 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 19:06:12 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1110.6885304450989 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:09:28 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 19:14:22 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 19:14:33 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 19:18:06 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 19:21:32 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 19:24:44 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1114.6272275447845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:28:02 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 19:32:57 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 19:33:09 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 19:36:23 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 19:39:44 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 19:42:53 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1084.4459023475647 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:46:12 2024]  Iteration number: 0 with current cost as 0.25178205775299267 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.00785819  0.53341231 -2.67483656
  3.07873793  2.25113987  1.13297075 -0.98880563  0.6789918   1.2624646
  1.41353056 -1.77078     0.75130681]. 
[Thu Apr  4 19:51:08 2024]  Iteration number: 50 with current cost as 0.15450756010392325 and parameters 
[-2.90318337  2.23743477 -2.12427977  0.14380352  1.69584575 -2.28932129
  3.31445781  1.67649386  1.06251816 -1.13841381 -0.02926142  1.14475287
  1.77740145 -0.82797471  1.42982247]. 
Working on 0.4 fold... 
[Thu Apr  4 19:51:19 2024]  Iteration number: 0 with current cost as 0.23866284372634614 and parameters 
[-2.90318344  2.23743464 -2.12427964 -0.03709709  0.53849335 -2.70114915
  3.06796791  2.24892978  1.15279638 -1.0302045   0.67672643  1.21150275
  1.41335002 -1.76876154  0.75913834]. 
Working on 0.6 fold... 
[Thu Apr  4 19:54:33 2024]  Iteration number: 0 with current cost as 0.2297021976687746 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02615261  0.5366896  -2.69114419
  3.07647498  2.24709779  1.1391739  -1.01238295  0.6743084   1.2323801
  1.4064937  -1.77608048  0.75593334]. 
Working on 0.8 fold... 
[Thu Apr  4 19:57:55 2024]  Iteration number: 0 with current cost as 0.22701217077566022 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.02549012  0.53524201 -2.69266338
  3.07585414  2.24571402  1.14103227 -1.00443105  0.67178604  1.24124417
  1.40930541 -1.77317767  0.75689463]. 
Working on 1.0 fold... 
[Thu Apr  4 20:01:04 2024]  Iteration number: 0 with current cost as 0.2412007893667698 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.0265099   0.53704819 -2.69099502
  3.07552237  2.24978773  1.13928425 -1.01628425  0.67669731  1.22839125
  1.40782304 -1.77510131  0.75498005]. 
Training complete taking 1093.4727265834808 seconds. 
Discarding model... 

Training complete taking 27302.67589521408 total seconds. 
Now scoring model... 
Scoring complete taking 0.9261934757232666 seconds. 
Saved predicted values as A1-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (92.77359297339554,), 'R2_train': 0.5505430783385121, 'MAE_train': 7.815642370651912, 'MSE_test': 108.54235460391446, 'R2_test': 0.34572688509000316, 'MAE_test': 8.255832775750633}. 
Saved model results as A1-A1-CZ_HWE-CNOT_results.json. 
