/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:19 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:53 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 17:37:29 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 17:49:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 17:51:07 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:53 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:08:18 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2756.0639305114746 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:50 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:17 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 18:35:45 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:18 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:03 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:54:52 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2822.305969953537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:03:51 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:10:17 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 19:22:13 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 19:23:43 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:21 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:53 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2724.6622660160065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:49:15 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:55:36 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:07:20 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 20:08:49 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:19 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:25:41 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2687.9280846118927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:34:03 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:34 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 20:53:00 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 20:54:45 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:03:22 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:47 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2759.7938237190247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:20:03 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:26:20 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 21:38:02 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 21:39:31 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:56:23 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2697.998646259308 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:05:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:11:49 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 22:23:32 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:02 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:34:00 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:42:27 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2760.311980485916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:51:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:57:16 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:08:54 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 23:10:23 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:18:53 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:27:12 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2671.9436931610107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:33 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:41:51 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Sun Mar 24 23:53:32 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Sun Mar 24 23:55:01 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:03:39 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:12:06 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2695.5999162197113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:20:28 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 00:26:49 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 00:39:11 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 00:40:52 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:49:52 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:58:11 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2766.925829410553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:06:35 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 01:12:58 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 01:24:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 01:26:07 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:34:39 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:43:01 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2689.1035232543945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:51:26 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 01:57:49 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:09:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 02:11:08 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:19:51 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:28:24 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2721.6060931682587 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:36:46 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 02:43:12 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 02:54:54 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 02:56:23 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:04:54 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:13:15 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2689.8005986213684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:21:36 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 03:27:53 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 03:39:38 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 03:41:07 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:50:06 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:58:38 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2724.1264379024506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 04:07:00 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 04:13:23 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 04:25:21 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 04:27:12 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:35:56 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:44:21 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2740.3322405815125 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:52:40 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 04:59:09 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:10:50 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 05:12:19 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:20:49 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:29:08 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2685.1439073085785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 05:37:26 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 05:43:42 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 05:55:22 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 05:56:51 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:05:22 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:13:42 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2675.6288883686066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 06:22:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 06:28:43 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 06:40:25 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 06:41:54 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:50:32 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:58:54 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2711.7064080238342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 07:07:14 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 07:13:28 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 07:25:06 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 07:26:34 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 07:35:07 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 07:43:35 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2688.5810582637787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 07:52:02 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 07:58:22 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:10:03 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 08:11:32 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 08:20:03 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 08:28:25 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2684.180774450302 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 08:36:46 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 08:43:04 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 08:54:39 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 08:56:08 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 09:04:36 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:12:57 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2670.4793140888214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 09:21:16 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 09:27:34 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 09:39:26 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 09:41:03 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 09:49:34 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 09:58:13 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2720.711629152298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 10:06:37 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 10:12:59 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 10:24:41 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 10:26:10 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 10:34:40 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 10:43:00 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2682.191461086273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 10:51:19 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 10:57:38 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:09:18 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 11:10:48 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 11:19:19 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 11:27:42 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2684.961494207382 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 11:36:04 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Mon Mar 25 11:42:34 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Mar 25 11:54:16 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Mon Mar 25 11:55:46 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 12:04:17 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 12:12:34 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2688.922614336014 seconds. 
Discarding model... 

Training complete taking 67801.01289606094 total seconds. 
Now scoring model... 
Scoring complete taking 0.9876642227172852 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (140.15648058384232,), 'R2_train': 0.32098889031724154, 'MAE_train': 10.849158815312844, 'MSE_test': 138.09857983751002, 'R2_test': 0.16756745949865337, 'MAE_test': 10.357025707433404}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:47 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:29:20 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 11:35:40 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 11:47:01 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 11:48:26 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 11:56:55 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 12:04:58 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2620.6995828151703 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:13:02 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 12:19:08 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 12:30:23 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:50 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:04 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 12:48:14 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2600.6200983524323 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:56:20 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 13:02:25 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:13:43 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:09 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:23:22 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 13:31:41 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2613.9195029735565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:39:55 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:06 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 13:57:24 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:03 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:07:20 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 14:15:27 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2620.2730288505554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:23:35 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 14:29:48 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 14:41:06 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 14:42:31 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:50:46 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 14:58:51 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2597.296168088913 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:06:53 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 15:12:59 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 15:24:27 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 15:25:54 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:34:14 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:42:22 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2617.3711853027344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:50:30 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 15:56:38 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:08:02 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 16:09:29 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:17:44 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 16:25:50 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2604.562784910202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:33:54 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 16:39:59 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 16:51:30 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 16:52:59 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:01:21 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:09:52 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2645.7664947509766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:18:01 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 17:24:09 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 17:35:33 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 17:37:03 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:45:18 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:53:24 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2608.583327770233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:01:28 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 18:07:36 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 18:18:59 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 18:20:25 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:28:40 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 18:36:53 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2612.25137925148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 18:45:02 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 18:51:09 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:02:27 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 19:03:53 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:12:09 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 19:20:16 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2601.759166240692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:28:23 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 19:34:31 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 19:45:53 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 19:47:20 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:55:57 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 20:04:18 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2655.418963432312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:12:38 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 20:18:48 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 20:30:32 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 20:31:58 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:40:45 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 20:49:06 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2673.8540155887604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:57:13 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 21:03:28 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:14:51 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 21:16:18 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:25:01 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 21:33:29 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2661.8764441013336 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:41:36 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 21:48:00 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 21:59:50 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 22:01:17 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:09:35 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:17:41 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2654.3222889900208 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 22:25:48 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 22:31:55 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 22:43:26 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 22:44:52 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:53:14 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:01:19 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2614.5878171920776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:09:24 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 23:15:31 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Thu Apr  4 23:26:57 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Thu Apr  4 23:28:30 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:36:49 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:45:02 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2634.013771057129 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 23:53:18 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 23:59:25 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:10:48 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 00:12:16 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:20:55 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:29:25 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2652.200715780258 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:37:29 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 00:43:36 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 00:54:54 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 00:56:20 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:04:38 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:12:47 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2604.3666145801544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:20:53 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 01:27:03 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 01:38:28 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 01:39:55 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 01:48:30 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 01:56:39 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2635.124860525131 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 02:04:49 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 02:11:01 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 02:22:34 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 02:24:00 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 02:32:27 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 02:40:32 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2656.541813135147 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 02:49:05 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 02:55:13 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:06:42 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 03:08:08 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 03:16:26 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 03:24:37 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2622.1826395988464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 03:32:48 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 03:39:21 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 03:50:43 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 03:52:09 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:00:24 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:08:31 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2635.127067565918 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 04:16:50 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 04:22:58 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 04:34:24 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 04:35:51 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 04:44:09 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 04:52:15 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2619.309321165085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 05:00:21 2024]  Iteration number: 0 with current cost as 0.39496814263352037 and parameters 
[-3.07068163  2.7935733  -2.15404793 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.76203631  1.14432447
  1.47509333 -1.8735468   0.72965078  2.88578419 -0.54534333 -0.47522485
 -2.02654242  0.72897368  1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Fri Apr  5 05:06:30 2024]  Iteration number: 0 with current cost as 0.3329148811725703 and parameters 
[-2.94079931  2.28856967 -2.13340513 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551999 -1.06648308  0.63440237  1.14432445
  1.35246954 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Fri Apr  5 05:17:52 2024]  Iteration number: 50 with current cost as 0.2146510907259614 and parameters 
[-4.73411435  3.47882981 -1.56775129 -0.11660818  0.55387472 -2.77011603
  3.06862345  2.1896368   1.18550877 -1.06650044  2.70616899  1.14434406
  0.18109032 -1.87354769  0.72972827  2.88586419 -0.54533477 -0.47524091
 -2.02653939  0.72896491  1.60515575  2.83076624 -1.26457465 -0.25134136]. 
Working on 0.6 fold... 
[Fri Apr  5 05:19:18 2024]  Iteration number: 0 with current cost as 0.37594103803661155 and parameters 
[-2.93815107  2.32171377 -2.13236955 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.63540797  1.14432445
  1.34722192 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 05:27:36 2024]  Iteration number: 0 with current cost as 0.37964965511427695 and parameters 
[-2.9482169   2.31995028 -2.13596513 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.64731679  1.14432446
  1.36289646 -1.87354679  0.72965081  2.8857842  -0.54534334 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.26456709 -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 05:35:38 2024]  Iteration number: 0 with current cost as 0.3716893875232391 and parameters 
[-2.93592156  2.31289541 -2.1314094  -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.63142415  1.14432445
  1.34408722 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077108 -1.2645671  -0.25136105]. 
Training complete taking 2600.789449453354 seconds. 
Discarding model... 

Training complete taking 65662.8206846714 total seconds. 
Now scoring model... 
Scoring complete taking 1.0447766780853271 seconds. 
Saved predicted values as A1-A1-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (140.15648058384232,), 'R2_train': 0.32098889031724154, 'MAE_train': 10.849158815312844, 'MSE_test': 138.09857983751002, 'R2_test': 0.16756745949865337, 'MAE_test': 10.357025707433404}. 
Saved model results as A1-A1-CNOT_Full-Pauli-CRX_results.json. 
