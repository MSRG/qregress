/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:55:07 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:58:23 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:00:54 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:03:56 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:07:28 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 908.427746295929 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:10:15 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:29 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:16:00 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:18:59 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:29 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 898.8253827095032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:12 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:28:27 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:31:00 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:02 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:36 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 905.1575748920441 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:40:21 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:43:40 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:13 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 18:49:13 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:44 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 909.9637422561646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:55:31 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:58:51 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:27 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:04:31 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:06 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 922.0812277793884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:51 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:14:09 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:16:42 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:49 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:23:23 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 916.6317937374115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:07 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:25 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:59 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:35:06 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:38:42 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 919.3844738006592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:29 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:44:50 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:27 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 19:50:31 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:07 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 925.3177108764648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:56:53 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:00:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:49 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:52 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:22 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 915.8698425292969 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:12 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:24 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:55 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:59 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:24:44 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 921.0338923931122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:31 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:30:55 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:30 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:33 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:09 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 928.2621099948883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:04 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:21 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 20:48:54 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 20:51:59 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 20:55:35 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 925.6317086219788 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:58:25 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:01:48 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:04:26 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:07:29 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:12 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 933.6843109130859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:13:55 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:19:55 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:22:58 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:34 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 924.8633432388306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:29:21 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:32:40 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:35:12 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:38:19 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:41:58 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 922.2483916282654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:44:46 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 21:48:04 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 21:50:31 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:36 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:10 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 913.2417352199554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:00:02 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:03:28 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:05:57 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:08:58 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 22:12:34 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 925.2363147735596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:15:30 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:18:51 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:21:29 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:24:36 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 22:28:17 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 943.3141181468964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:31:11 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:34:38 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:37:20 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:40:39 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 22:44:35 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 978.2256977558136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:47:35 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:10 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:50 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:56 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:00:33 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 957.8736503124237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:03:33 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:07:08 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:09:44 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 23:13:00 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:16:45 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 971.9936912059784 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:19:38 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:26:03 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 23:29:24 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:06 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 979.6240491867065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:35:54 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:18 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:42:02 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Sun Mar 24 23:45:16 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Sun Mar 24 23:49:16 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 973.7996706962585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:15 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:46 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Sun Mar 24 23:58:22 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Mon Mar 25 00:01:35 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:14 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 953.9699399471283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:08:04 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Mon Mar 25 00:11:39 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Mon Mar 25 00:14:22 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Mon Mar 25 00:17:35 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Mon Mar 25 00:21:17 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 965.7516565322876 seconds. 
Discarding model... 

Training complete taking 23340.414640188217 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.3910632133483887 seconds. 
Saved predicted values as M-A2-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (301.5658324309453,), 'R2_train': -0.4609852478341061, 'MAE_train': 15.604482381115016, 'MSE_test': 238.44666165439634, 'R2_test': -0.437312103923031, 'MAE_test': 13.249463316977838}. 
Saved model results as M-A2-CNOT_ESU2_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:07 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:53 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:34:07 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 12:36:31 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:31 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 12:43:03 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 895.3391959667206 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:45:44 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:48:59 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 12:51:28 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:29 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:54 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 891.3630113601685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:36 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:03:48 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 13:06:14 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 13:09:12 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 13:12:38 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 887.7746770381927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:15:26 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:18:38 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 13:21:07 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 13:24:03 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 13:27:33 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 892.9667901992798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:19 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:33:37 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 13:36:08 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 13:39:14 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:47 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 915.368367433548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:45:38 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:48:57 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 13:51:24 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:34 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 13:58:11 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 924.9522678852081 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:01:00 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:04:16 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:54 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:51 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 14:13:22 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 910.0380530357361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:16:09 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:19:27 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:54 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:50 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 14:28:14 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 887.882985830307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:30:57 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:34:12 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 14:36:40 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 14:39:37 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 14:43:06 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 892.8194329738617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:49 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:49:01 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 14:51:30 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 14:54:32 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 14:58:03 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 897.572491645813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:00:48 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:04:03 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 15:06:32 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 15:09:38 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 15:13:10 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 908.1454064846039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:15:56 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:19:15 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 15:21:48 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 15:24:47 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 15:28:14 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 902.4134654998779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:30:58 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:34:14 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 15:36:40 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 15:39:38 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 15:43:10 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 895.5335071086884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:45:52 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 15:49:06 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 15:51:42 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 15:54:39 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 15:58:11 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 906.8589608669281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:01:01 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:04:15 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 16:06:42 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 16:09:39 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 16:13:11 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 899.0802850723267 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:15:58 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:19:08 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 16:21:35 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 16:24:31 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 16:27:59 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 884.0666873455048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:30:43 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:33:59 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 16:36:27 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 16:39:24 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 16:42:50 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 889.1920464038849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:45:30 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 16:48:47 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 16:51:15 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 16:54:14 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 16:57:45 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 896.6849699020386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:00:30 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 17:03:44 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 17:06:13 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 17:09:13 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 17:12:48 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 906.4003982543945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:15:35 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 17:18:47 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 17:21:15 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 17:24:18 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 17:27:48 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 899.5850059986115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:30:34 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 17:33:49 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 17:36:15 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 17:39:09 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 17:42:41 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 889.2460868358612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:45:26 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 17:48:45 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 17:51:16 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 17:54:17 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 17:57:49 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 907.5313773155212 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:00:32 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 18:03:45 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 18:06:18 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 18:09:16 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 18:12:45 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 895.8442554473877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:15:30 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 18:18:41 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 18:21:08 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 18:24:04 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 18:27:31 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 885.7525887489319 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:30:13 2024]  Iteration number: 0 with current cost as 0.5440367205841936 and parameters 
[ 5.18199714  2.23743464 -2.12427638 -0.11652777  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552324 -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 18:33:31 2024]  Iteration number: 0 with current cost as 0.49701264831887393 and parameters 
[-0.51695477  2.23743464 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960161  1.1855203  -1.06648308]. 
Working on 0.6 fold... 
[Thu Apr  4 18:36:00 2024]  Iteration number: 0 with current cost as 0.5251841701394515 and parameters 
[-0.29319474  2.23743405 -2.12427964 -0.11653103  0.55388708 -2.77010927
  3.06858439  2.18960145  1.18551998 -1.06648338]. 
Working on 0.8 fold... 
[Thu Apr  4 18:38:58 2024]  Iteration number: 0 with current cost as 0.4954141580681748 and parameters 
[-0.25601667  2.23743406 -2.12427935 -0.11653103  0.55388679 -2.77010955
  3.06858469  2.18960116  1.18551998 -1.06648366]. 
Working on 1.0 fold... 
[Thu Apr  4 18:42:33 2024]  Iteration number: 0 with current cost as 0.47487953332300936 and parameters 
[-1.16647095  2.23743448 -2.12427964 -0.11653103  0.55388708 -2.77010913
  3.06858498  2.18960145  1.18551998 -1.06648324]. 
Training complete taking 904.8499200344086 seconds. 
Discarding model... 

Training complete taking 22467.26311635971 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2474446296691895 seconds. 
Saved predicted values as M-A2-CNOT_ESU2_predicted_values.csv
Model scores: {'MSE_train': (301.5658324309453,), 'R2_train': -0.4609852478341061, 'MAE_train': 15.604482381115016, 'MSE_test': 238.44666165439634, 'R2_test': -0.437312103923031, 'MAE_test': 13.249463316977838}. 
Saved model results as M-A2-CNOT_ESU2_results.json. 
