/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:51:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:09 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:11 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:01:46 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:22 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 846.5265970230103 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:12 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:12:06 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:37 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:11 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 830.1965746879578 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:01 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:21 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:25:56 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:29:28 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:07 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 841.7212400436401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:35:07 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:37:26 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:00 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:30 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 832.9892535209656 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:56 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:51:14 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 18:53:49 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:15 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:48 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 826.591019153595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:46 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:06 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:07:44 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:11:16 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:56 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 843.4935793876648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:16:50 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:10 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:21:49 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:18 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:52 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 844.2515761852264 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:30:51 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:06 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:35:43 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:08 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:41:40 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 823.4759180545807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:44:34 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:58 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:36 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:53:11 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:55:53 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 849.9392659664154 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:58:46 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:01:09 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:03:49 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:13 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:09:53 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 844.7806425094604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:12:55 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:15:17 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:54 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:21:25 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:58 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 843.482807636261 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:26:56 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:19 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:32:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:35:28 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:38:10 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 850.7600374221802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:41:08 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:43:28 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 20:46:05 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:49:38 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:52:20 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 847.15558385849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:55:13 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:57:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:10 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:03:40 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:06:22 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 846.4204363822937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:09:23 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:11:44 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:14:21 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:17:50 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:20:28 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 847.51172041893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:23:25 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:25:48 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:28:23 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:31:52 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:34:30 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 836.3816106319427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:37:22 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:39:42 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:42:17 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:50 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:48:41 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 855.2867481708527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:51:39 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:54:01 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 21:56:37 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:00:09 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:03:00 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 861.7706527709961 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:06:02 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:22 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:11:03 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:14:37 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:17:13 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 850.2965066432953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:20:07 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:22:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:25:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:28:29 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:31:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 828.4626984596252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:55 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:14 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:38:50 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:37 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:18 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 854.2206840515137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:15 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:50:38 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 22:53:21 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:56:46 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:59:18 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 844.1146986484528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:02:23 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:04:55 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 23:07:40 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:11:18 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:14:04 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 885.1027717590332 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:17:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:20 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 23:21:57 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:25:27 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:28:03 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 841.7689638137817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:33:30 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Sun Mar 24 23:36:13 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:39:47 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:42:30 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 867.2689235210419 seconds. 
Discarding model... 

Training complete taking 21143.971536636353 total seconds. 
Now scoring model... 
Scoring complete taking 2.100876808166504 seconds. 
Saved predicted values as A2-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (214.41641349602324,), 'R2_train': -0.03877556182668673, 'MAE_train': 12.53180210339704, 'MSE_test': 182.38628763166471, 'R2_test': -0.09939060158675028, 'MAE_test': 11.257106544005005}. 
Saved model results as A2-A2-CNOT_Efficient-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:13 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:23 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:32:49 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 12:35:35 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:10 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:41:51 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 871.2562546730042 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:44:57 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:47:25 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:10 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:53:48 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:56:37 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 884.4790329933167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:59:43 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:02:06 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 13:04:54 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:08:43 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:11:25 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 892.7147669792175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:14:34 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:17:01 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 13:19:45 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:23:32 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:26:29 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 909.2799746990204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:29:46 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:32:25 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 13:35:08 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:38:58 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:41:47 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 908.4621272087097 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:44:50 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:47:24 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 13:50:14 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:59 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:56:47 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 913.6420121192932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:21 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:02:58 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:01 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:09:58 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:12:56 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 953.96706199646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:16:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:18:38 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:34 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:25 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:28:28 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 946.4059429168701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:32:01 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:34:40 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 14:37:27 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:41:10 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:44:07 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 939.2161347866058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:47:29 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:50:04 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 14:53:07 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:05 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:00:12 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 953.0334804058075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:03:32 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:06:03 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 15:09:07 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:13:14 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:16:23 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 985.7592558860779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:19:50 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:22:38 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 15:26:00 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:29:41 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:32:27 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 953.7460293769836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:35:44 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:38:23 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 15:41:12 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:44:53 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:47:42 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 905.40345454216 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:50:45 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:53:13 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 15:56:14 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:00:14 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:03:13 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 944.2563645839691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:06:40 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:09:10 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 16:11:53 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:15:40 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:18:35 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 915.6069881916046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:21:40 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:24:16 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 16:27:05 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:31:06 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:33:49 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 908.2180502414703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:36:48 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:39:19 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 16:42:13 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:45:59 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:48:44 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 893.2254574298859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:51:49 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:54:37 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 16:57:33 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:01:39 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:04:37 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 948.5060663223267 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 17:07:32 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:10:00 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 17:12:40 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:16:15 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:18:58 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 861.6584408283234 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:21:56 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:24:19 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 17:26:56 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:30:48 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:33:35 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 882.6051762104034 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:36:35 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:38:55 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 17:41:33 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:45:13 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:47:49 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 865.273848772049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:51:06 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:53:34 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 17:56:32 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:00:12 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:03:06 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 907.146642446518 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:06:03 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:08:32 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 18:11:17 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:15:07 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:17:50 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 882.8709855079651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:20:54 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:23:27 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 18:26:07 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:29:46 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:32:43 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 894.1818203926086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:35:45 2024]  Iteration number: 0 with current cost as 0.3998665479188296 and parameters 
[ 1.53086512  2.23743479 -2.12427948 -0.11653118  0.55388692 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648324  0.60271526  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:38:15 2024]  Iteration number: 0 with current cost as 0.3200277979857381 and parameters 
[ 1.23494549  2.23743477 -2.1242795  -0.11653116  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648308  0.60271524  1.14432445
  1.31029899 -1.87354667]. 
Working on 0.6 fold... 
[Thu Apr  4 18:41:09 2024]  Iteration number: 0 with current cost as 0.5505231940956641 and parameters 
[-0.1438627   2.23743464 -2.12427964 -0.11653121  0.55388689 -2.77010907
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:44:47 2024]  Iteration number: 0 with current cost as 0.52281325246049 and parameters 
[-0.21869584  2.23743464 -2.12427964 -0.11653103  0.5538869  -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648318  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:47:35 2024]  Iteration number: 0 with current cost as 0.3586339798474303 and parameters 
[ 1.1185537   2.23743464 -2.12427964 -0.11653116  0.55388681 -2.77010911
  3.06858485  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 883.7050881385803 seconds. 
Discarding model... 

Training complete taking 22804.62127804756 total seconds. 
Now scoring model... 
Scoring complete taking 2.065579414367676 seconds. 
Saved predicted values as A2-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (214.41641349602324,), 'R2_train': -0.03877556182668673, 'MAE_train': 12.53180210339704, 'MSE_test': 182.38628763166471, 'R2_test': -0.09939060158675028, 'MAE_test': 11.257106544005005}. 
Saved model results as A2-A2-CNOT_Efficient-CRZ_results.json. 
