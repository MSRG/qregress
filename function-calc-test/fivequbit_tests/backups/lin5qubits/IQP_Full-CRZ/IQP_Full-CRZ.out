/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:31:55 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:40:14 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:51 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:14 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:05:44 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2542.8132584095 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:14:06 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:24 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:52 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:39:17 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:47:34 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2507.1074707508087 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:04 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:04:30 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:53 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:21:25 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:29:55 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2540.106514930725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:38:18 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:46:34 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:54:29 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:02:20 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:10:14 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2407.5089433193207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:18:19 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:26:23 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:34:29 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:42:49 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:51:02 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2450.8817489147186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:09 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:07:36 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:15:41 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:23:58 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:32:28 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2501.850238084793 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:41:07 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:26 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:58:04 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:06:36 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:14:52 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2554.9841940402985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:23:43 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:32:09 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:40:39 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:48:51 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:57:24 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2532.267981529236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:05:45 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:14:18 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:22:55 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:31:25 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:40:05 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2563.362701177597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:36 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:57:13 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:05:40 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:14:17 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:22:43 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2524.5533854961395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 00:30:25 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:38:06 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:45:49 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:53:34 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:01:12 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2312.7098019123077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:08:55 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:16:33 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:17 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:32:01 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:39:45 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2309.4749631881714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:47:26 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:55:09 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:03:05 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:11:34 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:19:57 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2442.5527050495148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:28:19 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:36:27 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:44:44 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:52:25 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:00:06 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2380.843290567398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:07:50 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:15:41 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:24:03 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:32:32 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:40:41 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2431.622597694397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:48:22 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:56:14 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:03:58 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:12:09 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:20:17 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2398.423063516617 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:28:30 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:07 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:45:15 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:53:41 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:02:03 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2526.5919885635376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:10:37 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:19:00 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:27:20 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:35:38 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:43:55 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2502.988894224167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:52:14 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:00:36 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:08:32 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:16:15 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:23:57 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2368.472996234894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:31:42 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:39:39 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:48:15 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:56:44 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:05:17 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2524.3019680976868 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 07:13:56 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:22:17 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:30:13 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:37:58 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:45:56 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2393.3663251399994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 07:53:34 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:01:28 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:09:33 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:18:07 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:26:22 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2460.912867307663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 08:34:36 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:42:13 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:49:52 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:57:32 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:05:16 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2309.896934747696 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 09:13:07 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:20:43 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 09:28:15 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:35:53 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:43:27 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2277.1498551368713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 09:51:03 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 09:58:39 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 10:06:23 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 10:14:03 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 10:21:32 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2281.9315366744995 seconds. 
Discarding model... 

Training complete taking 61046.677173137665 total seconds. 
Now scoring model... 
Scoring complete taking 2.5937695503234863 seconds. 
Saved predicted values as IQP_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.36924393992999,), 'R2_train': 0.9885218082670201, 'MAE_train': 1.1378144288491423, 'MSE_test': 1.7527885580910945, 'R2_test': 0.9894345167481791, 'MAE_test': 1.177123249985424}. 
Saved model results as IQP_Full-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:46 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:30:26 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:38:15 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 11:45:40 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 11:53:11 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:00:44 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2257.5817618370056 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:08:04 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:15:29 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:23:00 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:30:20 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:37:57 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2244.925979614258 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:45:28 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:53:20 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:00:49 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:08:20 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:49 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2263.6931190490723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:23:14 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:30:41 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:38:09 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:45:36 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:06 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2244.9296550750732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:00:33 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:07:54 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:15:14 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:22:34 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:30:09 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2211.7754323482513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:37:26 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:44:49 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:52:22 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:59:52 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:07:20 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2240.197399377823 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:14:46 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:23:07 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:31:12 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:39:28 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:47:44 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2443.9570870399475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:55:44 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:04:01 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:12:14 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:19:57 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:28:02 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2410.2308554649353 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:35:39 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:42:54 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:50:08 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:57:27 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:04:42 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2175.7302644252777 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:11:55 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:19:17 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:26:31 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:33:46 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:40:58 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2175.345446586609 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:48:12 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:55:26 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:02:39 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:09:55 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:17:15 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2179.6592168807983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:24:30 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:31:43 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:38:57 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:46:11 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:53:23 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2187.970902442932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:00:58 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:08:21 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:15:52 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:23:09 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:30:31 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2207.0890707969666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:37:45 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:45:35 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:52:51 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:00:04 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:07:21 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2213.355534553528 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:14:40 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:21:58 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:29:29 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:36:57 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:44:23 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2237.1183140277863 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:51:58 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:59:21 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:06:54 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:14:38 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:21:55 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2252.324541091919 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:29:29 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:36:55 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:44:24 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:51:50 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:59:22 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2243.41806435585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:06:52 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:14:21 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:21:47 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:30:03 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:37:46 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2323.641422510147 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:45:42 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:53:27 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:01:29 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:09:24 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:17:18 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2361.115011692047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:25:03 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:33:08 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:41:04 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:48:53 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:56:37 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2371.560493707657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:04:37 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:12:29 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:20:18 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:28:14 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:36:04 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2356.3304467201233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:43:47 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:51:14 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:59:05 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:06:24 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:13:39 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2242.2688250541687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:21:08 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:28:33 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:35:52 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:43:18 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:50:50 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2224.891181230545 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:58:13 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:05:37 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:13:11 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:20:29 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:27:58 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2223.554841041565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:35:16 2024]  Iteration number: 0 with current cost as 0.09769982316331628 and parameters 
[-3.69726586  2.23743464 -2.12427963 -0.11653103  0.55388709 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:42:40 2024]  Iteration number: 0 with current cost as 0.11346798178550643 and parameters 
[-3.58747418  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:50:01 2024]  Iteration number: 0 with current cost as 0.10839238645901819 and parameters 
[-3.61222036  2.23743464 -2.12427963 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432446
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522486
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:57:33 2024]  Iteration number: 0 with current cost as 0.10728114172983215 and parameters 
[-3.62186158  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.87354681  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136104
 -2.39279218 -2.27309775  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:05:03 2024]  Iteration number: 0 with current cost as 0.11133338139627401 and parameters 
[-3.61308652  2.23743463 -2.12427964 -0.11653103  0.55388707 -2.77010898
  3.06858497  2.18960145  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309775  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2232.142229318619 seconds. 
Discarding model... 

Training complete taking 56524.80794405937 total seconds. 
Now scoring model... 
Scoring complete taking 2.5522425174713135 seconds. 
Saved predicted values as IQP_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.36924393992999,), 'R2_train': 0.9885218082670201, 'MAE_train': 1.1378144288491423, 'MSE_test': 1.7527885580910945, 'R2_test': 0.9894345167481791, 'MAE_test': 1.177123249985424}. 
Saved model results as IQP_Full-CRZ_results.json. 
