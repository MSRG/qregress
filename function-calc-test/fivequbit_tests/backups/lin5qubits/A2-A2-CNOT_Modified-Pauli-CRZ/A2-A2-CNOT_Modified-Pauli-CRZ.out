/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:05 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:35:31 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:23 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:37:23 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:38:42 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:39:41 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.7927656173706 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:40:47 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:40 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:42:39 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:59 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:57 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 317.914174079895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:46:03 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:46:57 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:47:56 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:14 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:50:13 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 314.2264151573181 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:51:18 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:11 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:53:10 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:54:30 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:55:29 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.94443821907043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 17:56:34 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:57:27 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:58:26 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:59:45 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:00:45 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 315.4978575706482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:51 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:44 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:43 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:01 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:06:01 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.68637347221375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:08 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:07:59 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:08:58 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:10:17 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:11:19 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 317.23136043548584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:12:25 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:19 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:18 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:15:38 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:16:37 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 319.5743989944458 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:17:44 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:36 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:35 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:54 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:21:53 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 315.05534315109253 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:23:00 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:23:53 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:24:52 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:26:11 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:27:11 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.9469118118286 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:28:16 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:08 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:06 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:31:27 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:32:26 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.1174759864807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:33:33 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:34:26 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:35:25 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:45 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:44 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 317.1972990036011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:49 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:42 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:40:41 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:41:58 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:42:57 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.07613015174866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:44:03 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:44:56 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:45:54 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:47:12 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:48:10 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.86599493026733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:49:15 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:50:06 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:51:06 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:25 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:26 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 317.08871245384216 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:32 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:23 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:56:22 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:41 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:58:39 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.9098742008209 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:59:45 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:00:39 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:01:38 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:58 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:03:57 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 316.0125651359558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:01 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:05:53 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:06:52 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:08:09 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:09:07 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.04048347473145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:10:14 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:06 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:12:04 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:13:23 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:14:22 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.43071126937866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:27 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:20 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:19 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:18:36 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:19:34 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.9235324859619 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:20:39 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:21:32 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:22:31 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:23:50 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:49 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 315.0134975910187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:54 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:47 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:27:45 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:29:02 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:01 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.5247929096222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:31:08 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:32:00 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:33:00 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:34:18 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:35:16 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.32708835601807 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:21 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:37:13 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:38:11 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:29 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:27 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 312.2030909061432 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:33 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:42:26 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:24 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:44:44 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:45:41 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 313.76781702041626 seconds. 
Discarding model... 

Training complete taking 7875.370651006699 total seconds. 
Now scoring model... 
Scoring complete taking 0.8208298683166504 seconds. 
Saved predicted values as A2-A2-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (214.41641345641656,), 'R2_train': -0.03877556163480578, 'MAE_train': 12.531802135353802, 'MSE_test': 182.38628735542522, 'R2_test': -0.09939059992163002, 'MAE_test': 11.257106521927211}. 
Saved model results as A2-A2-CNOT_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:15:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:27 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:17:30 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:18:40 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:20:16 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:21:26 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 378.56190967559814 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:45 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:50 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:25:00 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:26:34 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:27:46 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 380.36385345458984 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:06 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:09 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:21 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:32:56 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:34:06 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 380.949227809906 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:25 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:36:29 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:40 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:39:14 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:40:26 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 378.6601495742798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:41:44 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:42:51 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:44:01 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:45:37 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:46:49 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 383.5171449184418 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:48:07 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:49:11 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:50:22 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:51:57 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:53:08 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 380.26837635040283 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:54:28 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:55:31 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:56:41 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:58:16 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:59:28 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 377.4221570491791 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:00:45 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:01:50 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:03:01 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:04:37 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:49 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 381.4094069004059 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:07:06 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:08:16 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:09:28 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:11:03 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:12:13 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 385.95076394081116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:13:33 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:35 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:46 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:21 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:31 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 376.4750859737396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:50 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:20:53 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:03 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:23:37 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:24:47 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 377.5121064186096 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:26:06 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:09 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:28:23 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:29:56 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:31:09 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 381.2900140285492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:32:28 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:33:31 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:34:42 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:36:16 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:28 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 377.7704904079437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:38:45 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:39:48 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:41:00 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:42:36 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:43:50 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 382.4560134410858 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:45:08 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:46:11 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:47:23 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:48:57 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:50:11 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 381.8803479671478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:51:30 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:52:32 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:53:43 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:55:23 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:56:33 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 384.09053111076355 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:57:54 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:58:56 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:00:06 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:47 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:03:01 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 384.9534344673157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:19 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:23 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:33 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:08:09 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:19 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 378.55241417884827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:10:37 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:11:40 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:12:50 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:14:25 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:15:35 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 377.44014286994934 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:16:55 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:17:58 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:08 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:20:42 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:21:54 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 376.36909675598145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:23:11 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:24:14 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:25:25 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:27:00 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:28:10 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 378.29531359672546 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:29:30 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:30:32 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:31:42 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:33:16 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:34:27 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 375.2326338291168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:45 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:36:47 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:37:59 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:39:38 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:59 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 391.45329332351685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:42:16 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:43:19 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:44:30 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:46:04 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:47:20 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 382.6073913574219 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:48:52 2024]  Iteration number: 0 with current cost as 0.39986659220602744 and parameters 
[ 1.53086559  2.23743479 -2.12427932 -0.11653087  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354649  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:49:57 2024]  Iteration number: 0 with current cost as 0.3200280844021362 and parameters 
[ 1.23494237  2.23743464 -2.12427937 -0.11653089  0.55388694 -2.77010897
  3.06858498  2.18960145  1.18552012 -1.06648335  0.6027151   1.14432445
  1.31029899 -1.87354653  0.72965067  2.88578406 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:51:07 2024]  Iteration number: 0 with current cost as 0.5505232284633694 and parameters 
[-0.14386238  2.23743473 -2.12427945 -0.11653093  0.55388699 -2.77010897
  3.06858489  2.18960145  1.18552008 -1.06648318  0.60271501  1.14432445
  1.31029908 -1.87354671  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:52:42 2024]  Iteration number: 0 with current cost as 0.5228132778229893 and parameters 
[-0.21869566  2.23743473 -2.12427936 -0.11653093  0.55388717 -2.77010888
  3.06858498  2.18960154  1.18552008 -1.06648308  0.60271519  1.14432454
  1.31029908 -1.87354662  0.72965071  2.8857841  -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:54:00 2024]  Iteration number: 0 with current cost as 0.35863435594650467 and parameters 
[ 1.1185514   2.2374349  -2.12427924 -0.11653076  0.55388708 -2.77010897
  3.06858498  2.18960159  1.18552025 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354653  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 399.1353187561035 seconds. 
Discarding model... 

Training complete taking 9532.618337869644 total seconds. 
Now scoring model... 
Scoring complete taking 0.9558441638946533 seconds. 
Saved predicted values as A2-A2-CNOT_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (214.41641345641656,), 'R2_train': -0.03877556163480578, 'MAE_train': 12.531802135353802, 'MSE_test': 182.38628735542522, 'R2_test': -0.09939059992163002, 'MAE_test': 11.257106521927211}. 
Saved model results as A2-A2-CNOT_Modified-Pauli-CRZ_results.json. 
