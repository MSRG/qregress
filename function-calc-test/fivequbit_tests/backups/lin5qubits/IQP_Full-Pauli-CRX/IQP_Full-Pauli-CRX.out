/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:32:52 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:33:16 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:20 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 17:39:56 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 17:43:01 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 17:46:07 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 957.867847442627 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:49:13 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 17:52:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 17:55:48 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:01:59 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 951.0813715457916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:05:05 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:16 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:44 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 18:14:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:17:58 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 957.3387682437897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:02 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:24:07 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:39 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 18:30:44 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:46 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 949.0750617980957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:36:50 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:39:52 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:17 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 18:46:19 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:21 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 936.3725271224976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:52:28 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:39 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 18:59:05 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:02:09 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:05:11 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 947.9692182540894 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:08:13 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:19 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 19:14:41 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:17:43 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:52 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 939.6157720088959 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:23:54 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:26:58 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 19:30:27 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:33:29 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:36:51 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 958.629693031311 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:39:53 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:42:55 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 19:46:21 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 19:49:27 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:28 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 937.499832868576 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:30 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 19:58:34 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:02:00 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:07 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:13 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 945.7164642810822 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:11:15 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:46 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:20:48 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:55 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 947.2758507728577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:27:03 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:30:05 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:33:34 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:39:57 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 957.0765478610992 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:01 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 20:46:04 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 20:49:34 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 20:52:38 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 20:56:01 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 964.6557650566101 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:59:05 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:02:09 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:05:34 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:08:36 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:11:40 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 938.5914702415466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 21:14:44 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:51 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:21:33 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:24:35 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:27:37 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 955.2793440818787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 21:30:39 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:33:42 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:37:07 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:40:09 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:43:10 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 933.1115040779114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:46:12 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 21:49:14 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 21:52:40 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 21:55:42 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 21:58:44 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 935.0134971141815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 22:01:47 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:04:50 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:08:16 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:11:19 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 22:14:21 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 937.2274088859558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:17:23 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:20:24 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:23:50 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:26:52 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:55 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 943.6994829177856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:33:08 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:36:15 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:39:40 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:42:42 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 22:45:44 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 941.0236067771912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:48:49 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:52 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 22:55:28 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 22:58:34 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:01:49 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 961.6887121200562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:04:51 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:07:52 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:11:38 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 23:14:46 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:17:48 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 959.1316909790039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:20:50 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:23:53 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:27:21 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 23:30:24 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:33:49 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 959.7449340820312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 23:36:50 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:39:57 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:43:26 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Sun Mar 24 23:46:28 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Sun Mar 24 23:49:29 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 942.244068145752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:52:31 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Sun Mar 24 23:55:35 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Sun Mar 24 23:59:00 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Mar 25 00:02:03 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Mar 25 00:05:07 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 940.0379917621613 seconds. 
Discarding model... 

Training complete taking 23696.969702243805 total seconds. 
Now scoring model... 
Scoring complete taking 1.003828763961792 seconds. 
Saved predicted values as IQP_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.355242500405745,), 'R2_train': 0.988589640542409, 'MAE_train': 1.1430239729859633, 'MSE_test': 1.788691417882077, 'R2_test': 0.9892181009905208, 'MAE_test': 1.1600935528937666}. 
Saved model results as IQP_Full-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:07:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:07:56 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 12:10:53 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 12:14:05 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 12:17:04 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 12:20:01 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 898.2661838531494 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:22:54 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 12:25:48 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 12:29:01 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 12:31:54 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 12:34:50 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 891.7649230957031 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:37:46 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 12:40:39 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 12:43:55 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 12:46:49 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 12:49:51 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 900.0095491409302 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:52:47 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 12:55:39 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 12:58:56 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 13:01:59 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 13:05:04 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 923.1954698562622 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:08:17 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 13:11:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:49 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 13:18:13 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 13:21:10 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 961.8682103157043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:24:11 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 13:27:26 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 13:30:51 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 13:33:55 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 13:36:52 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 941.5122923851013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:39:54 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 13:43:08 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 13:46:31 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 13:49:35 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 13:53:00 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 981.3579576015472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:16 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 13:59:26 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 14:03:08 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 14:06:21 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:40 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 994.8079648017883 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:12:53 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 14:16:02 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 14:19:28 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 14:22:30 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:33 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 935.0582733154297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:28:24 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 14:31:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:37 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:29 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 14:40:23 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 891.8198373317719 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:43:15 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 14:46:04 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 14:49:20 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 14:52:15 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 14:55:10 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 886.0718891620636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:58:03 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 15:00:49 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 15:04:08 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 15:07:03 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 15:10:00 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 897.5737235546112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:13:00 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 15:15:49 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 15:19:08 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 15:21:58 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 15:24:45 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 881.8151841163635 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:27:42 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 15:30:34 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 15:33:48 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 15:36:37 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 15:39:31 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 894.0027146339417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:42:35 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 15:45:31 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 15:48:57 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 15:52:01 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 15:54:58 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 921.2902402877808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:57:56 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 16:00:57 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 16:04:19 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 16:07:17 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 16:10:24 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 928.9938912391663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:13:25 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 16:16:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 16:19:58 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 16:23:14 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 16:26:32 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 959.2700545787811 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:29:25 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 16:32:33 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 16:35:54 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 16:38:58 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 16:42:07 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 947.4162881374359 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:45:13 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 16:48:08 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 16:51:32 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 16:54:30 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 16:57:28 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 912.6443662643433 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:00:26 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 17:03:33 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 17:07:12 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 17:10:16 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 17:13:14 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 952.9987637996674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:16:17 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 17:19:18 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 17:22:51 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 17:25:59 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 17:29:14 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 964.5596816539764 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:32:26 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 17:35:23 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 17:38:44 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 17:41:39 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 17:44:35 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 916.18718957901 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:47:42 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 17:50:40 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 17:54:08 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 17:57:07 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 18:00:12 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 936.3378858566284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:03:18 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 18:06:31 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 18:09:51 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 18:12:53 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 18:15:46 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 926.0734629631042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:18:41 2024]  Iteration number: 0 with current cost as 0.011798474742533479 and parameters 
[-3.04179813  3.2122105  -2.13864647 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.7182454   1.14432445
  1.429532   -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512665  2.83077107 -1.26456708 -0.25136103]. 
Working on 0.4 fold... 
[Thu Apr  4 18:21:33 2024]  Iteration number: 0 with current cost as 0.01172687846768206 and parameters 
[-3.00008261  3.05859969 -2.13490724 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960146  1.18552    -1.06648308  0.72948464  1.14432445
  1.40461336 -1.87354679  0.7296508   2.8857842  -0.54534335 -0.47522484
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136103]. 
Working on 0.6 fold... 
[Thu Apr  4 18:24:53 2024]  Iteration number: 0 with current cost as 0.007628816006848019 and parameters 
[-3.01564321  3.12483073 -2.1363291  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.72778154  1.14432445
  1.41436089 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Thu Apr  4 18:27:50 2024]  Iteration number: 0 with current cost as 0.007806112327644769 and parameters 
[-3.02036746  3.12225395 -2.13687045 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648308  0.72625578  1.14432445
  1.41807218 -1.87354679  0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Thu Apr  4 18:30:47 2024]  Iteration number: 0 with current cost as 0.008916880880891532 and parameters 
[-3.01317657  3.11760863 -2.13606234 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960146  1.18551999 -1.06648308  0.72771907  1.14432446
  1.41270546 -1.87354679  0.7296508   2.8857842  -0.54534334 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136103]. 
Training complete taking 898.9686253070831 seconds. 
Discarding model... 

Training complete taking 23143.866269350052 total seconds. 
Now scoring model... 
Scoring complete taking 0.939340353012085 seconds. 
Saved predicted values as IQP_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.355242500405745,), 'R2_train': 0.988589640542409, 'MAE_train': 1.1430239729859633, 'MSE_test': 1.788691417882077, 'R2_test': 0.9892181009905208, 'MAE_test': 1.1600935528937666}. 
Saved model results as IQP_Full-Pauli-CRX_results.json. 
