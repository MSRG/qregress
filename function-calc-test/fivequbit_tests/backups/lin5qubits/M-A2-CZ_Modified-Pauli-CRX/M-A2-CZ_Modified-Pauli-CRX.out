/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:58 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:59:35 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:05:46 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:12:09 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:18:45 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1871.3451294898987 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:25:08 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:30:42 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:36:54 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:18 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:49:54 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1868.2885127067566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:56:16 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:01:56 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:08:10 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:34 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:21:10 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1875.4330384731293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:27:33 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:07 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:39:19 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:44 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:52:38 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1888.1324112415314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:59:01 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:04:42 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:10:49 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:17:12 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:49 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1870.5317511558533 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:30:10 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:35:44 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:41:52 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:48:11 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:54:59 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1874.2176764011383 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:01:28 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:07:04 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:13:13 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:19:33 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:26:15 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1891.873323917389 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:32:58 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:31 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:50 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 21:51:18 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:51 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1873.2076919078827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:04:11 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:09:53 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:16:03 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:25 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 22:29:00 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1872.5330302715302 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:35:22 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 22:40:55 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 22:47:05 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 22:53:29 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:00:04 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1862.6926069259644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:06:25 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:11:58 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:18:10 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:24:32 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 23:31:08 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1868.348560333252 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:37:33 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 23:43:08 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 23:49:18 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 23:55:44 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:02:21 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1871.3713912963867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 00:08:45 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:14:26 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:20:50 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:27:14 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 00:34:06 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1902.209636926651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:40:27 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 00:46:03 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 00:52:39 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 00:59:03 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:05:59 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1909.977656841278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 01:12:18 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:17:52 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:24:04 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 01:30:27 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 01:37:03 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1866.7941970825195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:43:24 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 01:48:58 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 01:55:09 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:01:31 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:08:19 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1876.4975366592407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 02:14:40 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:20:13 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:26:26 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 02:32:49 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 02:39:25 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1866.744907617569 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:45:47 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 02:51:32 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 02:58:12 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:04:33 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:11:09 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1905.8955569267273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 03:17:33 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:23:09 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 03:29:20 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 03:35:42 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 03:42:35 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1886.1101422309875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:49:00 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 03:54:36 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:00:46 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:07:10 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:13:45 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1864.9589450359344 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 04:20:05 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:25:40 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 04:31:53 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 04:38:17 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 04:44:53 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1870.648294210434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:51:15 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 04:56:47 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:02:57 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:09:21 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:15:55 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1862.6395540237427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:22:18 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:27:52 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 05:34:02 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 05:40:24 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 05:46:59 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1865.0065443515778 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:53:23 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 05:58:58 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:05:09 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:11:28 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:18:01 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1862.4607617855072 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:24:26 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Mon Mar 25 06:30:00 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Mon Mar 25 06:36:12 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Mon Mar 25 06:42:35 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Mon Mar 25 06:49:05 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1862.0371119976044 seconds. 
Discarding model... 

Training complete taking 46889.9575278759 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2362539768218994 seconds. 
Saved predicted values as M-A2-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (72.0727614094662,), 'R2_train': 0.6508316597371517, 'MAE_train': 5.641116256833634, 'MSE_test': 51.41745485297763, 'R2_test': 0.6900651504183549, 'MAE_test': 5.372469422989449}. 
Saved model results as M-A2-CZ_Modified-Pauli-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:12 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:29:49 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:35:36 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:42:05 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:48:42 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:29 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1936.3399708271027 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:02:05 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:07:51 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:11 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:20:41 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:27:47 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1931.9017400741577 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:17 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:40:03 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:46:27 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:12 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:59:55 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1928.4369673728943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:06:27 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:17 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:18:39 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:25:21 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:32:02 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1921.5336499214172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:38:27 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:44:06 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:50:19 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:57:08 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:56 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1917.473358631134 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 15:10:24 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:16:05 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:22:28 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:28:53 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:35:49 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1917.031434059143 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:42:22 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:48:02 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:54:28 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:00:59 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:07:53 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1937.1463079452515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:14:39 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:20:19 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:26:37 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 16:33:06 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 16:39:47 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1893.9685814380646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:46:13 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 16:51:53 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 16:58:11 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:04:41 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:11:21 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1898.4366827011108 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:17:50 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:23:28 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 17:29:51 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 17:36:23 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 17:43:04 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1901.8088009357452 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:49:33 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 17:55:14 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:01:28 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:08:04 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:14:51 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1909.527773141861 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:21:22 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:27:02 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 18:33:18 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 18:39:43 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 18:46:25 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1892.6990985870361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 18:52:55 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 18:58:44 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:05:02 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:11:30 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:18:08 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1902.1524558067322 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:24:37 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 19:30:18 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 19:36:35 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 19:43:04 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 19:49:43 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1893.8747594356537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:56:11 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:01:52 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:08:10 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:14:40 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:21:20 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1896.7503776550293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 20:27:47 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 20:33:27 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 20:39:42 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 20:46:12 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 20:52:52 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1894.653130531311 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:59:22 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:05:02 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:11:16 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:17:47 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:24:45 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1911.8717050552368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:31:14 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 21:36:55 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 21:43:31 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 21:49:58 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 21:57:03 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1942.7740528583527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:03:41 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:09:27 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:15:44 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:22:10 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 22:28:48 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1900.0152325630188 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:35:17 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 22:40:58 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 22:47:23 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 22:53:53 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:00:39 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1908.583755016327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:07:06 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:12:46 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:19:05 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:25:35 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 23:32:27 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1929.9920735359192 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 23:39:15 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 23:45:20 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 23:51:47 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 23:58:17 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:04:59 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1932.14426279068 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 00:11:28 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:17:07 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:23:22 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 00:30:01 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 00:36:49 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1919.3810040950775 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 00:43:27 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 00:49:09 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 00:55:24 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:01:51 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:08:31 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1895.3669016361237 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 01:15:02 2024]  Iteration number: 0 with current cost as 0.42550268376781764 and parameters 
[-4.40194585  2.59540877 -2.43720726 -0.11653099  0.55388712 -2.77010894
  3.06858502  2.18960149  1.18552002 -1.06648305  2.35510913  1.14432449
  1.31029902 -1.87354676  0.72965077  2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Fri Apr  5 01:20:54 2024]  Iteration number: 0 with current cost as 0.480600750139794 and parameters 
[-4.70165801  2.64527144 -2.51142507 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960153  1.1855201  -1.06648305  2.87499507  1.14432449
  1.31029899 -1.87354669  0.7296508   2.88578419 -0.54534316 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Fri Apr  5 01:27:25 2024]  Iteration number: 0 with current cost as 0.4543176369573384 and parameters 
[-4.65903533  2.58880893 -2.51585101 -0.11653095  0.55388708 -2.77010894
  3.06858506  2.18960145  1.1855201  -1.06648305  2.78021944  1.14432445
  1.31029895 -1.87354673  0.7296508   2.88578419 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Fri Apr  5 01:33:57 2024]  Iteration number: 0 with current cost as 0.46529269657767824 and parameters 
[-4.70114932  2.60707426 -2.53518608 -0.11653103  0.553887   -2.77010897
  3.06858502  2.18960145  1.18552002 -1.06648312  2.7936145   1.14432441
  1.31029891 -1.87354676  0.72965073  2.88578419 -0.54534328 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Fri Apr  5 01:40:48 2024]  Iteration number: 0 with current cost as 0.4406827236816935 and parameters 
[-4.55938128  2.60021492 -2.4805572  -0.11653099  0.55388704 -2.77010901
  3.06858502  2.18960141  1.18552002 -1.06648312  2.69647443  1.14432445
  1.31029895 -1.87354676  0.72965077  2.88578423 -0.54534324 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1974.376012802124 seconds. 
Discarding model... 

Training complete taking 47888.242141485214 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.2625923156738281 seconds. 
Saved predicted values as M-A2-CZ_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (72.0727614094662,), 'R2_train': 0.6508316597371517, 'MAE_train': 5.641116256833634, 'MSE_test': 51.41745485297763, 'R2_test': 0.6900651504183549, 'MAE_test': 5.372469422989449}. 
Saved model results as M-A2-CZ_Modified-Pauli-CRX_results.json. 
