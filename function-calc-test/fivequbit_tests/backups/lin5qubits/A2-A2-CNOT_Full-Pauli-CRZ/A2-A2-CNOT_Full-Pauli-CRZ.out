/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:29:45 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:05 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 17:36:31 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 17:43:19 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 17:49:30 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 17:56:21 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1924.7097086906433 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:02:09 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:08:25 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:14:56 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:21:06 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:28:00 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1926.9578149318695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:34:17 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:28 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 18:46:45 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:59 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 18:59:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.88538813591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:05:47 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:04 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:18:11 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:24:23 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1873.7585124969482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:37:02 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:11 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 19:49:18 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 19:55:31 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:02:25 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1872.9968621730804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:08:14 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:14:22 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:20:31 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:26:49 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 20:33:49 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1884.056087255478 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:39:39 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 20:45:49 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 20:52:09 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 20:58:25 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:05:18 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.750769853592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:11:13 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:17:29 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:23:39 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 21:29:59 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 21:36:50 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1889.224758386612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:42:39 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 21:48:48 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 21:55:02 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:01:15 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:08:09 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1889.2204308509827 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:14:08 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:20:16 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:26:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 22:32:33 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 22:39:22 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1863.524650812149 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:45:12 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 22:51:22 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 22:57:34 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:03:44 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:10:35 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1873.8165369033813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:16:24 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:22:36 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Sun Mar 24 23:28:50 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Sun Mar 24 23:35:23 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Sun Mar 24 23:42:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1900.4066078662872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:48:05 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Sun Mar 24 23:54:17 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:00:27 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:06:36 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:13:26 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1866.354547739029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:19:12 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:25:40 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 00:31:51 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 00:38:01 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 00:44:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.4291574954987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:50:43 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 00:56:51 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:03:00 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:09:10 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:16:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1868.0301487445831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:21:51 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:28:13 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 01:34:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 01:40:38 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 01:47:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1912.0360040664673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:53:42 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 01:59:57 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:06:03 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:12:09 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:19:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1866.4636554718018 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:24:50 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 02:30:56 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 02:37:07 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 02:43:34 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 02:50:51 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1912.9210872650146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:56:41 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:02:54 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:09:04 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:15:13 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:22:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1869.5780668258667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:27:52 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 03:34:09 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 03:40:40 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 03:46:50 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 03:53:56 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1911.1745524406433 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:59:42 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:05:51 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:12:02 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:18:12 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:25:13 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1878.534143447876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:31:02 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 04:37:31 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 04:43:37 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 04:50:10 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 04:57:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1920.1718068122864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 05:03:02 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:09:49 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 05:15:57 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:22:03 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 05:28:50 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1898.7170782089233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:34:40 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 05:41:15 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 05:47:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 05:53:54 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:01:01 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1934.1935501098633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 06:07:08 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Mar 25 06:13:21 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Mon Mar 25 06:19:32 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Mar 25 06:25:41 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Mon Mar 25 06:32:31 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1903.160837173462 seconds. 
Discarding model... 

Training complete taking 47312.07443404198 total seconds. 
Now scoring model... 
Scoring complete taking 0.9834396839141846 seconds. 
Saved predicted values as A2-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (102.82277698041185,), 'R2_train': 0.5018581544906382, 'MAE_train': 9.204393674647502, 'MSE_test': 130.68886215951196, 'R2_test': 0.21223185878756112, 'MAE_test': 9.835769608290548}. 
Saved model results as A2-A2-CNOT_Full-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:28:30 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:28:54 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 11:35:05 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 11:41:15 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 11:47:23 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 11:54:13 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1874.7751157283783 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:00:06 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:06:17 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:12:46 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:18:56 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 12:25:50 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1896.8030149936676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:31:42 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 12:38:34 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 12:44:47 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 12:50:58 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 12:57:51 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1921.8793654441833 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:03:44 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:10:00 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:16:08 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:22:29 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 13:29:19 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1882.4982695579529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:35:05 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 13:41:13 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 13:47:20 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:25 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 14:00:18 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1862.5721416473389 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:06:08 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:17 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:18:23 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:24:35 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 14:31:22 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1860.563723564148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:37:09 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 14:43:21 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 14:49:55 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 14:56:07 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:03:05 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1903.1413595676422 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 15:08:53 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:15:00 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:21:08 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:28:04 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 15:34:54 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1909.2444994449615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:40:41 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 15:46:50 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 15:53:05 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 15:59:31 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 16:06:30 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1897.9387679100037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 16:12:20 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:18:29 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:24:36 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 16:30:51 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 16:37:47 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1890.2537932395935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:43:49 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 16:49:59 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 16:56:06 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:02:12 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:09:00 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1860.2567851543427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 17:14:49 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:20:58 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:27:19 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 17:33:27 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 17:40:12 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1869.4677259922028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:46:00 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 17:52:10 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 17:58:19 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:04:28 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 18:11:13 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1860.4187862873077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:17:01 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:23:19 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 18:29:30 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 18:35:37 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 18:42:30 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1893.8599073886871 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 18:48:34 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 18:54:43 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:00:58 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:07:33 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 19:14:33 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1911.669495344162 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:20:25 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:26:46 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 19:32:52 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 19:39:11 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 19:46:04 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1884.709328174591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 19:51:51 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 19:58:02 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:04:09 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:10:29 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 20:17:19 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1878.9991855621338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 20:23:10 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 20:29:25 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 20:35:40 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 20:41:48 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 20:48:34 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1882.3390736579895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 20:54:31 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:00:47 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 21:06:58 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:13:11 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 21:19:58 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1873.948089838028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 21:25:46 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 21:31:58 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 21:38:13 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 21:44:27 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 21:51:20 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1883.7571864128113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:57:09 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:03:21 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:09:30 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:16:01 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:22:58 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1898.5006306171417 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 22:28:49 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 22:34:59 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 22:41:08 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 22:47:18 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 22:54:06 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1866.7138454914093 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:59:55 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:06:05 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 23:12:17 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:18:23 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:25:39 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1894.95565199852 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:31:30 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Apr  4 23:37:38 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Thu Apr  4 23:44:11 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Apr  4 23:50:18 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Thu Apr  4 23:57:05 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1886.1503517627716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 00:02:56 2024]  Iteration number: 0 with current cost as 0.4958932821769848 and parameters 
[-3.15542828  2.25085835 -2.13066552 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.60731508  1.14432445
  1.59312264 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Fri Apr  5 00:09:38 2024]  Iteration number: 0 with current cost as 0.3874226354927425 and parameters 
[-3.24266215  2.2198048  -2.13373569 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.59719529  1.14432445
  1.70546837 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Working on 0.6 fold... 
[Fri Apr  5 00:15:57 2024]  Iteration number: 0 with current cost as 0.41798265330906825 and parameters 
[-3.21678442  2.23460078 -2.13022526 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960144  1.18551998 -1.06648309  0.59973283  1.14432445
  1.6692523  -1.8735468   0.72965079  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Fri Apr  5 00:22:13 2024]  Iteration number: 0 with current cost as 0.42124815713418 and parameters 
[-3.21568305  2.23351791 -2.13089025 -0.11653102  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.60027365  1.14432446
  1.66667753 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 1.0 fold... 
[Fri Apr  5 00:29:09 2024]  Iteration number: 0 with current cost as 0.41920775453777726 and parameters 
[-3.21823375  2.23304738 -2.13201937 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6015229   1.14432445
  1.67294842 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 1923.2779569625854 seconds. 
Discarding model... 

Training complete taking 47168.69588017464 total seconds. 
Now scoring model... 
Scoring complete taking 0.9941165447235107 seconds. 
Saved predicted values as A2-A2-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (102.82277698041185,), 'R2_train': 0.5018581544906382, 'MAE_train': 9.204393674647502, 'MSE_test': 130.68886215951196, 'R2_test': 0.21223185878756112, 'MAE_test': 9.835769608290548}. 
Saved model results as A2-A2-CNOT_Full-Pauli-CRZ_results.json. 
