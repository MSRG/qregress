/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Thu Mar 28 03:24:10 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 03:25:56 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 03:27:58 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 03:31:01 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:33:03 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:34:49 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 700.3186347484589 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:37:36 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 03:39:37 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 03:42:40 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:44:42 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:46:29 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 700.018230676651 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:49:16 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 03:51:17 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 03:54:17 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:56:18 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:58:03 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.5087685585022 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:00:49 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 04:02:50 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 04:05:52 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:07:54 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:09:43 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 699.7918429374695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:12:29 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 04:14:30 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 04:17:31 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:19:33 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:21:20 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 696.7430837154388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:24:05 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 04:26:05 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 04:29:06 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:31:07 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:32:53 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.534600019455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 04:35:40 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 04:37:41 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 04:40:43 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:42:44 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:44:29 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 695.6921377182007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:47:14 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 04:49:15 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 04:52:16 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:54:16 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:56:01 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 692.5210182666779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:58:47 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 05:00:48 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 05:03:50 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:05:51 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:07:37 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 695.6660025119781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:10:24 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 05:12:25 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 05:15:26 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:17:27 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:19:13 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 696.1608309745789 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:22:00 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 05:24:01 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 05:27:02 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:29:02 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:30:48 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 694.8649749755859 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:33:34 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 05:35:34 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 05:38:35 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:40:38 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:42:24 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 696.1094648838043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 05:45:10 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 05:47:11 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 05:50:13 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:52:14 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:54:00 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 695.6463339328766 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:56:46 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 05:58:47 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 06:01:48 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:03:49 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:05:35 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 695.7568061351776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 06:08:21 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 06:10:27 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 06:13:28 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:15:29 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:17:15 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 699.5045740604401 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 06:20:01 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 06:22:04 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 06:25:06 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:27:07 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:28:52 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 697.7823572158813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:31:39 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 06:33:40 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 06:36:40 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:38:41 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:40:27 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 694.4596664905548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:43:14 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 06:45:14 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 06:48:15 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:50:16 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:52:01 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.3380026817322 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 06:54:47 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 06:56:47 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 06:59:49 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:01:50 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:03:36 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 695.5864493846893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 07:06:22 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 07:08:23 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 07:11:26 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:13:26 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:15:12 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 696.2618720531464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 07:17:59 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 07:20:00 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 07:23:01 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:25:02 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:26:48 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 697.0499958992004 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 07:29:36 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 07:31:36 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 07:34:37 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:36:37 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:38:23 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 693.412034034729 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 07:41:09 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 07:43:12 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 07:46:15 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:48:16 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:50:02 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 699.256726026535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 07:52:48 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 07:54:49 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 07:57:51 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:59:52 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:01:38 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 695.9916725158691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 08:04:24 2024]  Iteration number: 0 with current cost as 0.2943888070988983 and parameters 
[-3.99694928  2.23743496 -2.12427866 -0.11653038  0.5538874  -2.77010865
  3.06858498  2.1896021   1.18552096 -1.06648341  0.60271543  1.14432478
  1.31029931 -1.87354648]. 
Working on 0.4 fold... 
[Thu Mar 28 08:06:25 2024]  Iteration number: 0 with current cost as 0.40005312952049255 and parameters 
[-2.51067543  2.23743471 -2.1242794  -0.11653087  0.55388716 -2.77010897
  3.0685849   2.18960153  1.18552022 -1.06648316  0.60271518  1.14432453
  1.31029906 -1.87354672]. 
Working on 0.6 fold... 
[Thu Mar 28 08:09:30 2024]  Iteration number: 0 with current cost as 0.2998860951107856 and parameters 
[-3.94387098  2.23743464 -2.12427964 -0.11653006  0.55388708 -2.77010897
  3.06858402  2.18960145  1.18552191 -1.06648405  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:11:31 2024]  Iteration number: 0 with current cost as 0.3197770841453957 and parameters 
[-2.52613687  2.23743464 -2.12427961 -0.11653097  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552002 -1.06648315  0.60271507  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:13:17 2024]  Iteration number: 0 with current cost as 0.3256669474905543 and parameters 
[-2.44927933  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.0664831   0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 698.726710319519 seconds. 
Discarding model... 

Training complete taking 17407.70370697975 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.107025146484375 seconds. 
Saved predicted values as M_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (8.006882265815866,), 'R2_train': -0.09940520921679341, 'MAE_train': 2.0870079231974747, 'MSE_test': 18.82194295795521, 'R2_test': -0.8763718831822125, 'MAE_test': 3.1189751445227842}. 
Saved model results as M_Efficient-CRZ_results.json. 
