/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:02:05 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:08:52 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:17:14 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 16:27:12 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 16:33:36 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:40:00 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2554.338501930237 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:51:05 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:59:15 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 17:09:12 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 17:15:34 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:21:57 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2527.1422457695007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:33:14 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:41:25 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 17:51:37 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 17:58:20 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:05:06 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2587.7309284210205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:16:35 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:25:33 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 18:36:32 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 18:43:12 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:49:49 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2684.969463825226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:01:32 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:10:08 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 19:21:03 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 19:28:07 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:35:05 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2720.4552845954895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:46:56 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:55:56 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 20:07:03 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 20:13:53 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:21:12 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2767.7257957458496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:32:57 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:41:58 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 20:53:02 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 21:00:00 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:07:12 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2763.6195278167725 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:19:04 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:27:58 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 21:38:53 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 21:45:43 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:52:36 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2717.0083446502686 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:03:58 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:12:26 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 22:22:51 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 22:29:34 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:36:12 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2609.855994939804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:47:47 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:56:43 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 23:07:26 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 23:14:10 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:20:52 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2677.651266813278 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:32:16 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:40:46 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Mon Apr  1 23:51:08 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Mon Apr  1 23:57:44 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:04:40 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2637.488785982132 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 00:16:07 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:24:45 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 00:35:13 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 00:41:46 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:48:27 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2615.375732898712 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:59:48 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:08:29 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 01:18:50 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 01:25:39 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:32:40 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2667.2404527664185 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 01:44:26 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:53:19 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 02:03:56 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 02:10:41 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:17:31 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2696.4356832504272 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:29:37 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:38:27 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 02:49:04 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 02:56:10 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:03:06 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2735.5939440727234 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 03:15:16 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:24:30 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 03:35:18 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 03:42:15 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:49:22 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2773.4184045791626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 04:01:10 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:10:10 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 04:21:13 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 04:28:06 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:35:01 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2725.0575489997864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:46:41 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:55:42 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 05:06:28 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 05:13:31 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:20:30 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2742.221925973892 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 05:32:32 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:41:35 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 05:52:13 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 05:58:53 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:05:26 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2682.0820581912994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 06:16:46 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 06:25:11 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 06:35:20 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 06:41:56 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:48:42 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2598.219974040985 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 07:00:07 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 07:08:49 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 07:19:29 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 07:26:12 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 07:32:59 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2657.1730573177338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 07:44:32 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 07:53:24 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 08:03:55 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 08:10:51 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 08:17:47 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2688.9281272888184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 08:29:16 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 08:38:07 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 08:48:42 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 08:55:24 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 09:02:21 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2681.9621057510376 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 09:14:00 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 09:22:40 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 09:33:14 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 09:40:03 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 09:46:46 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2666.2086069583893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 09:58:41 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 10:07:14 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Tue Apr  2 10:17:50 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Tue Apr  2 10:24:28 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 10:31:15 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2667.495635986328 seconds. 
Discarding model... 

Training complete taking 66845.40011548996 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.9403836727142334 seconds. 
Saved predicted values as M-A1-CZ_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (4.623524790523353,), 'R2_train': 0.3651552413420669, 'MAE_train': 1.5177881591217415, 'MSE_test': 11.981639061387453, 'R2_test': -0.19445748504529248, 'MAE_test': 2.47062093903899}. 
Saved model results as M-A1-CZ_Full-CRX_results.json. 
