/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Thu Mar 28 02:25:33 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:25:41 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 02:29:54 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 02:30:28 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:33:19 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 02:36:39 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 02:40:08 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 02:45:06 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1182.0104780197144 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 02:45:23 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 02:49:34 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 02:50:08 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:52:59 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 02:56:19 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 02:59:49 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 03:04:49 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1182.9096052646637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:05:06 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 03:09:15 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 03:09:49 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:12:38 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 03:15:58 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 03:19:24 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 03:24:20 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1171.2081544399261 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 03:24:37 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 03:28:47 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 03:29:20 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:32:11 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 03:35:31 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 03:38:59 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 03:43:55 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1174.3798580169678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 03:44:12 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 03:48:21 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 03:48:54 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:51:43 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 03:55:00 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 03:58:26 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 04:03:23 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1168.0236587524414 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:03:40 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 04:07:50 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 04:08:24 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:11:13 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 04:14:32 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 04:18:07 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 04:23:04 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1181.773235321045 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 04:23:21 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 04:27:30 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 04:28:04 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:30:53 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 04:34:11 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 04:37:38 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 04:42:33 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1169.0126960277557 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:42:50 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 04:46:59 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 04:47:33 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:50:23 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 04:53:42 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 04:57:10 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 05:02:07 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1173.5166070461273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:02:24 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 05:06:35 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 05:07:09 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:09:58 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 05:13:16 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 05:16:44 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 05:21:38 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1170.982699394226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:21:56 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 05:26:05 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 05:26:39 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:29:28 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 05:32:47 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 05:36:14 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 05:41:11 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1173.5527698993683 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:41:29 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 05:45:37 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 05:46:11 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:49:00 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 05:52:19 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 05:55:48 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 06:00:45 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1173.6340794563293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:01:02 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 06:05:11 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 06:05:44 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:08:32 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 06:11:52 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 06:15:20 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 06:20:24 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1179.1311564445496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:20:41 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 06:24:52 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 06:25:26 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:28:15 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 06:31:34 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 06:35:01 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 06:39:57 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1173.1294841766357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 06:40:14 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 06:44:23 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 06:44:57 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:47:46 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 06:51:04 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 06:54:31 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 06:59:27 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1169.0840816497803 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 06:59:43 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 07:03:53 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 07:04:27 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 07:07:16 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 07:10:35 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 07:14:03 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 07:18:59 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1172.171330690384 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 07:19:16 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 07:23:25 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 07:23:59 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 07:26:48 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 07:30:08 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 07:33:36 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 07:38:41 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1182.695193529129 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 07:38:59 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 07:43:08 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 07:43:43 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 07:46:32 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 07:49:51 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 07:53:19 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 07:58:15 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1174.0331695079803 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 07:58:32 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 08:02:44 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 08:03:18 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 08:06:07 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 08:09:26 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 08:12:53 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 08:17:59 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1183.199749469757 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 08:18:16 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 08:22:25 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 08:23:00 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 08:25:49 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 08:29:07 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 08:32:36 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 08:37:31 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1172.2907519340515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 08:37:48 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 08:41:57 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 08:42:31 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 08:45:25 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 08:48:44 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 08:52:11 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 08:57:07 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1175.7463145256042 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 08:57:24 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 09:01:33 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 09:02:07 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 09:04:57 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 09:08:16 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 09:11:44 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 09:16:39 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1172.255827665329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 09:16:56 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 09:21:06 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 09:21:39 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 09:24:30 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 09:27:49 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 09:31:17 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 09:36:13 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1173.9782752990723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 09:36:31 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 09:40:40 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 09:41:14 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 09:44:03 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 09:47:21 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 09:50:49 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 09:55:46 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1173.4622721672058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 09:56:03 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 10:00:12 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 10:00:46 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 10:03:35 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 10:06:53 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 10:10:21 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 10:15:17 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1171.1625735759735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 10:15:34 2024]  Iteration number: 0 with current cost as 0.43394119023592925 and parameters 
[-2.90318345  2.23743463 -2.12427963 -0.11580207  0.4988931  -2.85655795
  3.06875263  2.21307464  1.17186303 -1.07357529  0.55570421  1.12101702
  1.30745404 -1.88376165  0.70262425]. 
[Thu Mar 28 10:19:52 2024]  Iteration number: 50 with current cost as 0.1282731600882607 and parameters 
[-2.90318337  2.23743446 -2.12428    -0.61967741  1.38151671 -4.57642894
  2.78140537  1.79291972  1.49229331 -0.53579379  2.2198209   1.21839542
  2.02911347 -1.46379919  1.77714195]. 
Working on 0.4 fold... 
[Thu Mar 28 10:20:26 2024]  Iteration number: 0 with current cost as 0.40010951197731937 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11444568  0.50495358 -2.84534174
  3.0684635   2.21108275  1.17349585 -1.07257437  0.56150635  1.12404611
  1.30750641 -1.88245796  0.7071157 ]. 
Working on 0.6 fold... 
[Thu Mar 28 10:23:15 2024]  Iteration number: 0 with current cost as 0.45334275525787276 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.11652124  0.49730841 -2.85991933
  3.06866796  2.21423182  1.17134996 -1.07367714  0.55396657  1.1203501
  1.3086619  -1.88428337  0.69670524]. 
Working on 0.8 fold... 
[Thu Mar 28 10:26:34 2024]  Iteration number: 0 with current cost as 0.4222590105535494 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.1121181   0.49704559 -2.85515937
  3.06835479  2.2143953   1.17179677 -1.07160931  0.55522789  1.12325457
  1.30983395 -1.88402032  0.69371586]. 
Working on 1.0 fold... 
[Thu Mar 28 10:30:01 2024]  Iteration number: 0 with current cost as 0.37752951727669803 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.11540392  0.50794524 -2.84171901
  3.06852173  2.20978896  1.17413229 -1.07289178  0.56383058  1.12438749
  1.31017696 -1.88229192  0.69875299]. 
[Thu Mar 28 10:35:00 2024]  Iteration number: 50 with current cost as 0.1231785985852526 and parameters 
[-2.90318347  2.23743495 -2.12427923  0.72870303 -2.10463045 -4.28608167
  2.63322978  1.32526092  1.70560006 -0.03298869 -1.2912098   1.58009571
  2.49227303 -1.79578004  1.40327773]. 
Training complete taking 1182.259268283844 seconds. 
Discarding model... 

Training complete taking 29375.60459637642 total seconds. 
Now scoring model... 
Scoring complete taking 0.8125536441802979 seconds. 
Saved predicted values as IQP_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (2.883418637893933,), 'R2_train': 0.6040849152500474, 'MAE_train': 1.2413168804850532, 'MSE_test': 5.098301473704559, 'R2_test': 0.49174697008618795, 'MAE_test': 1.7794081081426083}. 
Saved model results as IQP_HWE-CNOT_results.json. 
