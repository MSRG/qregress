/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:01:57 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:09:35 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:15:31 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:22:20 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 16:29:50 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 16:37:46 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2267.3292632102966 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:39:43 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:46:46 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:52:35 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:59:33 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:07:02 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 17:14:57 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2229.7024445533752 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:16:54 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:24:04 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:30:00 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:37:06 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 17:44:32 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 17:52:26 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2253.8369011878967 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:54:31 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:01:45 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:07:39 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:14:28 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:21:49 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 18:30:02 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2250.330400943756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:31:59 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:39:17 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:44:59 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:51:50 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 18:59:23 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 19:07:15 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2242.5397593975067 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:09:19 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:16:38 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:22:20 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:29:07 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 19:36:31 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 19:44:56 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2251.9962043762207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:46:51 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:54:01 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:00:44 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:07:51 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:15:19 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 20:23:36 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2320.9509937763214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:25:34 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:32:39 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:38:20 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:45:09 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 20:52:29 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 21:00:25 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2206.9475321769714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:02:21 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:09:31 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:15:41 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:22:30 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 21:29:57 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 21:37:57 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2252.519152879715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:39:53 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:47:20 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:53:02 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:59:51 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:07:21 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 22:15:25 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2248.2395651340485 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:17:22 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:24:27 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:30:12 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:37:01 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 22:44:50 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 22:52:54 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2249.3427636623383 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:54:51 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:02:00 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:07:43 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:14:49 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:22:14 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 23:30:08 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2234.68359041214 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:32:04 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:39:11 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:44:54 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:51:41 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Mon Apr  1 23:59:04 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 00:07:00 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2211.1122229099274 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:08:57 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:16:00 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:21:42 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:28:31 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 00:35:57 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 00:43:55 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2214.9740178585052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:45:52 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:52:56 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:58:38 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:05:42 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 01:13:17 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 01:21:39 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2263.7646358013153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:23:36 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:30:40 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 01:36:25 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:43:14 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 01:50:39 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 01:58:45 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2226.043317079544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:00:42 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:07:49 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 02:13:38 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 02:20:39 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 02:28:08 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 02:36:10 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2245.0653626918793 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:38:07 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:45:09 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 02:51:07 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 02:58:01 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 03:05:33 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 03:13:25 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2239.9659926891327 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:15:25 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:22:29 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 03:28:13 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 03:35:03 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 03:42:26 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 03:50:20 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2210.482506752014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:52:18 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:59:19 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:05:02 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 04:11:49 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 04:19:13 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 04:27:08 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2206.975290775299 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 04:29:04 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:36:06 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:41:48 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 04:48:37 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 04:56:09 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 05:04:07 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2219.592663049698 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 05:06:02 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 05:13:07 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 05:18:48 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 05:25:45 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 05:33:20 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 05:41:16 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2234.850264072418 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 05:43:18 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 05:50:27 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 05:56:25 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 06:03:50 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 06:11:13 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 06:19:09 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2266.588888168335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 06:21:04 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 06:28:36 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 06:34:35 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 06:42:02 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 06:49:30 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 06:58:00 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2331.4288918972015 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 06:59:57 2024]  Iteration number: 0 with current cost as 0.10638708319073556 and parameters 
[-2.84289658  2.26704713 -2.13029703 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.625325    1.14432445
  1.2349793  -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 07:07:21 2024]  Iteration number: 0 with current cost as 0.10553200857348165 and parameters 
[-2.85365415  2.28340266 -2.13159074 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.64002918  1.14432445
  1.24416566 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 07:13:06 2024]  Iteration number: 0 with current cost as 0.1166353146562033 and parameters 
[-2.83565317  2.27860469 -2.1313237  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63569421  1.14432445
  1.22515664 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 07:19:54 2024]  Iteration number: 0 with current cost as 0.10084604463717628 and parameters 
[-2.82479521  2.27936114 -2.1312899  -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63673034  1.14432445
  1.21354715 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Tue Apr  2 07:27:19 2024]  Iteration number: 0 with current cost as 0.11790918251336155 and parameters 
[-2.78080588  2.36771852 -2.14317048 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.71234986  1.14432445
  1.14773125 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 07:35:11 2024]  Iteration number: 50 with current cost as 0.04706894861080996 and parameters 
[-4.21747376  3.07690872 -4.11819903 -0.11653708  0.5539049  -2.7701439
  3.06859444  2.18959525  1.18551701 -1.06647501 -3.05507988  1.14435675
  0.73256688 -1.87353932  0.72962782  2.88579272 -0.54533645 -0.4752299
 -2.02653778  0.72898967  1.60514295  2.83076213 -1.26457378 -0.25136579]. 
Training complete taking 2232.5431480407715 seconds. 
Discarding model... 

Training complete taking 56111.80753111839 total seconds. 
Now scoring model... 
Scoring complete taking 0.825690746307373 seconds. 
Saved predicted values as A2_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (1.0850094994171406,), 'R2_train': 0.8510200280074479, 'MAE_train': 0.8442269268162665, 'MSE_test': 1.684664130075897, 'R2_test': 0.8320547239282756, 'MAE_test': 1.0432693101413808}. 
Saved model results as A2_Full-Pauli-CRZ_results.json. 
