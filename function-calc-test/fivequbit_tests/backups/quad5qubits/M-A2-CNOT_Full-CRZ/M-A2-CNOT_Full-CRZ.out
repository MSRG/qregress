/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:46:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:51:14 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 15:59:24 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:07:25 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:15:19 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:23:21 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2421.3956139087677 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:31:20 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 16:39:20 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:47:24 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:55:24 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:03:26 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2412.552876472473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:11:41 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 17:19:45 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:27:44 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:36:07 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:44:10 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2442.567536354065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:52:24 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:00:36 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:08:42 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:17:07 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:25:17 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2474.768512248993 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:33:34 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:41:39 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:49:53 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:57:58 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:06:07 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2444.0999040603638 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:14:24 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:22:33 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:30:53 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:39:07 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:47:14 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2459.575830221176 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:55:24 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 20:03:46 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:11:48 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:19:54 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:28:07 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2458.4341332912445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:36:18 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 20:44:28 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:52:59 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:01:00 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:09:08 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2480.426059484482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:17:49 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:26:25 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:35:00 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:43:32 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:51:49 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2539.991017818451 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:00:01 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:08:20 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:16:34 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:24:52 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:32:56 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2468.5477821826935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:41:06 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:49:07 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:57:27 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:05:36 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:13:36 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2441.2171137332916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:21:45 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 23:29:48 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:37:57 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:46:14 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:54:11 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2428.2408328056335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:02:10 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:10:23 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:18:26 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:26:35 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:35:01 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2455.1253139972687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:43:31 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:51:44 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:00:11 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:08:32 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:16:48 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2506.397245645523 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 01:24:52 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 01:33:03 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:41:23 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:49:56 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:58:31 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2520.0771539211273 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 02:07:02 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:15:16 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:23:15 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:31:19 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:39:34 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2453.1423358917236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:47:46 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:55:52 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:03:55 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:12:08 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:20:10 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2437.5168795585632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 03:28:33 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 03:37:05 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:45:29 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:53:44 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:01:48 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2505.303209543228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 04:10:32 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 04:18:31 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:26:46 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:34:48 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:42:48 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2443.6367104053497 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 04:50:49 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 04:58:50 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:06:46 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:14:48 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:22:50 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2394.7460277080536 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 05:30:52 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 05:38:51 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:46:58 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:55:00 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:03:19 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2435.92316865921 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 06:11:22 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 06:19:39 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 06:27:55 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 06:36:02 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:44:08 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2450.391743183136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 06:52:26 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 07:00:45 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 07:09:00 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 07:17:08 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 07:25:13 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2466.4463300704956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 07:33:19 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 07:41:23 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 07:49:33 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 07:57:53 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 08:06:33 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2493.5218019485474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 08:15:07 2024]  Iteration number: 0 with current cost as 0.20767318926165448 and parameters 
[-2.40376861  2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010897
  3.068585    2.18960145  1.18552002 -1.0664831   0.60271512  1.14432448
  1.31029899 -1.8735468   0.72965079  2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512664  2.83077106 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550786 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 08:24:03 2024]  Iteration number: 0 with current cost as 0.19813386690871543 and parameters 
[-2.39265684  2.23743464 -2.12427961 -0.116531    0.55388709 -2.77010896
  3.06858498  2.18960145  1.18552001 -1.0664831   0.60271513  1.14432448
  1.31029897 -1.87354679  0.7296508   2.88578421 -0.54534334 -0.47522485
 -2.02654243  0.72897367  1.60512664  2.83077107 -1.26456708 -0.25136103
 -2.39279216 -2.27309774  3.13337152  2.54856955 -0.67550789 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 08:32:59 2024]  Iteration number: 0 with current cost as 0.19068530213583346 and parameters 
[-2.45716183  2.23743464 -2.12427961 -0.116531    0.55388708 -2.77010897
  3.06858498  2.18960145  1.18552001 -1.06648308  0.6027151   1.14432448
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 08:42:03 2024]  Iteration number: 0 with current cost as 0.1455065000565824 and parameters 
[-2.51316813  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552002 -1.06648307  0.60271513  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534335 -0.47522484
 -2.02654242  0.72897368  1.60512665  2.83077109 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 08:50:41 2024]  Iteration number: 0 with current cost as 0.1535315600554532 and parameters 
[-2.4813466   2.23743465 -2.1242796  -0.11653103  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552    -1.0664831   0.60271512  1.14432447
  1.31029897 -1.87354682  0.72965079  2.88578419 -0.54534335 -0.47522483
 -2.02654242  0.72897368  1.60512662  2.83077107 -1.26456711 -0.25136105
 -2.3927922  -2.27309774  3.13337153  2.54856957 -0.67550787 -2.69002203]. 
Training complete taking 2653.018543243408 seconds. 
Discarding model... 

Training complete taking 61687.06453227997 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.7417795658111572 seconds. 
Saved predicted values as M-A2-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.9415562023217277,), 'R2_train': 0.4587946622177893, 'MAE_train': 1.381709061178546, 'MSE_test': 6.028612064500459, 'R2_test': 0.3990036949049045, 'MAE_test': 1.859773572551869}. 
Saved model results as M-A2-CNOT_Full-CRZ_results.json. 
