/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 10:51:47 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 10:51:57 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 10:54:37 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 10:57:21 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 11:00:06 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 11:03:20 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 860.905519247055 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 11:06:18 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 11:08:57 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 11:11:40 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 11:14:25 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 11:17:37 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 862.9058921337128 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 11:20:41 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 11:23:19 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 11:26:02 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 11:28:45 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 11:31:56 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 852.3378324508667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 11:34:53 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 11:37:31 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 11:40:15 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 11:42:58 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 11:46:12 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 856.5919737815857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 11:49:10 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 11:51:48 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 11:54:32 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 11:57:16 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 12:00:30 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 865.5440969467163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 12:03:35 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 12:06:14 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 12:08:56 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 12:11:39 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 12:14:57 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 859.8591639995575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 12:17:55 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 12:20:34 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 12:23:17 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 12:26:01 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 12:29:13 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 856.3289906978607 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 12:32:12 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 12:34:50 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 12:37:34 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 12:40:18 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 12:43:31 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 858.2550299167633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 12:46:30 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 12:49:08 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 12:51:50 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 12:54:33 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 12:57:45 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 853.4582915306091 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 13:00:43 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 13:03:23 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 13:06:07 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 13:08:51 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 13:12:08 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 862.3985333442688 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 13:15:06 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 13:17:45 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 13:20:29 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 13:23:13 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 13:26:26 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 857.7735028266907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 13:29:23 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 13:32:02 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 13:34:46 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 13:37:29 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 13:40:42 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 857.0855674743652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 13:43:41 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 13:46:20 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 13:49:04 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 13:51:49 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 13:55:01 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 856.9699141979218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 13:57:57 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 14:00:36 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 14:03:28 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 14:06:12 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 14:09:26 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 868.5490181446075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 14:12:27 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 14:15:16 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 14:17:59 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 14:20:42 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 14:23:55 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 867.5570750236511 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 14:26:54 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 14:29:33 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 14:32:16 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 14:35:00 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 14:38:13 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 857.9496278762817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 14:41:11 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 14:43:49 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 14:46:33 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 14:49:16 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 14:52:28 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 854.6667840480804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 14:55:26 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 14:58:06 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 15:00:50 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 15:03:33 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 15:06:44 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 856.0987567901611 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 15:09:42 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 15:12:21 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 15:15:05 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 15:17:50 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 15:21:03 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 858.5735161304474 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 15:24:01 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 15:26:39 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 15:29:22 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 15:32:07 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 15:35:21 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 858.1611080169678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 15:38:19 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 15:40:58 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 15:43:43 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 15:46:27 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 15:49:40 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 857.7782344818115 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 15:52:37 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 15:55:16 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 15:58:01 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 16:00:48 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 16:04:06 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 869.0165612697601 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 16:07:07 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 16:09:53 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 16:12:36 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 16:15:19 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 16:18:31 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 864.6495378017426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 16:21:31 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 16:24:11 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 16:26:55 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 16:29:39 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 16:32:52 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 859.6827306747437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 16:35:50 2024]  Iteration number: 0 with current cost as 0.4899857465430309 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07995718  0.5674042  -2.70564633
  2.98391589  2.25512982  1.29416718 -1.12152705  0.69532779  1.10646328
  1.40101167 -1.80301493  0.67789634]. 
Working on 0.4 fold... 
[Wed Mar 27 16:38:28 2024]  Iteration number: 0 with current cost as 0.45562533048584486 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09448342  0.56998237 -2.71863324
  2.98855858  2.25325836  1.28722919 -1.13790907  0.69190119  1.08550601
  1.39063362 -1.81208353  0.68023969]. 
Working on 0.6 fold... 
[Wed Mar 27 16:41:13 2024]  Iteration number: 0 with current cost as 0.5113256333798368 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.08711929  0.57041563 -2.70928667
  2.97788697  2.25672076  1.30365472 -1.13228403  0.69666917  1.09382492
  1.39788493 -1.80780127  0.67124873]. 
Working on 0.8 fold... 
[Wed Mar 27 16:43:57 2024]  Iteration number: 0 with current cost as 0.4760171638190952 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07716448  0.56995417 -2.69831446
  2.98134226  2.2659407   1.29244723 -1.11659433  0.70401352  1.11516671
  1.39738856 -1.81239429  0.65647077]. 
Working on 1.0 fold... 
[Wed Mar 27 16:47:13 2024]  Iteration number: 0 with current cost as 0.43332051032686314 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09431893  0.57179345 -2.71556465
  3.00599695  2.25714144  1.25495488 -1.13521766  0.68799818  1.08755277
  1.36106169 -1.84437756  0.66381445]. 
Training complete taking 860.844119310379 seconds. 
Discarding model... 

Training complete taking 21493.943056821823 total seconds. 
Now scoring model... 
Scoring complete taking 0.9924168586730957 seconds. 
Saved predicted values as A1-A1-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (3.520416098878009,), 'R2_train': 0.5166203686744516, 'MAE_train': 1.6850814924202708, 'MSE_test': 4.922555931203176, 'R2_test': 0.5092671589041454, 'MAE_test': 1.8119939185648337}. 
Saved model results as A1-A1-CZ_HWE-CNOT_results.json. 
