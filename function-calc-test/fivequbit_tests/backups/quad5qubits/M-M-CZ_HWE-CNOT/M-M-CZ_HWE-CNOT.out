/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:01:51 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 16:06:49 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 16:12:26 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 16:18:18 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 16:18:38 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 16:24:23 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 16:25:06 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1734.3235023021698 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:30:44 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 16:35:41 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 16:41:25 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 16:47:20 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 16:47:40 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 16:53:30 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 16:54:12 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1746.405576467514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:59:50 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 17:04:54 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 17:10:41 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 17:16:33 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 17:16:54 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 17:22:41 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 17:23:24 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1755.2125930786133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:29:04 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 17:34:04 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 17:39:50 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 17:45:44 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 17:46:06 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 17:51:51 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 17:52:32 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1749.0196652412415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:58:13 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 18:03:10 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 18:08:50 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 18:14:45 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 18:15:06 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 18:20:57 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 18:21:38 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1740.591816663742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:27:14 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 18:32:12 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 18:37:51 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 18:43:48 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 18:44:09 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 18:49:57 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 18:50:38 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1743.2041447162628 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:56:17 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 19:01:21 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 19:07:00 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 19:12:50 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 19:13:12 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 19:18:59 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 19:19:40 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1749.8221979141235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:25:27 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 19:30:25 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 19:36:11 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 19:42:08 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 19:42:28 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 19:48:20 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 19:49:01 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1751.8715965747833 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:54:39 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 19:59:36 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 20:05:24 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 20:11:25 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 20:11:46 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 20:17:50 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 20:18:32 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1781.9620637893677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:24:21 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 20:29:16 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 20:35:03 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 20:40:51 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 20:41:12 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 20:46:58 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 20:47:38 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1736.3146913051605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:53:17 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 20:58:37 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 21:04:22 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 21:10:13 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 21:10:36 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 21:16:20 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 21:17:02 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1762.222485780716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:22:40 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 21:27:37 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 21:33:16 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 21:39:07 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 21:39:28 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 21:45:12 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 21:45:52 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1728.6428847312927 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:51:28 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 21:56:23 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 22:02:02 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 22:07:54 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 22:08:16 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 22:14:01 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 22:14:42 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1732.0440120697021 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:20:20 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 22:25:16 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 22:30:54 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 22:36:47 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 22:37:09 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 22:42:54 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 22:43:36 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1738.3366603851318 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:49:19 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 22:54:27 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 23:00:03 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 23:06:09 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 23:06:30 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 23:12:15 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 23:12:57 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1756.670663356781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:18:35 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 23:23:33 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 23:29:11 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Mon Apr  1 23:35:00 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Mon Apr  1 23:35:21 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Mon Apr  1 23:41:05 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Mon Apr  1 23:41:47 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1734.366938829422 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:47:30 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Mon Apr  1 23:52:28 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Mon Apr  1 23:58:06 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 00:03:57 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 00:04:19 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 00:10:00 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 00:10:42 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1729.7240505218506 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:16:21 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 00:21:18 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 00:26:56 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 00:32:47 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 00:33:08 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 00:38:51 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 00:39:33 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1728.8445763587952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:45:08 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 00:50:04 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 00:55:45 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 01:01:38 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 01:01:58 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 01:07:44 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 01:08:26 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1735.098580121994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 01:14:03 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 01:18:59 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 01:24:39 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 01:30:33 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 01:30:53 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 01:36:40 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 01:37:22 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1737.5982568264008 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:43:01 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 01:47:57 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 01:53:34 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 01:59:26 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 01:59:46 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 02:05:46 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 02:06:28 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1745.9543597698212 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:12:07 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 02:17:04 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 02:22:43 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 02:28:33 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 02:28:53 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 02:34:39 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 02:35:21 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1732.0059671401978 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:40:59 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 02:45:57 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 02:51:38 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 02:57:30 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 02:57:50 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 03:03:39 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 03:04:21 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1739.3663668632507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:09:58 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 03:14:58 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 03:20:39 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 03:26:36 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 03:26:57 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 03:32:43 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 03:33:25 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1742.4829502105713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:39:01 2024]  Iteration number: 0 with current cost as 0.4172488488630451 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.10622205  0.58475748 -2.70897918
  2.98681789  2.25402884  1.28979128 -1.04704663  0.68710229  1.19430239
  1.39021933 -1.80571838  0.70441617]. 
Working on 0.4 fold... 
[Tue Apr  2 03:43:59 2024]  Iteration number: 0 with current cost as 0.3969666415429765 and parameters 
[-2.90318345  2.23743463 -2.12427964 -0.11200484  0.58326428 -2.71814911
  3.00416191  2.24766192  1.26351696 -1.05443533  0.67810377  1.18251989
  1.38479037 -1.81101707  0.70365619]. 
Working on 0.6 fold... 
[Tue Apr  2 03:49:39 2024]  Iteration number: 0 with current cost as 0.4397687074990164 and parameters 
[-2.90318346  2.23743464 -2.12427964 -0.11464018  0.58519769 -2.71817834
  2.96676589  2.27114037  1.31461498 -1.05700455  0.69490008  1.18465262
  1.38597745 -1.80635131  0.71637779]. 
[Tue Apr  2 03:55:33 2024]  Iteration number: 50 with current cost as 0.1451433526899215 and parameters 
[-2.9031838   2.23743429 -2.12428017 -0.34949721  0.97083579 -2.56621592
  2.39140896  2.77082615  2.06103096 -0.67373927  0.19253322  0.98736761
  0.32512162 -3.72761072  0.51184599]. 
Working on 0.8 fold... 
[Tue Apr  2 03:55:54 2024]  Iteration number: 0 with current cost as 0.41264205960457645 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.1230391   0.58584795 -2.72702143
  2.97207822  2.26963068  1.30631655 -1.054606    0.69319472  1.187031
  1.38058033 -1.81043774  0.71985084]. 
[Tue Apr  2 04:01:41 2024]  Iteration number: 50 with current cost as 0.11418130241153146 and parameters 
[-2.90318433  2.23743446 -2.12427969 -0.36908817  0.92365424 -2.6504501
  2.37257776  2.83996593  2.06118451 -0.64098685  0.30726845  0.95950596
  0.14404481 -4.23097485  0.42418391]. 
Working on 1.0 fold... 
[Tue Apr  2 04:02:24 2024]  Iteration number: 0 with current cost as 0.3744803517212087 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.12920085  0.58710031 -2.73227822
  2.9906466   2.26658554  1.27603974 -1.06184724  0.68470866  1.17558772
  1.35800062 -1.83106667  0.72173378]. 
Training complete taking 1739.3516309261322 seconds. 
Discarding model... 

Training complete taking 43571.439907073975 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.2482283115386963 seconds. 
Saved predicted values as M-M-CZ_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (2.9012736262717373,), 'R2_train': 0.6016332909371898, 'MAE_train': 1.2849415215790356, 'MSE_test': 13.752472937393275, 'R2_test': -0.3709930798107375, 'MAE_test': 2.453556829995592}. 
Saved model results as M-M-CZ_HWE-CNOT_results.json. 
