/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:46:32 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:48:57 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 15:51:41 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 15:55:51 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 15:58:33 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:01:01 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 951.4961416721344 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:04:49 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 16:07:34 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 16:11:47 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 16:14:43 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:17:20 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 979.8667371273041 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:21:16 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 16:24:02 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 16:28:18 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 16:31:08 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:33:34 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 980.4003365039825 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:37:36 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 16:40:30 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 16:44:52 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 16:48:00 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 16:50:32 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 1019.5096218585968 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 16:54:33 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 16:57:30 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 17:01:49 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 17:04:36 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:07:05 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 985.1228642463684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:10:55 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 17:13:51 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 17:18:02 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 17:20:51 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:23:27 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 988.1880798339844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:27:29 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 17:30:17 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 17:34:30 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 17:37:20 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:39:51 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 982.8711745738983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:44:00 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 17:46:54 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 17:51:13 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 17:54:02 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 17:56:43 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 1009.0568425655365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:00:34 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 18:03:20 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 18:07:34 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 18:10:20 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:12:54 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 969.5209839344025 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:16:40 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 18:19:31 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 18:23:39 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 18:26:27 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:29:06 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 976.250913143158 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:33:04 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 18:35:54 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 18:40:19 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 18:43:14 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 18:45:41 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 995.8896429538727 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:49:36 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 18:52:21 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 18:56:45 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 18:59:26 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:01:49 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 964.3440890312195 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:05:44 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 19:08:43 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 19:12:46 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 19:15:25 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:17:44 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 948.4881203174591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:21:22 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 19:24:13 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 19:28:23 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 19:31:15 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:33:39 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 964.1561288833618 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:37:34 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 19:40:31 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 19:44:46 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 19:47:30 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 19:49:56 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 974.0611650943756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:53:45 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 19:56:27 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 20:00:33 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 20:03:21 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:05:42 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 942.0276114940643 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:09:25 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 20:12:13 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 20:16:18 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 20:19:11 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:21:41 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 963.124728679657 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:25:28 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 20:28:06 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 20:32:15 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 20:34:54 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:37:16 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 931.2480137348175 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:40:54 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 20:43:40 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 20:47:41 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 20:50:24 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 20:52:46 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 928.0222661495209 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:56:23 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 20:59:01 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 21:03:03 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 21:05:45 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 21:08:04 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 919.1187753677368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:11:44 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 21:14:30 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 21:18:37 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 21:21:22 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 21:23:46 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 943.9747958183289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:27:35 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 21:30:15 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 21:34:25 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 21:37:20 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 21:39:59 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 975.263929605484 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:43:48 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 21:46:40 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 21:50:44 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 21:53:25 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 21:55:45 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 944.7513780593872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:59:31 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 22:02:15 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 22:06:14 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 22:08:53 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 22:11:12 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 929.0360004901886 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:14:56 2024]  Iteration number: 0 with current cost as 0.29438880569293624 and parameters 
[-3.99694951  2.23743431 -2.12427899 -0.1165307   0.55388675 -2.77010897
  3.06858498  2.18960113  1.18551998 -1.06648373  0.60271445  1.14432413
  1.31029834 -1.87354615]. 
Working on 0.4 fold... 
[Mon Apr  1 22:17:37 2024]  Iteration number: 0 with current cost as 0.4000531295204922 and parameters 
[-2.51067543  2.23743456 -2.1242794  -0.11653095  0.553887   -2.77010905
  3.06858498  2.18960153  1.18552006 -1.06648316  0.60271502  1.14432453
  1.31029891 -1.87354664]. 
Working on 0.6 fold... 
[Mon Apr  1 22:21:39 2024]  Iteration number: 0 with current cost as 0.29988609512028763 and parameters 
[-3.94387096  2.23743464 -2.12427771 -0.11653103  0.55388708 -2.77010993
  3.06858498  2.18960145  1.18551998 -1.06648501  0.60271414  1.14432445
  1.31029802 -1.87354488]. 
Working on 0.8 fold... 
[Mon Apr  1 22:24:22 2024]  Iteration number: 0 with current cost as 0.3197770841033776 and parameters 
[-2.52613687  2.23743457 -2.12427957 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354674]. 
Working on 1.0 fold... 
[Mon Apr  1 22:26:43 2024]  Iteration number: 0 with current cost as 0.32566694810617225 and parameters 
[-2.44927934  2.2374346  -2.1242796  -0.11653103  0.55388706 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.0664831   0.60271508  1.14432445
  1.31029895 -1.87354676]. 
Training complete taking 923.7843134403229 seconds. 
Discarding model... 

Training complete taking 24089.575505256653 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.2071962356567383 seconds. 
Saved predicted values as M_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (8.006882278556338,), 'R2_train': -0.09940521096615607, 'MAE_train': 2.087007936146331, 'MSE_test': 18.82194280691046, 'R2_test': -0.8763718681244621, 'MAE_test': 3.118975128319556}. 
Saved model results as M_Efficient-CRX_results.json. 
