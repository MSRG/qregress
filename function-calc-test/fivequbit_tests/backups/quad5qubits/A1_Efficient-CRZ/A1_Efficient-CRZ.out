/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Thu Mar 28 02:46:58 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:47:43 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:49:12 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:50:40 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:52:08 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 02:53:36 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 441.58180809020996 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 02:55:04 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:56:32 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:58:00 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:59:28 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:00:57 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.8892397880554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:02:25 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:03:53 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:05:21 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:06:50 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:08:18 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 441.77656745910645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 03:09:46 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:11:15 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:12:44 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:14:13 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:15:42 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 444.30036973953247 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 03:17:11 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:18:40 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:20:08 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:21:37 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:23:05 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 443.7443492412567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 03:24:34 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:26:03 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:27:31 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:28:59 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:30:28 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 442.9666905403137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:32:00 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:33:28 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:34:56 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:36:24 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:37:52 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 442.1534614562988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:39:19 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:40:48 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:42:16 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:43:44 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:45:12 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 441.4457492828369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 03:46:42 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:48:10 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:49:38 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:51:07 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:52:36 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 443.3358852863312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 03:54:04 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:55:32 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:57:00 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:58:28 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 03:59:56 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 439.8859386444092 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:01:24 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:02:53 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:04:21 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:05:48 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:07:16 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 439.7549035549164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 04:08:44 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:10:13 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:11:40 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:13:09 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:14:37 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.7531707286835 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:16:05 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:17:33 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:19:02 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:20:31 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:21:59 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 442.2011351585388 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:23:27 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:24:56 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:26:23 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:27:51 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:29:19 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.0208053588867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:30:47 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:32:14 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:33:42 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:35:10 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:36:39 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.48789048194885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:38:07 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:39:36 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:41:04 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:42:32 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:44:00 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.7620668411255 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 04:45:28 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:46:56 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:48:24 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:49:52 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:51:20 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 441.3873825073242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:52:50 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:54:17 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:55:45 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:57:13 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 04:58:41 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 438.7563271522522 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:00:08 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:01:36 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:03:04 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:04:32 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:06:00 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.13289189338684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:07:28 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:08:58 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:10:26 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:11:55 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:13:23 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 443.354088306427 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:14:53 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:16:22 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:17:49 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:19:19 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:20:46 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 442.5938527584076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:22:15 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:23:43 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:25:11 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:26:40 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:28:08 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 441.54040455818176 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 05:29:36 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:31:04 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:32:37 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:34:05 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:35:33 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 444.906697511673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:37:00 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:38:31 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:39:58 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:41:26 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:42:53 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 440.3116557598114 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:44:21 2024]  Iteration number: 0 with current cost as 0.28647264636833947 and parameters 
[-1.96813942  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010897
  3.06858498  2.18960149  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029902 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:45:49 2024]  Iteration number: 0 with current cost as 0.29001726441155845 and parameters 
[-1.90475592  2.2374346  -2.1242796  -0.11653099  0.55388704 -2.77010894
  3.06858498  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:47:19 2024]  Iteration number: 0 with current cost as 0.29464261974528966 and parameters 
[-1.98878494  2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010897
  3.06858498  2.18960149  1.18552006 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:48:50 2024]  Iteration number: 0 with current cost as 0.24369965341677752 and parameters 
[-2.02043654  2.2374346  -2.12427962 -0.11653099  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Thu Mar 28 05:50:18 2024]  Iteration number: 0 with current cost as 0.25745411930284223 and parameters 
[-1.92596474  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.06858495  2.18960149  1.18552002 -1.06648308  0.60271514  1.14432449
  1.31029902 -1.8735468 ]. 
Training complete taking 445.0265598297119 seconds. 
Discarding model... 

Training complete taking 11044.070727825165 total seconds. 
Now scoring model... 
Scoring complete taking 1.856482982635498 seconds. 
Saved predicted values as A1_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (6.049104124082392,), 'R2_train': 0.16941246738397964, 'MAE_train': 2.02668666491203, 'MSE_test': 9.820162566610893, 'R2_test': 0.021021529529225647, 'MAE_test': 2.7577882435086396}. 
Saved model results as A1_Efficient-CRZ_results.json. 
