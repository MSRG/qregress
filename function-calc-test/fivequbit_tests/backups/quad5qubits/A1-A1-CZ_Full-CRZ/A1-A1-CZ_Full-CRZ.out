/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 18:56:06 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 18:59:42 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 19:06:08 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 19:12:37 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 19:19:02 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 19:25:28 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1933.4272348880768 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 19:31:52 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 19:38:14 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 19:44:36 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 19:50:58 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 19:57:20 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1911.5870740413666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 20:03:46 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 20:10:09 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 20:16:32 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 20:22:55 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 20:29:18 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1917.6399161815643 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 20:35:42 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 20:42:05 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 20:48:28 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 20:54:50 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 21:01:13 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1915.3007447719574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 21:07:36 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 21:13:59 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 21:20:26 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 21:26:49 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 21:33:12 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1919.0163323879242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 21:39:38 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 21:46:01 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 21:52:23 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 21:58:46 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 22:05:11 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1919.1018149852753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 22:11:36 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 22:17:59 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 22:24:23 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 22:30:44 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 22:37:08 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1916.9933378696442 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 22:43:32 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 22:49:54 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 22:56:19 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 23:02:43 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 23:09:09 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1921.6672859191895 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 23:15:32 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:21:56 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Wed Mar 27 23:28:20 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 23:34:44 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Wed Mar 27 23:41:09 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1919.4715485572815 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 23:47:33 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:53:56 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 00:00:19 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:06:44 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 00:13:06 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1916.2079825401306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 00:19:28 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:25:52 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 00:32:18 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:38:43 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 00:45:08 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1922.6097309589386 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 00:51:31 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:57:54 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 01:04:17 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 01:10:41 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 01:17:04 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1917.223423242569 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 01:23:28 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:29:54 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 01:36:17 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 01:42:40 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 01:49:03 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1918.7720746994019 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 01:55:28 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:01:51 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 02:08:18 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:14:41 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 02:21:05 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1920.5763039588928 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 02:27:29 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:33:56 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 02:40:19 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:46:44 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 02:53:07 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1922.065890789032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:59:32 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:05:56 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 03:12:21 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:18:45 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 03:25:08 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1922.0702414512634 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:31:33 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:37:57 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 03:44:20 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:50:48 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 03:57:12 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1921.687555551529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:03:38 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:10:01 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 04:16:30 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:22:55 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 04:29:17 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1926.8666331768036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:35:41 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:42:04 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 04:48:28 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:54:52 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 05:01:15 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1918.3343040943146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:07:44 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:14:07 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 05:20:33 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:26:59 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 05:33:23 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1926.9817328453064 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:39:47 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:46:12 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 05:52:36 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:58:59 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 06:05:26 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1922.8442618846893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:11:49 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 06:18:12 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 06:24:36 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 06:31:00 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 06:37:22 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1916.154512643814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:43:45 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 06:50:09 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 06:56:31 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 07:02:53 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 07:09:15 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1913.4208633899689 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 07:15:38 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 07:22:00 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 07:28:26 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 07:34:51 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 07:41:16 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1923.1196258068085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 07:47:42 2024]  Iteration number: 0 with current cost as 0.15958078336700227 and parameters 
[-2.49771543  2.23743467 -2.1242796  -0.11653101  0.55388708 -2.77010895
  3.06858502  2.18960147  1.18552002 -1.06648308  0.60271514  1.14432445
  1.310299   -1.87354676  0.72965082  2.88578421 -0.54534335 -0.47522481
 -2.0265424   0.72897371  1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309771  3.13337155  2.54856958 -0.67550785 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 07:54:07 2024]  Iteration number: 0 with current cost as 0.16883560063022118 and parameters 
[-2.45188492  2.23743462 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.06858498  2.18960143  1.18551998 -1.06648312  0.60271512  1.14432445
  1.31029897 -1.87354678  0.72965078  2.88578417 -0.54534337 -0.47522487
 -2.02654242  0.72897368  1.60512662  2.83077105 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337153  2.54856958 -0.67550789 -2.69002204]. 
Working on 0.6 fold... 
[Thu Mar 28 08:00:29 2024]  Iteration number: 0 with current cost as 0.1642196386099384 and parameters 
[-2.51311811  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960147  1.18552    -1.06648312  0.60271514  1.14432445
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 08:06:56 2024]  Iteration number: 0 with current cost as 0.12811785346541532 and parameters 
[-2.52172059  2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960147  1.18552    -1.0664831   0.60271512  1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002204]. 
Working on 1.0 fold... 
[Thu Mar 28 08:13:19 2024]  Iteration number: 0 with current cost as 0.14843409407386665 and parameters 
[-2.45687503  2.23743461 -2.12427962 -0.11653105  0.55388706 -2.77010899
  3.068585    2.18960143  1.18552001 -1.06648313  0.60271512  1.14432445
  1.31029896 -1.8735468   0.72965078  2.88578417 -0.54534337 -0.47522483
 -2.02654243  0.72897367  1.60512662  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309776  3.13337153  2.5485696  -0.67550787 -2.69002204]. 
Training complete taking 1919.6776044368744 seconds. 
Discarding model... 

Training complete taking 48002.818633556366 total seconds. 
Now scoring model... 
Scoring complete taking 2.4255685806274414 seconds. 
Saved predicted values as A1-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.379615249588902,), 'R2_train': 0.535953385200931, 'MAE_train': 1.2918253801174622, 'MSE_test': 6.433428334911213, 'R2_test': 0.3586472944339115, 'MAE_test': 1.9329044157626725}. 
Saved model results as A1-A1-CZ_Full-CRZ_results.json. 
