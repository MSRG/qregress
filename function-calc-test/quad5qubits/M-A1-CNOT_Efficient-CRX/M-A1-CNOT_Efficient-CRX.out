/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:07:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:09:12 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 16:11:53 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:14:34 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 16:17:16 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 16:20:11 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 802.0936408042908 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:22:35 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 16:25:14 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:27:52 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 16:30:34 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 16:33:31 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 800.4307076931 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:35:57 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 16:38:39 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:41:22 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 16:44:03 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 16:47:02 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 809.7578310966492 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:49:27 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 16:52:10 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:54:48 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 16:57:31 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 17:00:30 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 810.2920157909393 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:02:58 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 17:05:41 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:08:20 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 17:11:04 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 17:13:58 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 806.1014468669891 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:16:21 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 17:19:02 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:21:47 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 17:24:27 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 17:27:23 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 805.4708132743835 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:29:47 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 17:32:27 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:35:08 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 17:37:49 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 17:40:46 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 804.0238819122314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:43:13 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 17:45:53 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:48:33 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 17:51:11 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 17:54:09 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 802.0453612804413 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:56:33 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 17:59:16 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:01:58 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 18:04:42 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 18:07:38 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 809.2777063846588 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:10:01 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 18:12:43 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:15:23 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 18:18:02 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 18:20:58 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 800.0837938785553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:23:24 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 18:26:13 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:28:56 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 18:31:47 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 18:34:44 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 825.1785998344421 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:37:12 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 18:39:58 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:42:39 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 18:45:25 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 18:48:27 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 823.0057933330536 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:50:57 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 18:53:42 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:56:18 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 18:58:56 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 19:01:57 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 809.7824070453644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:04:21 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 19:07:06 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:09:52 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 19:12:34 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 19:15:32 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 814.8828423023224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:17:58 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 19:20:39 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:23:22 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 19:26:03 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 19:29:07 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 816.7955429553986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:31:38 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 19:34:23 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:37:08 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 19:39:49 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 19:42:49 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 822.4330277442932 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:45:13 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 19:47:57 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:50:40 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 19:53:21 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 19:56:22 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 812.0371346473694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:58:50 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 20:01:42 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:04:31 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 20:07:16 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 20:10:16 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 834.9658572673798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:12:44 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 20:15:28 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:18:18 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 20:21:05 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 20:24:01 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 823.5729713439941 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:26:28 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 20:29:12 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:31:57 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 20:34:44 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 20:37:40 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 818.7393906116486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:40:02 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 20:42:48 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:45:38 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 20:48:18 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 20:51:16 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 816.4129226207733 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:53:40 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 20:56:26 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:59:17 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 21:01:59 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 21:05:02 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 825.8592147827148 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:07:29 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 21:10:15 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:13:05 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 21:15:45 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 21:18:44 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 822.81862616539 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:21:15 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 21:24:01 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:26:53 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 21:29:40 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 21:32:41 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 836.9744215011597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:35:13 2024]  Iteration number: 0 with current cost as 0.18802849598935342 and parameters 
[-3.94895968  2.2374349  -2.1242799  -0.11653129  0.55388681 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648335  0.6027151   1.14432445
  1.31029872 -1.87354627]. 
Working on 0.4 fold... 
[Mon Apr  1 21:37:59 2024]  Iteration number: 0 with current cost as 0.18521975091590404 and parameters 
[-3.94485431  2.23743413 -2.12428065 -0.11653153  0.55388657 -2.77010948
  3.06858498  2.18960044  1.18551948 -1.06648409  0.6027146   1.14432445
  1.31029798 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:40:46 2024]  Iteration number: 0 with current cost as 0.17727545918288576 and parameters 
[-3.87572681  2.23743482 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648327  0.6027151   1.14432445
  1.3102988  -1.87354643]. 
Working on 0.8 fold... 
[Mon Apr  1 21:43:29 2024]  Iteration number: 0 with current cost as 0.13847785953952244 and parameters 
[-3.79725604  2.23743503 -2.12427964 -0.11653103  0.55388708 -2.77010878
  3.06858479  2.18960126  1.18551998 -1.06648308  0.60271491  1.14432445
  1.3102986  -1.87354641]. 
Working on 1.0 fold... 
[Mon Apr  1 21:46:30 2024]  Iteration number: 0 with current cost as 0.15198260350839526 and parameters 
[-3.80861471  2.23743292 -2.12428049 -0.11653188  0.55388622 -2.77010897
  3.06858584  2.1896006   1.18551913 -1.06648394  0.60271596  1.14432445
  1.31029813 -1.87354594]. 
Training complete taking 827.2930052280426 seconds. 
Discarding model... 

Training complete taking 20380.329698085785 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.8199951648712158 seconds. 
Saved predicted values as M-A1-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.736446665914658,), 'R2_train': 0.4869577455877927, 'MAE_train': 1.3235909358210503, 'MSE_test': 6.288999676694811, 'R2_test': 0.3730454824428945, 'MAE_test': 1.880291384871472}. 
Saved model results as M-A1-CNOT_Efficient-CRX_results.json. 
