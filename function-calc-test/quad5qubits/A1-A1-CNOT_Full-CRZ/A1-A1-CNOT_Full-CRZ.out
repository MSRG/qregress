/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:05:11 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:12:25 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:19:35 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:26:26 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:32:51 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2051.09445810318 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:39:20 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:46:31 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:53:34 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:59:42 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:05:55 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1970.1190700531006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:12:04 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:18:56 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:25:55 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:32:05 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:38:21 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1947.6766593456268 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:44:28 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:51:22 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:58:13 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:04:23 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:10:36 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1939.3322355747223 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:16:49 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:23:42 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:30:35 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:36:51 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:43:05 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1948.3289632797241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:49:21 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:56:14 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:03:08 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:09:23 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:15:32 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1944.2127919197083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:21:42 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:28:29 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:35:22 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:41:30 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:47:39 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1924.6961867809296 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:53:49 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:00:43 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:07:41 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:13:47 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:19:59 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1944.8707404136658 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:26:09 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:33:03 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:39:57 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:46:12 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:52:25 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1944.079018354416 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:58:36 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:05:30 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:12:25 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:18:41 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:24:50 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1943.2142136096954 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:30:58 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:38:05 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:45:05 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:51:19 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:57:26 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1960.0092453956604 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:03:39 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:10:24 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:17:15 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:23:31 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:29:44 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1933.5430235862732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:35:55 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:42:47 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:49:36 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:55:43 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:01:54 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1931.9200284481049 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:08:02 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:14:57 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:21:45 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:28:05 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:34:19 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1947.5737335681915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 23:40:34 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:47:26 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:54:22 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:00:30 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:06:43 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1941.3083367347717 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 00:12:55 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:19:44 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:26:35 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:32:46 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:38:56 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1934.6026215553284 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 00:45:07 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:51:59 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:58:51 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:05:01 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:11:09 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1933.586937904358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 01:17:23 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:24:13 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:31:04 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:37:16 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:43:29 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1939.3120002746582 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 01:49:42 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:56:34 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:03:30 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:09:42 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:15:58 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1954.1279971599579 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:22:16 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:29:07 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:36:02 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:42:13 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:48:38 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1955.204710483551 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 02:54:50 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:01:39 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:08:30 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:14:47 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:21:00 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1940.1370213031769 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 03:27:09 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:34:09 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:41:07 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:47:25 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:53:46 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1972.9035351276398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:00:06 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:07:10 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:13:59 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:20:16 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:26:30 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1955.0445530414581 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 04:32:40 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:39:35 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:46:29 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:52:42 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:58:53 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1949.2539865970612 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 05:05:10 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:12:05 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:19:03 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:25:18 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:31:30 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1955.2255177497864 seconds. 
Discarding model... 

Training complete taking 48761.37824463844 total seconds. 
Now scoring model... 
Scoring complete taking 2.189767837524414 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (4.3918062294910545,), 'R2_train': 0.3969719441002373, 'MAE_train': 1.364581866894287, 'MSE_test': 3.592703193534579, 'R2_test': 0.6418410537091763, 'MAE_test': 1.5701882391478335}. 
Saved model results as A1-A1-CNOT_Full-CRZ_results.json. 
