/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:16 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:44:34 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 15:52:12 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 15:59:35 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:07:24 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 16:15:27 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 16:16:39 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 16:24:47 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2448.391718149185 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:25:23 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:32:58 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:40:22 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 16:48:15 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 16:56:19 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 16:57:31 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 17:05:43 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2456.8991961479187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:06:20 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:13:53 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:21:31 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 17:29:23 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 17:37:28 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 17:38:44 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 17:47:20 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2496.7377943992615 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:47:56 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:55:26 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:03:01 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:10:57 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 18:19:17 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 18:20:37 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 18:28:42 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2481.836941242218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:29:19 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:36:48 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:44:35 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 18:52:22 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 19:00:19 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 19:01:31 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 19:09:38 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2455.4852924346924 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:10:13 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:17:46 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:25:08 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 19:32:58 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 19:40:54 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 19:42:06 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 19:50:28 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2450.6358387470245 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:51:04 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:58:33 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:06:19 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:14:09 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 20:22:07 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 20:23:19 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 20:31:27 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2459.0046005249023 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:32:03 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:39:34 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:46:57 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 20:54:47 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 21:02:54 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 21:04:05 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 21:12:26 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2461.772670030594 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:13:07 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:21:04 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:28:34 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 21:36:23 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 21:44:28 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 21:45:40 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 21:53:57 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2488.3633766174316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:54:36 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:02:07 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:09:32 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:17:42 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 22:25:42 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 22:26:54 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 22:35:01 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2463.290855407715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:35:37 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:43:08 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:50:31 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 22:58:32 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 23:06:29 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 23:07:42 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 23:16:19 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2482.0415437221527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:17:01 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:24:57 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:32:20 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Mon Apr  1 23:40:11 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 23:48:18 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Mon Apr  1 23:49:29 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Mon Apr  1 23:57:46 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2483.0238909721375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:58:21 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:06:43 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:14:11 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 00:22:19 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 00:30:21 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 00:31:33 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 00:39:43 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2516.9004986286163 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:40:18 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:47:50 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:55:14 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:03:04 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 01:11:03 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 01:12:15 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 01:20:22 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2440.4469299316406 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 01:20:58 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:28:32 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 01:35:56 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 01:43:49 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 01:51:49 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 01:53:02 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 02:01:27 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2465.588377714157 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 02:02:04 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:09:49 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 02:17:14 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 02:25:11 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 02:33:10 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 02:34:22 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 02:42:40 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2472.3061051368713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:43:16 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:50:49 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 02:58:12 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 03:06:03 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 03:14:13 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 03:15:25 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 03:23:33 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2453.5900464057922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 03:24:10 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:31:41 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 03:39:09 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 03:46:57 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 03:55:04 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 03:56:16 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 04:04:25 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2450.3579845428467 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 04:05:02 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:12:34 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:19:57 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 04:27:46 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 04:35:44 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 04:36:56 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 04:45:02 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2437.042703151703 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 04:45:37 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:53:10 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 05:00:33 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 05:09:15 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 05:17:14 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 05:18:26 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 05:26:34 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2492.1280059814453 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 05:27:11 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 05:34:43 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 05:42:08 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 05:49:57 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 05:57:57 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 05:59:09 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 06:07:40 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2467.3247311115265 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 06:08:17 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 06:15:49 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 06:23:37 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 06:31:38 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 06:39:39 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 06:40:51 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 06:48:57 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2477.588641643524 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 06:49:34 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 06:57:07 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 07:04:38 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 07:12:28 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 07:20:26 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 07:21:38 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 07:29:46 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2448.6266565322876 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 07:30:23 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 07:37:54 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 07:45:18 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 07:53:07 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 08:01:02 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 08:02:16 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 08:10:23 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2435.259363412857 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 08:10:58 2024]  Iteration number: 0 with current cost as 0.182846188009857 and parameters 
[-2.72395064  2.29896799 -2.13014837 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62706382  1.14432445
  1.09858169 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 08:18:32 2024]  Iteration number: 0 with current cost as 0.17533585986434974 and parameters 
[-2.74528706  2.28997376 -2.12842429 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.62250746  1.14432445
  1.12362177 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 08:25:54 2024]  Iteration number: 0 with current cost as 0.1717296515393517 and parameters 
[-2.68779486  2.29233281 -2.12983523 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.62357824  1.14432445
  1.05706142 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136105]. 
Working on 0.8 fold... 
[Tue Apr  2 08:33:46 2024]  Iteration number: 0 with current cost as 0.13815115352549878 and parameters 
[-2.66463506  2.30828086 -2.12745491 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.63159014  1.14432445
  1.02609541 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 08:41:44 2024]  Iteration number: 50 with current cost as 0.1218896924035016 and parameters 
[-1.95850168  0.08429034 -3.00780495 -0.11653256  0.55388536 -2.7701067
  3.06858363  2.18959872  1.18551898 -1.0664812   3.05830805  1.14432162
  1.7878528  -1.87354795  0.72965105  2.88578319 -0.54534551 -0.47522616
 -2.02654416  0.72897235  1.60512575  2.83077238 -1.26456801 -0.25135966]. 
Working on 1.0 fold... 
[Tue Apr  2 08:42:56 2024]  Iteration number: 0 with current cost as 0.14590117748841946 and parameters 
[-2.71264296  2.29555062 -2.12426269 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.62319553  1.14432445
  1.08267917 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
[Tue Apr  2 08:51:03 2024]  Iteration number: 50 with current cost as 0.13361433983037302 and parameters 
[-1.88230326  0.19761984 -3.04602987 -0.11652824  0.55388195 -2.77010597
  3.06858336  2.18959781  1.18551755 -1.06648107  2.80283092  1.144323
  1.9213752  -1.87354814  0.72964933  2.8857835  -0.54534319 -0.47522542
 -2.02654338  0.72897308  1.60512415  2.8307704  -1.26456939 -0.25136117]. 
Training complete taking 2441.244916677475 seconds. 
Discarding model... 

Training complete taking 61625.89025950432 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.0773844718933105 seconds. 
Saved predicted values as M-A1-CNOT_Full-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.2759392387766533,), 'R2_train': 0.5501888819365869, 'MAE_train': 1.2945465714395865, 'MSE_test': 4.881692947188559, 'R2_test': 0.5133408166789686, 'MAE_test': 1.7109343688212466}. 
Saved model results as M-A1-CNOT_Full-Pauli-CRZ_results.json. 
