/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 20:44:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 20:50:08 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 20:57:35 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Wed Mar 27 21:04:17 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 21:11:43 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 21:17:37 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2184.930195569992 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 21:26:31 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 21:33:56 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Wed Mar 27 21:40:38 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 21:48:03 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 21:53:59 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2181.8644993305206 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 22:02:53 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 22:10:20 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Wed Mar 27 22:17:02 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 22:24:27 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 22:30:23 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2185.169312477112 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 22:39:19 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 22:46:45 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Wed Mar 27 22:53:25 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 23:00:51 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 23:06:48 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2184.1223318576813 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 23:15:45 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:23:10 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Wed Mar 27 23:29:51 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Wed Mar 27 23:37:21 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 23:43:20 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2192.142884016037 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 23:52:16 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:59:43 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 00:06:27 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:13:57 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:19:56 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2197.2163560390472 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 00:28:51 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:36:19 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 00:43:01 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 00:50:29 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:56:28 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2190.6457760334015 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 01:05:25 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:12:53 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 01:19:35 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 01:27:01 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 01:32:59 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2191.3961520195007 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 01:41:54 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:49:19 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 01:56:01 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:03:27 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:09:26 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2186.6489822864532 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 02:18:21 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:25:47 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 02:32:29 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 02:39:55 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:45:53 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2186.697902202606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:54:47 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:02:14 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 03:08:56 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:16:22 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:22:19 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2185.860310792923 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:31:13 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:38:38 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 03:45:18 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 03:52:46 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:58:41 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2183.2907145023346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:07:39 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:15:09 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 04:21:50 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 04:29:16 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 04:35:12 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2192.0728948116302 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:44:08 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:51:36 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 04:58:20 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:05:46 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:11:43 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2188.869464635849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:20:38 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:28:06 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 05:34:48 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 05:42:15 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:48:11 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2187.926019668579 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:57:07 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 06:04:32 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 06:11:16 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 06:18:41 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 06:24:41 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2189.7214770317078 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:33:37 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 06:41:05 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 06:47:47 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 06:55:12 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 07:01:10 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2189.018814086914 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 07:10:04 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 07:17:29 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 07:24:10 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 07:31:37 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 07:37:36 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2185.736563682556 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 07:46:31 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 07:53:59 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 08:00:41 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 08:08:06 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 08:14:05 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2189.5698380470276 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 08:23:00 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 08:30:27 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 08:37:10 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 08:44:34 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 08:50:31 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2187.013158798218 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 08:59:26 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 09:06:53 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 09:13:34 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 09:20:59 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 09:26:55 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2182.0863423347473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 09:35:48 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 09:43:13 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 09:49:54 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 09:57:19 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 10:03:17 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2185.578249692917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 10:12:18 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 10:19:44 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 10:26:26 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 10:33:53 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 10:39:49 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2190.5209953784943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 10:48:45 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 10:56:12 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 11:02:52 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 11:10:21 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 11:16:18 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2188.760149002075 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 11:25:12 2024]  Iteration number: 0 with current cost as 0.1673453919852575 and parameters 
[-3.83548525  2.23743455 -2.12427955 -0.11653103  0.55388708 -2.77010915
  3.0685848   2.18960145  1.18551998 -1.06648317  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965071  2.88578419 -0.54534353 -0.47522485
 -2.02654249  0.7289737   1.60512655  2.83077089 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550778 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 11:32:39 2024]  Iteration number: 0 with current cost as 0.18150727317582355 and parameters 
[-3.85734405  2.23743435 -2.12427935 -0.11653117  0.55388693 -2.77010926
  3.06858469  2.18960131  1.18551998 -1.06648308  0.6027151   1.14432431
  1.31029884 -1.8735468   0.72965051  2.88578405 -0.54534349 -0.47522456
 -2.02654255  0.7289737   1.60512649  2.83077093 -1.26456724 -0.25136105
 -2.39279232 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002216]. 
Working on 0.6 fold... 
[Thu Mar 28 11:39:21 2024]  Iteration number: 0 with current cost as 0.16903398742444908 and parameters 
[-3.80294258  2.23743456 -2.12427956 -0.11653103  0.55388708 -2.77010912
  3.06858491  2.18960145  1.18552006 -1.06648316  0.60271517  1.14432445
  1.31029906 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522471
 -2.02654248  0.7289737   1.60512664  2.830771   -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Mar 28 11:46:47 2024]  Iteration number: 0 with current cost as 0.18023726350836233 and parameters 
[-2.64172255  2.23743408 -2.12427964 -0.1165313   0.5538868  -2.77010981
  3.06858443  2.18960117  1.18551998 -1.06648364  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965025  2.88578391 -0.54534363 -0.47522485
 -2.02654296  0.7289737   1.60512636  2.83077051 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 11:52:42 2024]  Iteration number: 0 with current cost as 0.19989290844624397 and parameters 
[-2.59250354  2.23743464 -2.12427956 -0.1165311   0.55388694 -2.77010912
  3.06858484  2.18960138  1.18552006 -1.06648323  0.6027151   1.14432445
  1.31029884 -1.87354687  0.72965066  2.88578405 -0.54534349 -0.47522485
 -2.02654255  0.72897362  1.60512657  2.83077093 -1.26456717 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2183.211142063141 seconds. 
Discarding model... 

Training complete taking 54690.071103811264 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.4313595294952393 seconds. 
Saved predicted values as M-A2-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (4.648558140561943,), 'R2_train': 0.36171797393599137, 'MAE_train': 1.549127230825972, 'MSE_test': 12.610030375182747, 'R2_test': -0.2571022287614597, 'MAE_test': 2.5033097954997436}. 
Saved model results as M-A2-CZ_Full-CRZ_results.json. 
