/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 23:33:17 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 23:33:53 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Wed Mar 27 23:40:12 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Wed Mar 27 23:47:09 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Wed Mar 27 23:56:31 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:01:01 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1951.4146223068237 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 00:06:24 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:12:39 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:19:29 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 00:28:52 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 00:33:21 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1938.4783673286438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 00:38:41 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 00:44:54 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 00:51:44 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 01:01:01 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 01:05:33 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1933.507309436798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 01:10:55 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 01:17:09 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 01:23:57 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 01:33:15 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 01:37:46 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1934.3682894706726 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 01:43:09 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 01:49:23 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 01:56:14 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:05:32 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 02:10:04 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1938.7816889286041 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:15:29 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:21:45 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 02:28:41 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 02:38:05 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 02:42:37 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1951.5599229335785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 02:48:00 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 02:54:13 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:01:03 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:10:25 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:14:58 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1942.6862289905548 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:20:23 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:26:40 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:33:33 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:42:54 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:47:25 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1946.3660542964935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 03:52:49 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:59:07 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:06:01 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:15:26 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:19:58 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1960.1699435710907 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:25:29 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:31:45 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:38:36 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:47:58 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:52:30 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1948.0321414470673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:57:57 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:04:16 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:11:09 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:20:32 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:25:04 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1950.1197888851166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:30:28 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:36:46 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:43:39 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:53:01 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:57:36 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1952.3484032154083 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:03:00 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:09:18 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:16:08 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:25:33 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:30:05 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1950.659381389618 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 06:35:31 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:41:49 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:48:42 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:58:03 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:02:36 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1950.4074969291687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 07:08:01 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 07:14:18 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 07:21:12 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:30:33 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:35:06 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1951.6257424354553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 07:40:33 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 07:46:53 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 07:53:48 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:03:10 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:07:39 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1952.2549080848694 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 08:13:05 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:19:23 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 08:26:24 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:35:48 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:40:19 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1959.5418779850006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 08:45:45 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:52:03 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 08:58:53 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 09:08:18 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 09:12:51 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1952.5854992866516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 09:18:17 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 09:24:37 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 09:31:31 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 09:40:57 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 09:45:29 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1955.5782284736633 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 09:50:52 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 09:57:10 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 10:04:02 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 10:13:25 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 10:17:58 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1951.1757197380066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 10:23:24 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 10:29:50 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 10:36:39 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 10:45:59 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 10:50:31 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1952.9358637332916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 10:55:57 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 11:02:16 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 11:09:08 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 11:18:30 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 11:23:02 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1949.582593202591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 11:28:26 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 11:34:45 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 11:41:37 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 11:51:01 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 11:55:33 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1953.049190044403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 12:00:59 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 12:07:18 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 12:14:12 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 12:23:31 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 12:28:08 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1954.8317835330963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 12:33:35 2024]  Iteration number: 0 with current cost as 0.20486052260714682 and parameters 
[-2.5753885   2.13493641 -1.72790019 -0.1165311   0.55388704 -2.77010905
  3.06858494  2.18960137  1.18551998 -1.06648316  0.99368322  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Mar 28 12:39:52 2024]  Iteration number: 0 with current cost as 0.2040067588362861 and parameters 
[-2.78916288  2.15973895 -1.65116054 -0.11653099  0.55388712 -2.77010897
  3.06858498  2.18960145  1.18552002 -1.06648308  1.07896028  1.14432449
  1.31029902 -1.87354672  0.7296508   2.88578423 -0.54534323 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Mar 28 12:46:45 2024]  Iteration number: 0 with current cost as 0.18735919454952138 and parameters 
[-2.39357364  2.09275043 -0.24434571 -0.11653103  0.55388708 -2.77010929
  3.06858482  2.18960145  1.18551998 -1.06648324  1.75485635  1.14432445
  1.31029899 -1.87354664  0.72965065  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Mar 28 12:56:11 2024]  Iteration number: 0 with current cost as 0.14033635825371257 and parameters 
[-2.93533416  2.37691628 -0.48934263 -0.11653134  0.55388676 -2.77010929
  3.06858482  2.18960129  1.18551998 -1.0664834   1.21654081  1.14432429
  1.31029867 -1.8735468   0.72965049  2.88578403 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Mar 28 13:00:45 2024]  Iteration number: 0 with current cost as 0.15753021763460018 and parameters 
[-3.01101332  2.24527754 -1.64768661 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960145  1.18551998 -1.06648312  0.84687525  1.14432437
  1.31029891 -1.8735468   0.72965076  2.88578415 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 1958.692497253418 seconds. 
Discarding model... 

Training complete taking 48740.75465130806 total seconds. 
Now scoring model... 
Scoring complete taking 1.0206007957458496 seconds. 
Saved predicted values as A2-A2-CNOT_Modified-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.5845437072340767,), 'R2_train': 0.5078151653080585, 'MAE_train': 1.2978996671771985, 'MSE_test': 3.572787009338186, 'R2_test': 0.643826511221712, 'MAE_test': 1.5360950321944746}. 
Saved model results as A2-A2-CNOT_Modified-Pauli-CRX_results.json. 
