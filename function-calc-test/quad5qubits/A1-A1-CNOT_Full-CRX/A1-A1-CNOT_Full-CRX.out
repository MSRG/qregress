/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:05:16 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:12:26 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:19:28 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:25:59 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:32:28 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2024.8708851337433 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:38:55 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:46:00 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:53:05 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:59:35 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:06:03 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2023.5421216487885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:12:36 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:19:41 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:26:56 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:33:22 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:39:45 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2015.7912373542786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:46:24 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:53:43 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:01:01 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:07:33 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:13:57 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2049.3665084838867 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:20:21 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:27:37 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:34:47 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:41:14 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:47:53 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2043.3946900367737 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:54:24 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:01:30 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:08:46 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:15:18 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:21:48 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2027.6198320388794 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:28:13 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:35:29 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:42:45 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:49:20 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:55:58 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2056.7664711475372 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:02:34 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:09:49 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:17:11 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:23:58 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:30:25 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2068.333434343338 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:36:56 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:44:17 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:51:17 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:57:43 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:04:02 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2006.748501777649 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:10:18 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:17:11 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:24:09 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:30:28 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:36:42 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1958.2299439907074 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:42:59 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:49:51 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:56:58 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:03:15 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:09:26 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1961.9876837730408 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:15:36 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:22:49 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:30:11 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:36:47 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:43:29 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2050.976791381836 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:50:01 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:57:15 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:04:34 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:11:08 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:17:39 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2047.5053033828735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:24:06 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:31:14 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:38:30 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:44:53 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:51:40 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2046.8874530792236 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 23:58:16 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:05:32 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:12:50 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:19:12 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:25:51 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2045.033792257309 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 00:32:22 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:39:32 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:46:42 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:53:04 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:59:30 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2023.2728650569916 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 01:06:01 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:13:11 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:20:31 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:27:06 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:33:43 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2050.6019644737244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 01:40:10 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:47:22 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:54:39 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:01:10 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:07:40 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2034.6967883110046 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 02:14:10 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:21:22 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:28:33 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:35:02 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:41:35 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2043.8541355133057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:48:08 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:55:26 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:02:32 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:08:55 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:15:16 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2005.2823305130005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 03:21:26 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:28:26 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:35:24 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:41:58 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:48:23 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1996.0745964050293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 03:54:47 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:01:54 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:08:57 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:15:27 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:21:53 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2013.370090007782 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:28:22 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:35:33 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:42:41 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:49:11 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:55:47 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2035.0347197055817 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 05:02:17 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:09:24 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:16:39 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:23:05 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:29:46 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2033.1323058605194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 05:36:17 2024]  Iteration number: 0 with current cost as 0.2337629153172528 and parameters 
[-2.36042348  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.770109
  3.06858495  2.18960145  1.18552005 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522482
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:43:28 2024]  Iteration number: 0 with current cost as 0.22056725555933132 and parameters 
[-2.34333037  2.23743467 -2.1242796  -0.11653099  0.55388711 -2.77010901
  3.06858495  2.18960149  1.18552005 -1.06648308  0.6027151   1.14432449
  1.31029902 -1.87354677  0.72965084  2.88578419 -0.54534332 -0.47522478
 -2.02654237  0.72897373  1.60512667  2.83077104 -1.26456706 -0.25136101
 -2.39279215 -2.27309774  3.13337158  2.54856962 -0.67550784 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:50:29 2024]  Iteration number: 0 with current cost as 0.2156427084414567 and parameters 
[-2.4207786   2.23743465 -2.1242796  -0.11653101  0.55388708 -2.77010901
  3.06858495  2.18960145  1.18551998 -1.0664831   0.60271509  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534337 -0.47522482
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136106
 -2.3927922  -2.27309774  3.13337158  2.54856958 -0.67550786 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:57:12 2024]  Iteration number: 0 with current cost as 0.15928698018958898 and parameters 
[-2.48335819  2.23743465 -2.12427962 -0.11653101  0.5538871  -2.77010899
  3.06858496  2.18960147  1.18552    -1.06648308  0.60271508  1.14432447
  1.31029899 -1.87354678  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.54856958 -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:03:56 2024]  Iteration number: 0 with current cost as 0.16863640973298766 and parameters 
[-2.43056607  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010902
  3.06858492  2.18960145  1.18552001 -1.06648311  0.60271508  1.14432445
  1.31029894 -1.87354678  0.72965082  2.88578419 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2054.172010421753 seconds. 
Discarding model... 

Training complete taking 50716.54710698128 total seconds. 
Now scoring model... 
Scoring complete taking 2.251605749130249 seconds. 
Saved predicted values as A1-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (4.3918062294910545,), 'R2_train': 0.3969719441002373, 'MAE_train': 1.364581866894287, 'MSE_test': 3.592703193534579, 'R2_test': 0.6418410537091763, 'MAE_test': 1.5701882391478335}. 
Saved model results as A1-A1-CNOT_Full-CRX_results.json. 
