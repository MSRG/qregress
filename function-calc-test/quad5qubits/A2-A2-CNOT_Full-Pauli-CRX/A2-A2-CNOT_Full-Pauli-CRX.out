/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 21:10:41 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 21:11:04 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Wed Mar 27 21:22:13 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Wed Mar 27 21:30:57 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Wed Mar 27 21:40:21 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Wed Mar 27 21:50:28 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3030.0811419487 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 22:01:33 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Wed Mar 27 22:12:37 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Wed Mar 27 22:21:20 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Wed Mar 27 22:30:46 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Wed Mar 27 22:40:57 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3026.3864409923553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 22:51:59 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Wed Mar 27 23:03:02 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Wed Mar 27 23:11:41 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Wed Mar 27 23:21:07 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Wed Mar 27 23:31:19 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3030.374397277832 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 23:42:31 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Wed Mar 27 23:53:38 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 00:02:19 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 00:11:43 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 00:21:55 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3029.897376060486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 00:33:00 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 00:44:09 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 00:52:54 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 01:02:22 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 01:12:32 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3037.3068075180054 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 01:23:37 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 01:34:47 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 01:43:31 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 01:53:02 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 02:03:15 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3046.8710572719574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 02:14:24 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 02:25:34 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 02:34:18 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 02:43:45 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 02:54:00 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3045.4438581466675 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 03:05:10 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 03:16:17 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 03:24:56 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 03:34:25 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 03:44:38 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3038.6868312358856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 03:55:48 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 04:06:57 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 04:15:37 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 04:25:09 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 04:35:25 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3043.128235578537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:46:31 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 04:57:40 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 05:06:22 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 05:15:52 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 05:26:01 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3037.2801496982574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:37:09 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 05:48:19 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 05:57:05 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 06:06:33 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 06:16:47 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3048.328388929367 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:27:57 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 06:39:10 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 06:47:56 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 06:57:27 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 07:07:39 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3050.6852185726166 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 07:18:48 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 07:29:53 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 07:38:36 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 07:48:06 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 07:58:21 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3042.5283813476562 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 08:09:30 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 08:20:46 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 08:29:26 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 08:38:54 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 08:49:08 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3045.5144090652466 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 09:00:16 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 09:11:23 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 09:20:05 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 09:29:31 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 09:39:41 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3032.8822927474976 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 09:50:48 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 10:01:58 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 10:10:44 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 10:20:19 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 10:30:27 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3047.3115417957306 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 10:41:36 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 10:52:45 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 11:01:31 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 11:10:58 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 11:21:08 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3035.351238965988 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 11:32:12 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 11:43:24 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 11:52:07 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 12:01:37 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 12:11:49 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3049.503293275833 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 12:23:01 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 12:34:07 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 12:42:48 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 12:52:21 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 13:02:33 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3041.0198674201965 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 13:13:42 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 13:24:46 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 13:33:24 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 13:42:54 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 13:53:05 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3029.462605714798 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 14:04:11 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 14:15:21 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 14:24:09 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 14:33:35 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 14:43:47 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3043.910369873047 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 14:54:55 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 15:06:02 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 15:14:47 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 15:24:12 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 15:34:18 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3033.22940158844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 15:45:28 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 15:56:37 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 16:05:17 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 16:14:47 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 16:25:04 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3043.1949775218964 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 16:36:11 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 16:47:21 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 16:56:06 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 17:05:35 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 17:15:47 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3041.1900107860565 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 17:26:52 2024]  Iteration number: 0 with current cost as 0.2242675787058344 and parameters 
[-2.49843073  2.4012877  -1.97018209 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.10748472  1.14432446
  0.68362649 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Thu Mar 28 17:37:55 2024]  Iteration number: 0 with current cost as 0.21561583178908847 and parameters 
[-2.54641572  2.38383836 -1.98822393 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.16469238  1.14432445
  0.75775817 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Thu Mar 28 17:46:39 2024]  Iteration number: 0 with current cost as 0.20735315607030322 and parameters 
[-2.46974372  2.40864217 -1.9562361  -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.06648309  0.07364044  1.14432445
  0.63984497 -1.87354681  0.72965081  2.88578419 -0.54534336 -0.47522484
 -2.0265424   0.72897369  1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.8 fold... 
[Thu Mar 28 17:56:07 2024]  Iteration number: 0 with current cost as 0.15324623969263895 and parameters 
[-2.45842263  2.42365937 -1.95115363 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.07005879  1.14432445
  0.62188573 -1.8735468   0.72965081  2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 1.0 fold... 
[Thu Mar 28 18:06:21 2024]  Iteration number: 0 with current cost as 0.1633805423043458 and parameters 
[-2.53581978  2.39826424 -1.98127135 -0.11653103  0.55388707 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.0664831   0.1608374   1.14432445
  0.73988816 -1.8735468   0.7296508   2.88578419 -0.54534336 -0.47522485
 -2.02654241  0.72897369  1.60512663  2.83077107 -1.2645671  -0.25136105]. 
Training complete taking 3036.72612285614 seconds. 
Discarding model... 

Training complete taking 75986.29553055763 total seconds. 
Now scoring model... 
Scoring complete taking 1.1152491569519043 seconds. 
Saved predicted values as A2-A2-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.710455081712938,), 'R2_train': 0.6278341135468544, 'MAE_train': 1.109219515382236, 'MSE_test': 2.840286044621754, 'R2_test': 0.7168500145692684, 'MAE_test': 1.3237470576315487}. 
Saved model results as A2-A2-CNOT_Full-Pauli-CRX_results.json. 
