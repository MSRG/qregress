/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:02:07 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 16:06:11 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 16:10:24 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 16:14:23 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 16:18:40 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1267.7844548225403 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:23:15 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 16:26:59 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 16:30:51 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 16:34:53 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 16:39:14 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1235.9588878154755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:43:51 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 16:47:52 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 16:52:04 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 16:56:11 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 17:00:30 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1310.3077836036682 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:05:41 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 17:09:24 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 17:13:19 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 17:17:20 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 17:21:38 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1240.2412822246552 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:26:21 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 17:30:08 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 17:34:05 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 17:38:06 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 17:42:23 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1238.4568419456482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:47:01 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 17:50:44 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 17:54:51 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 17:58:52 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 18:03:20 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1257.2216436862946 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:07:57 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 18:11:40 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 18:15:32 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 18:19:30 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 18:23:53 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1231.1366057395935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:28:28 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 18:32:12 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 18:36:07 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 18:40:05 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 18:44:22 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1225.7366905212402 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:48:54 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 18:52:37 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 18:56:30 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 19:00:30 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 19:04:50 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1230.3540456295013 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:09:24 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 19:13:08 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 19:17:00 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 19:20:58 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 19:25:17 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1228.4695768356323 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:29:53 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 19:33:43 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 19:37:45 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 19:41:46 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 19:46:02 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1254.6841113567352 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:50:49 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 19:54:29 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 19:58:53 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 20:02:53 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 20:07:25 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1272.4366693496704 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:12:00 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 20:15:52 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 20:19:47 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 20:23:49 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 20:28:08 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1265.5709109306335 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:33:05 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 20:36:50 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 20:40:44 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 20:44:44 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 20:49:01 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1229.1372566223145 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:53:35 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 20:57:17 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 21:01:08 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 21:05:08 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 21:09:25 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1225.8370683193207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:14:00 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 21:17:55 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 21:21:50 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 21:25:50 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 21:30:06 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1238.366592168808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:34:39 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 21:38:22 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 21:42:20 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 21:46:21 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 21:50:38 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1234.711371421814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:55:13 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 21:58:58 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 22:02:53 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 22:06:51 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 22:11:10 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1232.8852636814117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:15:48 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 22:19:36 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 22:23:32 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 22:27:33 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 22:32:13 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1264.6427128314972 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:36:52 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 22:40:32 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 22:44:51 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 22:48:51 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 22:53:17 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1261.959840297699 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:57:54 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 23:01:39 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 23:05:42 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 23:09:52 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 23:14:07 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1254.9104068279266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:18:48 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 23:22:40 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 23:26:34 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 23:31:02 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 23:35:21 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1286.634643793106 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:40:15 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Mon Apr  1 23:44:03 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Mon Apr  1 23:47:56 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Mon Apr  1 23:51:56 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Mon Apr  1 23:56:19 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1239.4230663776398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:00:55 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Tue Apr  2 00:04:39 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Tue Apr  2 00:08:33 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Tue Apr  2 00:12:34 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Tue Apr  2 00:16:51 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1239.548887014389 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:21:36 2024]  Iteration number: 0 with current cost as 0.4620891511944496 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06782864  0.54964399 -2.71958097
  2.98272944  2.10135087  1.38378949 -1.04662014  0.59903057  1.16728384
  1.45607173 -1.7628213   0.63711323]. 
Working on 0.4 fold... 
[Tue Apr  2 00:25:29 2024]  Iteration number: 0 with current cost as 0.4322291570721063 and parameters 
[-2.90318346  2.23743463 -2.12427964 -0.07563413  0.55327797 -2.72298948
  2.99151807  2.10526404  1.36640974 -1.04647918  0.5968373   1.16676921
  1.4429503  -1.77326935  0.64371966]. 
Working on 0.6 fold... 
[Tue Apr  2 00:29:22 2024]  Iteration number: 0 with current cost as 0.46526655369579134 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.06967451  0.54445854 -2.72998354
  2.97284184  2.09154106  1.40642191 -1.04557725  0.59120579  1.16610332
  1.47398569 -1.74837596  0.62874246]. 
Working on 0.8 fold... 
[Tue Apr  2 00:33:23 2024]  Iteration number: 0 with current cost as 0.4369844788909938 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.07277374  0.55115292 -2.72299989
  2.97897715  2.10087547  1.39052898 -1.03550758  0.59680772  1.18007824
  1.46855748 -1.75162307  0.63532446]. 
Working on 1.0 fold... 
[Tue Apr  2 00:38:35 2024]  Iteration number: 0 with current cost as 0.3949504692180704 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.08162836  0.55078167 -2.73400052
  3.00277494  2.09421189  1.35329763 -1.03838977  0.58062367  1.17151952
  1.45726672 -1.76402138  0.62880891]. 
Training complete taking 1300.073884010315 seconds. 
Discarding model... 

Training complete taking 31266.492000818253 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.1045336723327637 seconds. 
Saved predicted values as M-M-CNOT_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (2.7314086791699346,), 'R2_train': 0.6249570268817481, 'MAE_train': 1.0278615990673081, 'MSE_test': 12.953923408010226, 'R2_test': -0.2913851515745378, 'MAE_test': 2.182269865344311}. 
Saved model results as M-M-CNOT_HWE-CNOT_results.json. 
