/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:03:27 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 16:06:50 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 16:10:13 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 16:13:30 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:16:50 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1000.3973169326782 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:20:04 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 16:23:19 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 16:26:38 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 16:29:55 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:33:12 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 983.8390758037567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:36:26 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 16:39:46 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 16:43:01 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 16:46:16 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:49:24 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 971.6319913864136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:52:36 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 16:55:50 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 16:59:08 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 17:02:26 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:05:41 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 976.1853156089783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:08:54 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 17:12:09 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 17:15:22 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 17:18:39 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:21:56 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 978.5774097442627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:25:16 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 17:28:31 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 17:31:50 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 17:35:02 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:38:17 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 982.9085087776184 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:41:38 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 17:44:55 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 17:48:20 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 17:51:46 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:55:12 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1018.483992099762 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:58:46 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 18:02:09 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 18:05:44 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 18:09:06 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:12:27 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1029.9842474460602 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:15:49 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 18:19:19 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 18:22:46 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 18:26:15 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:29:43 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1037.0706691741943 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:33:12 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 18:37:01 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 18:40:38 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 18:44:17 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:47:59 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1102.0190799236298 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:51:31 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 18:55:07 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 18:58:42 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 19:02:09 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:05:40 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1053.6348786354065 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:09:03 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 19:12:29 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 19:16:09 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 19:19:37 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:23:30 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1075.6826066970825 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:27:03 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 19:30:41 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 19:34:25 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 19:37:48 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:41:11 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1060.8158502578735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:44:44 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 19:48:42 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 19:52:12 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 19:56:12 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:59:53 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1123.7014465332031 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:03:20 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 20:06:42 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 20:09:53 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 20:13:09 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:16:22 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 978.0547478199005 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:19:37 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 20:22:55 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 20:26:12 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 20:29:30 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:32:50 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 990.6959686279297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:36:04 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 20:39:20 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 20:42:46 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 20:46:00 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:49:20 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 990.5521607398987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:52:38 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 20:55:55 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 20:59:06 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 21:02:24 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:05:37 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 976.9409878253937 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:08:55 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 21:12:11 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 21:15:31 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 21:18:48 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:22:03 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 985.4285280704498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:25:17 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 21:28:38 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 21:31:58 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 21:35:15 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:38:30 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 981.5634667873383 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:41:40 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 21:44:57 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 21:48:12 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 21:51:27 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:54:44 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 979.0501296520233 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:58:00 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 22:01:15 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 22:04:32 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 22:07:45 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:11:01 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 977.1816964149475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:14:18 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 22:17:34 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 22:20:45 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 22:23:58 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:27:15 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 974.7730276584625 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:30:34 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 22:33:46 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 22:37:06 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 22:40:25 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:43:55 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1012.6508808135986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:47:33 2024]  Iteration number: 0 with current cost as 0.20767318902539744 and parameters 
[-2.40376861  2.23743462 -2.12427962 -0.11653104  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432441
  1.310299   -1.87354679]. 
Working on 0.4 fold... 
[Mon Apr  1 22:50:55 2024]  Iteration number: 0 with current cost as 0.19813386703034985 and parameters 
[-2.39265685  2.23743465 -2.12427961 -0.11653101  0.55388708 -2.77010899
  3.06858495  2.18960147  1.18551998 -1.0664831   0.60271512  1.14432442
  1.31029901 -1.87354677]. 
Working on 0.6 fold... 
[Mon Apr  1 22:54:26 2024]  Iteration number: 0 with current cost as 0.19068530204535594 and parameters 
[-2.45716183  2.23743462 -2.12427964 -0.11653103  0.55388707 -2.770109
  3.06858496  2.18960145  1.18551997 -1.06648311  0.60271509  1.14432442
  1.310299   -1.87354679]. 
Working on 0.8 fold... 
[Mon Apr  1 22:58:03 2024]  Iteration number: 0 with current cost as 0.14550650008691582 and parameters 
[-2.51316813  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858497  2.18960145  1.18551998 -1.0664831   0.60271507  1.14432442
  1.31029902 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 23:01:38 2024]  Iteration number: 0 with current cost as 0.15353156001716342 and parameters 
[-2.4813466   2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010899
  3.06858495  2.18960147  1.18551997 -1.06648312  0.60271514  1.14432442
  1.31029899 -1.87354678]. 
Training complete taking 1053.0306718349457 seconds. 
Discarding model... 

Training complete taking 25294.855712413788 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.5236704349517822 seconds. 
Saved predicted values as M-A2-CNOT_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.9415561902083533,), 'R2_train': 0.4587946638810467, 'MAE_train': 1.3817090911024592, 'MSE_test': 6.028612045819596, 'R2_test': 0.39900369676721203, 'MAE_test': 1.859773596766971}. 
Saved model results as M-A2-CNOT_Efficient-CRX_results.json. 
