/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:18:57 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:22:16 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:28:46 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:35:34 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 16:42:13 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:48:22 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1929.967015504837 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:54:28 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:01:14 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:08:00 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 17:14:40 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:20:51 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1951.2350273132324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:27:03 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:33:49 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:40:36 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 17:47:30 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:53:41 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1962.5999059677124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:59:45 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:06:27 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:13:09 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 18:19:58 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:26:06 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1950.927937746048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:32:16 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:38:52 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:45:47 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 18:52:32 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:58:42 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1955.5746734142303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:04:47 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:11:37 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:18:33 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 19:25:16 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:31:22 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1959.697095632553 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:37:29 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:44:22 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:51:12 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 19:58:07 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:04:14 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1970.4543521404266 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:10:22 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:17:09 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:24:05 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 20:30:47 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:36:55 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1959.3204817771912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:43:01 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:49:56 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:56:46 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 21:03:35 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:09:37 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1966.2794902324677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:15:51 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:22:39 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:29:22 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 21:36:17 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:42:24 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1959.039922952652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:48:22 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:55:07 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:02:03 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 22:08:47 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:14:54 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1960.3307433128357 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:21:09 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:27:59 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:34:58 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 22:41:53 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:48:00 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1976.3312501907349 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:54:00 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:00:45 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:07:26 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 23:14:11 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:20:16 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1939.5358664989471 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:26:21 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:33:02 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:39:49 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Mon Apr  1 23:46:31 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:52:37 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1939.8595983982086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 23:58:45 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:05:29 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:12:10 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 00:18:56 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:24:55 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1939.1553342342377 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 00:31:05 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:37:52 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:44:38 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 00:51:20 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:57:27 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1952.2220764160156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 01:03:32 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:10:17 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:16:58 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 01:23:50 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:29:54 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1946.6074957847595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 01:35:57 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 01:42:38 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:49:17 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 01:55:56 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:01:49 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1912.498883008957 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 02:07:50 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:14:30 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:21:07 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 02:27:49 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:34:00 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1936.4390711784363 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:40:07 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 02:47:09 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:54:08 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 03:01:03 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:07:17 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2000.1292638778687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 03:13:27 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:20:27 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:27:26 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 03:34:18 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:40:28 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1991.183193206787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 03:46:51 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 03:53:48 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:00:40 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 04:07:38 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:13:55 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 2002.1418912410736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:20:10 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:27:08 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:34:12 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 04:41:01 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:46:50 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1968.5734448432922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 04:52:46 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 04:59:28 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:06:02 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 05:12:32 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:18:21 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1887.6128559112549 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 05:24:06 2024]  Iteration number: 0 with current cost as 0.16568026486602191 and parameters 
[-2.63993171  2.23743459 -2.12427964 -0.11653103  0.55388703 -2.77010902
  3.06858496  2.18960148  1.18551996 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354675  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136107
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 05:30:31 2024]  Iteration number: 0 with current cost as 0.17313270197007372 and parameters 
[-2.62073802  2.23743464 -2.12427964 -0.11653103  0.55388704 -2.77010901
  3.06858496  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432443
  1.31029899 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654242  0.7289737   1.60512662  2.83077103 -1.26456712 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:36:55 2024]  Iteration number: 0 with current cost as 0.1675375044816612 and parameters 
[-2.62635382  2.23743461 -2.12427961 -0.11653105  0.55388705 -2.770109
  3.06858498  2.18960148  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.72897372  1.60512661  2.83077104 -1.26456712 -0.25136105
 -2.39279215 -2.27309774  3.13337152  2.54856956 -0.67550787 -2.69002204]. 
Working on 0.8 fold... 
[Tue Apr  2 05:43:22 2024]  Iteration number: 0 with current cost as 0.13156835156500912 and parameters 
[-2.57114679  2.23743464 -2.12427964 -0.11653101  0.55388708 -2.77010899
  3.068585    2.18960147  1.18552    -1.06648308  0.60271512  1.14432447
  1.310299   -1.87354676  0.72965082  2.88578419 -0.54534333 -0.47522483
 -2.0265424   0.72897371  1.60512664  2.83077105 -1.2645671  -0.25136103
 -2.39279218 -2.27309774  3.13337155  2.5485696  -0.67550785 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:49:07 2024]  Iteration number: 0 with current cost as 0.1482949744651309 and parameters 
[-2.4977156   2.23743464 -2.1242796  -0.11653103  0.55388708 -2.77010897
  3.068585    2.18960149  1.18552002 -1.06648308  0.60271514  1.14432447
  1.31029899 -1.87354676  0.72965084  2.88578423 -0.54534331 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136101
 -2.39279216 -2.27309774  3.13337157  2.5485696  -0.67550783 -2.69002202]. 
Training complete taking 1846.5071535110474 seconds. 
Discarding model... 

Training complete taking 48764.224640607834 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.064457654953003 seconds. 
Saved predicted values as M-M-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.4614219201854617,), 'R2_train': 0.524720713504668, 'MAE_train': 1.405538477924619, 'MSE_test': 15.2799557855674, 'R2_test': -0.5232688504237617, 'MAE_test': 2.617093479400039}. 
Saved model results as M-M-CZ_Full-CRZ_results.json. 
