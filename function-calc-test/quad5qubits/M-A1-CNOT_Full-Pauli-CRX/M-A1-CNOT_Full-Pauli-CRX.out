/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:35 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:02:01 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 16:11:39 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 16:20:31 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 16:29:51 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 16:41:11 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 16:45:53 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3252.435230255127 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:56:13 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:05:19 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 17:14:09 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 17:23:44 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 17:35:01 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 17:39:44 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3225.4539284706116 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:49:57 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 17:59:10 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 18:08:31 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 18:17:44 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 18:29:07 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 18:33:50 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3256.5125501155853 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:44:16 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 18:53:20 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:02:07 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 19:11:24 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 19:22:37 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 19:27:25 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3208.172169446945 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:37:42 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 19:46:46 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 19:55:37 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 20:04:54 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 20:16:38 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 20:21:25 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3243.021968126297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:31:45 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 20:40:50 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 20:49:38 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 20:58:49 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 21:10:27 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 21:15:09 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3218.6156861782074 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:25:24 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 21:34:26 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 21:43:14 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 21:52:29 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 22:03:46 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 22:08:30 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3202.8451042175293 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:18:45 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 22:27:56 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 22:36:46 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 22:46:08 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 22:57:32 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 23:02:39 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3255.1798894405365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:13:02 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Mon Apr  1 23:22:16 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Mon Apr  1 23:31:17 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Mon Apr  1 23:40:53 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Mon Apr  1 23:52:30 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Mon Apr  1 23:57:22 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3285.705244541168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:07:48 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 00:16:52 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 00:25:41 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 00:34:56 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 00:46:14 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 00:50:56 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3205.235591173172 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:01:13 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 01:10:15 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 01:19:13 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 01:28:38 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 01:40:12 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 01:44:59 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3251.4093885421753 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 01:55:25 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:04:28 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 02:13:17 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 02:22:38 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 02:33:53 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 02:38:34 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3204.557205438614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:48:49 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 02:58:14 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 03:07:28 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 03:16:46 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 03:28:08 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 03:32:54 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3267.8877787590027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:43:17 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 03:52:19 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:01:26 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 04:10:36 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 04:21:53 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 04:26:36 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3216.0330278873444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 04:36:53 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 04:45:58 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 04:54:52 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 05:04:17 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 05:15:33 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 05:20:18 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3220.7613067626953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 05:30:34 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 05:39:38 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 05:48:30 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 05:57:51 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 06:09:11 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 06:13:56 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3244.22327876091 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 06:24:38 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 06:34:14 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 06:43:23 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 06:52:35 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 07:03:46 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 07:08:28 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3272.575525045395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 07:19:10 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 07:28:12 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 07:37:10 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 07:46:32 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 07:57:47 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 08:02:31 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3217.2262873649597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 08:12:48 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 08:22:07 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 08:31:13 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 08:40:41 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 08:52:54 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 08:57:44 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3320.201339483261 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 09:08:08 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 09:17:11 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 09:26:29 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 09:35:51 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 09:47:07 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 09:51:49 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3233.3279728889465 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 10:02:00 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 10:11:01 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 10:19:51 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 10:29:07 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 10:40:24 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 10:45:08 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3218.91410779953 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 10:55:41 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 11:04:43 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 11:13:30 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 11:22:39 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 11:33:52 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 11:38:33 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3187.524291038513 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 11:48:48 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 11:58:13 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 12:07:22 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 12:16:43 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 12:28:07 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 12:32:50 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3255.266386985779 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 12:43:02 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 12:52:08 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 13:00:57 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 13:10:11 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 13:21:28 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 13:26:11 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3217.740156173706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 13:36:41 2024]  Iteration number: 0 with current cost as 0.22812200893762946 and parameters 
[-2.51344184  2.41098389 -1.96261876 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.12344446  1.14432445
  0.70117074 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.4 fold... 
[Tue Apr  2 13:45:41 2024]  Iteration number: 0 with current cost as 0.20811841018741534 and parameters 
[-2.55565536  2.38935658 -1.97900978 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648309  0.17532643  1.14432445
  0.76526636 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654241  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
Working on 0.6 fold... 
[Tue Apr  2 13:54:31 2024]  Iteration number: 0 with current cost as 0.21090974003031723 and parameters 
[-2.4772468   2.40487547 -1.94828333 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648308  0.08215695  1.14432445
  0.64427696 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136104]. 
Working on 0.8 fold... 
[Tue Apr  2 14:03:42 2024]  Iteration number: 0 with current cost as 0.1655554915945429 and parameters 
[-2.47427862  2.41804565 -1.94509993 -0.11653102  0.55388708 -2.77010897
  3.06858498  2.18960145  1.18551998 -1.06648308  0.08288742  1.14432445
  0.63721182 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105]. 
[Tue Apr  2 14:15:26 2024]  Iteration number: 50 with current cost as 0.12157893402355878 and parameters 
[-3.77618496  3.12484372 -3.05771143 -0.11653155  0.55388613 -2.77010919
  3.06858524  2.18960193  1.18552108 -1.06648399 -2.72289993  1.14432438
  2.74518931 -1.87354825  0.72965065  2.88578335 -0.5453437  -0.47522564
 -2.02654261  0.72897355  1.60512618  2.83077079 -1.2645667  -0.25136119]. 
Working on 1.0 fold... 
[Tue Apr  2 14:20:07 2024]  Iteration number: 0 with current cost as 0.16245537640650617 and parameters 
[-2.55198389  2.39125231 -1.97523875 -0.11653102  0.55388708 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648308  0.17380436  1.14432446
  0.75523225 -1.8735468   0.72965081  2.8857842  -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456709 -0.25136104]. 
Training complete taking 3231.0182423591614 seconds. 
Discarding model... 

Training complete taking 80911.84543204308 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.1339778900146484 seconds. 
Saved predicted values as M-A1-CNOT_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (3.2759607256594108,), 'R2_train': 0.5501859316258355, 'MAE_train': 1.2944994698835817, 'MSE_test': 4.883533781279378, 'R2_test': 0.51315730271671, 'MAE_test': 1.7110906721123769}. 
Saved model results as M-A1-CNOT_Full-Pauli-CRX_results.json. 
