/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:20 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:50:08 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 15:56:44 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:06:52 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:13:31 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:19:19 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2319.2773587703705 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:28:45 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 16:35:25 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:45:33 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:52:19 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:58:14 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2318.1853873729706 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:07:47 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 17:14:59 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:25:10 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:31:53 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:37:48 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2378.8235387802124 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:47:10 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 17:53:56 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:04:05 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:10:43 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:16:34 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2331.1688933372498 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:26:01 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 18:32:52 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:43:21 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:50:35 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:56:33 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2398.3843944072723 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:06:06 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 19:12:42 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:23:16 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:30:04 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:35:50 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2341.3646609783173 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:44:49 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 19:51:15 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:00:50 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:07:20 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:13:07 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2240.615081548691 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:22:22 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 20:28:53 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:38:37 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:45:04 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:50:48 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2255.1170885562897 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:59:42 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 21:06:15 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:16:01 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:22:41 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:28:46 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2292.6692986488342 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:38:22 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 21:45:25 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:55:31 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:02:13 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:07:56 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2337.2981972694397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:17:01 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 22:23:34 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:33:30 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:40:06 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:45:47 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2273.4316205978394 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:54:51 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 23:01:18 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:10:57 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:17:19 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:22:53 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2221.598575115204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:31:42 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Mon Apr  1 23:38:06 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:47:47 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:54:15 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:59:52 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2223.1922991275787 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:09:07 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 00:15:41 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:25:39 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:32:13 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:38:00 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2284.18595123291 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:46:58 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 00:53:28 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:03:13 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:09:46 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:15:26 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2247.5899605751038 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:24:20 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 01:30:43 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:40:16 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:46:38 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:52:14 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2205.0264036655426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:01:04 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 02:07:34 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:17:10 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:23:36 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:29:14 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2219.0706310272217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:38:02 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 02:44:25 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:53:55 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:00:15 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:05:49 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2193.4191484451294 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:14:35 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 03:21:01 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:31:07 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:37:38 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:43:21 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2256.642956972122 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:52:15 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 03:58:47 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:09:05 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:15:49 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:21:45 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2321.8212337493896 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 04:31:14 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 04:38:06 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:48:18 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:55:08 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:01:05 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2352.9280829429626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 05:10:49 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 05:17:30 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:27:53 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:34:57 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:40:56 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2384.3057198524475 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 05:50:05 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 05:56:52 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 06:06:42 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 06:13:16 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:19:20 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2309.418823480606 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 06:28:32 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 06:35:16 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 06:45:14 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 06:52:06 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 06:58:04 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2321.8990523815155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 07:07:32 2024]  Iteration number: 0 with current cost as 0.29438880663995853 and parameters 
[-3.99694936  2.23743496 -2.12427899 -0.11653038  0.55388675 -2.7701093
  3.06858466  2.18960145  1.18552031 -1.06648373  0.6027151   1.14432445
  1.31029899 -1.87354648  0.7296508   2.88578387 -0.54534335 -0.4752242
 -2.02654305  0.72897337  1.60512664  2.83077042 -1.2645671  -0.25136137
 -2.39279218 -2.27309774  3.13337122  2.54856926 -0.6755082  -2.69002234]. 
Working on 0.4 fold... 
[Tue Apr  2 07:14:32 2024]  Iteration number: 0 with current cost as 0.40005313034146106 and parameters 
[-2.51067551  2.23743464 -2.12427956 -0.11653095  0.55388692 -2.77010905
  3.06858483  2.18960145  1.18552006 -1.06648324  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.4752247
 -2.02654256  0.7289737   1.60512656  2.83077091 -1.2645671  -0.25136112
 -2.39279218 -2.27309782  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 07:24:44 2024]  Iteration number: 0 with current cost as 0.29988609508043185 and parameters 
[-3.94387102  2.2374356  -2.12427867 -0.1165291   0.55388708 -2.77010801
  3.06858498  2.18960241  1.18552191 -1.06648405  0.60271606  1.14432541
  1.31029995 -1.87354584  0.72965177  2.88578419 -0.54534239 -0.47522293
 -2.02654337  0.7289737   1.6051276   2.83077011 -1.2645671  -0.25136105
 -2.39279122 -2.27309774  3.13337251  2.54857054 -0.67550691 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 07:31:51 2024]  Iteration number: 0 with current cost as 0.3197770842997127 and parameters 
[-2.52613688  2.23743464 -2.12427961 -0.11653097  0.55388705 -2.77010903
  3.06858492  2.18960142  1.18551998 -1.06648315  0.60271507  1.14432442
  1.31029899 -1.87354677  0.72965077  2.88578413 -0.54534335 -0.47522482
 -2.02654247  0.72897366  1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 07:37:49 2024]  Iteration number: 0 with current cost as 0.3256669468749364 and parameters 
[-2.44927931  2.23743465 -2.12427958 -0.11653099  0.55388708 -2.77010895
  3.068585    2.18960147  1.18552004 -1.0664831   0.60271514  1.14432447
  1.310299   -1.87354674  0.72965082  2.88578421 -0.54534331 -0.4752248
 -2.02654242  0.7289737   1.60512666  2.83077105 -1.26456708 -0.25136105
 -2.39279218 -2.27309774  3.13337157  2.5485696  -0.67550785 -2.690022  ]. 
Training complete taking 2376.1418232917786 seconds. 
Discarding model... 

Training complete taking 57403.57697892189 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.5061419010162354 seconds. 
Saved predicted values as M_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (8.006882271475131,), 'R2_train': -0.09940520999385316, 'MAE_train': 2.0870079289493058, 'MSE_test': 18.821942890861713, 'R2_test': -0.8763718764936177, 'MAE_test': 3.118975137325373}. 
Saved model results as M_Full-CRZ_results.json. 
