/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:07 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:47:56 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 15:54:39 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:01:07 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:07:56 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:14:33 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2007.5752964019775 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:21:21 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 16:28:03 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:34:44 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:41:34 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:48:20 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2022.5506837368011 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:55:05 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 17:02:05 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:08:56 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:15:52 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:22:52 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2079.9141943454742 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:29:46 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 17:36:33 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:43:20 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:50:03 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:56:38 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2018.110132932663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:03:24 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:10:23 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:17:29 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:24:41 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:31:40 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2105.8908030986786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:38:34 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:45:39 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:52:41 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:00:00 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:06:58 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2121.378294944763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:13:55 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:20:38 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:27:22 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:34:32 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:41:17 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2069.9498195648193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:48:18 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:55:24 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:02:17 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:09:11 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:16:12 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2075.6444330215454 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:22:57 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 20:29:50 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:37:17 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:44:19 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:51:46 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2149.3245515823364 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:58:48 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:06:03 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:13:03 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:20:01 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:27:16 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2136.784946203232 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:34:17 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:41:05 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:48:15 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:55:04 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:02:32 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2120.8810436725616 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:09:57 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:16:44 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:23:38 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:30:35 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:37:37 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2084.505930185318 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:44:32 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:51:25 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:57:59 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:04:34 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:11:34 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2035.479229927063 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 23:18:34 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 23:25:13 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:32:05 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:38:33 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:45:09 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2011.8235068321228 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 23:51:52 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 23:58:21 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:04:51 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:11:25 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:18:10 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1980.8490986824036 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 00:24:47 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:31:17 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:37:53 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:44:37 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:51:05 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1981.4138402938843 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 00:57:49 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 01:04:33 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:11:23 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:18:08 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:24:38 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2004.6999232769012 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 01:31:17 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 01:38:02 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:44:38 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:51:19 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:58:11 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2012.6463141441345 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 02:04:56 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:11:28 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:18:25 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:25:16 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:32:19 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2068.2924904823303 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 02:39:26 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:46:28 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:53:22 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:00:25 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:07:22 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2094.718270301819 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 03:14:27 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 03:20:54 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:27:46 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:34:21 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:41:02 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2014.8328311443329 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 03:47:57 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 03:54:37 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:01:15 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:07:50 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:14:21 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1985.4344010353088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 04:20:49 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 04:27:25 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 04:34:14 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 04:40:39 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 04:47:03 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1962.1751353740692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 04:53:36 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 05:00:16 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:06:49 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:13:20 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:20:08 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 2003.5918517112732 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 05:27:00 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 05:33:32 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 05:40:05 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 05:46:36 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 05:53:24 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1994.0740716457367 seconds. 
Discarding model... 

Training complete taking 51142.541758060455 total seconds. 
Now scoring model... 
Scoring complete taking 2.1096320152282715 seconds. 
Saved predicted values as A2-A2-CNOT_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (4.13034833887576,), 'R2_train': 0.43287208068155114, 'MAE_train': 1.3214689645294884, 'MSE_test': 6.030030169581079, 'R2_test': 0.39886232307593783, 'MAE_test': 1.843984063573942}. 
Saved model results as A2-A2-CNOT_Full-CRZ_results.json. 
