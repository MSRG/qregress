/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:07:23 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:08:37 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 16:10:50 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:13:02 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 16:15:21 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 16:17:38 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 674.9910597801208 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:19:53 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 16:22:08 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:24:24 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 16:26:41 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 16:28:54 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 674.4557933807373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:31:07 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 16:33:19 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:35:32 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 16:37:42 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 16:39:54 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 665.1999607086182 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:42:11 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 16:44:23 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:46:38 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 16:48:53 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 16:51:08 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 672.2418439388275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 16:53:27 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 16:55:41 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:57:56 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 17:00:15 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 17:02:33 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 681.8099634647369 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:04:48 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 17:07:06 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:09:24 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 17:11:44 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 17:14:01 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 692.3613348007202 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:16:21 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 17:18:37 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:20:53 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 17:23:14 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 17:25:32 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 689.124950170517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:27:50 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 17:30:05 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:32:20 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 17:34:34 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 17:36:51 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 678.1743171215057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:39:05 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 17:41:20 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:43:33 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 17:45:46 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 17:48:01 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 669.4154255390167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:50:16 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 17:52:33 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:54:47 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 17:57:00 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 17:59:13 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 672.12855219841 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:01:28 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 18:03:40 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:05:53 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 18:08:07 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 18:10:22 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 668.1020436286926 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:12:36 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 18:14:54 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:17:07 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 18:19:21 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 18:21:36 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 674.0956304073334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:23:50 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 18:26:05 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:28:19 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 18:30:31 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 18:32:44 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 667.5099833011627 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:34:58 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 18:37:11 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:39:24 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 18:41:38 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 18:43:54 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 672.56560754776 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:46:12 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 18:48:25 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:50:37 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 18:52:51 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 18:55:06 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 668.8824324607849 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:57:19 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 18:59:31 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:01:46 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 19:03:58 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 19:06:12 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 667.3537549972534 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:08:25 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 19:10:36 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:12:48 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 19:15:05 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 19:17:20 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 671.1645917892456 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:19:41 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 19:22:00 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:24:17 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 19:26:31 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 19:28:48 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 684.4766774177551 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:31:01 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 19:33:21 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:35:40 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 19:37:56 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 19:40:14 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 691.9956662654877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:42:36 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 19:44:54 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:47:12 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 19:49:27 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 19:51:42 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 685.0689499378204 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:54:00 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 19:56:14 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:58:33 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 20:00:51 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 20:03:11 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 689.7573866844177 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:05:30 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 20:07:46 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:10:05 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 20:12:23 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 20:14:42 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 688.7772810459137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:17:01 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 20:19:23 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:21:38 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 20:24:02 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 20:26:22 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 699.4144661426544 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:28:41 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 20:30:58 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:33:14 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 20:35:31 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 20:37:46 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 686.60001039505 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:40:07 2024]  Iteration number: 0 with current cost as 0.20493595313687296 and parameters 
[-2.44392558  2.23743462 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858493  2.18960145  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029897 -1.87354682]. 
Working on 0.4 fold... 
[Mon Apr  1 20:42:20 2024]  Iteration number: 0 with current cost as 0.20650690649951375 and parameters 
[-2.42433008  2.23743464 -2.12427964 -0.116531    0.55388708 -2.77010902
  3.06858496  2.18960145  1.18552003 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:44:37 2024]  Iteration number: 0 with current cost as 0.201178425270031 and parameters 
[-2.49794522  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010902
  3.06858496  2.18960145  1.18551998 -1.06648313  0.6027151   1.14432445
  1.31029896 -1.87354682]. 
Working on 0.8 fold... 
[Mon Apr  1 20:46:53 2024]  Iteration number: 0 with current cost as 0.15329324824419854 and parameters 
[-2.62784633  2.23743468 -2.12427959 -0.11653098  0.5538871  -2.77010897
  3.06858498  2.18960145  1.18552003 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Mon Apr  1 20:49:07 2024]  Iteration number: 0 with current cost as 0.1686799786596886 and parameters 
[-2.5731491   2.2374346  -2.1242796  -0.11653099  0.55388708 -2.77010901
  3.06858494  2.18960149  1.18552002 -1.06648312  0.6027151   1.14432449
  1.31029895 -1.87354676]. 
Training complete taking 678.270875453949 seconds. 
Discarding model... 

Training complete taking 16963.939312696457 total seconds. 
Now scoring model... 
Scoring complete taking 1.6717290878295898 seconds. 
Saved predicted values as A2-A2-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (4.130348339790592,), 'R2_train': 0.43287208055593784, 'MAE_train': 1.3214689628024523, 'MSE_test': 6.030030164788483, 'R2_test': 0.39886232355371476, 'MAE_test': 1.8439840612722551}. 
Saved model results as A2-A2-CNOT_Efficient-CRZ_results.json. 
