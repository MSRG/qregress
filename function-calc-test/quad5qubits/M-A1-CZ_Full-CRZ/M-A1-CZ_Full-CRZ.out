/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Wed Mar 27 18:28:18 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 18:33:26 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 18:40:02 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 18:48:06 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 18:53:16 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 18:58:23 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2024.9133625030518 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 19:07:10 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 19:13:45 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 19:21:46 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 19:26:52 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 19:31:58 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2014.3947577476501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 19:40:43 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 19:47:16 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 19:55:16 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 20:00:21 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 20:05:28 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2009.0101583003998 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 20:14:15 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 20:20:57 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 20:29:00 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 20:34:09 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 20:39:18 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2030.2309594154358 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 20:48:04 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 20:54:41 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 21:02:43 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 21:07:51 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 21:12:59 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2023.1464126110077 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Wed Mar 27 21:21:47 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 21:28:23 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 21:36:26 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 21:41:34 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 21:46:39 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2017.3849737644196 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Wed Mar 27 21:55:25 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 22:02:01 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 22:10:03 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 22:15:11 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 22:20:21 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2024.1644096374512 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Wed Mar 27 22:29:08 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 22:35:43 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 22:43:48 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 22:48:55 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 22:54:02 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2020.6478927135468 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Wed Mar 27 23:02:51 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:09:25 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 23:17:30 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 23:22:37 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Wed Mar 27 23:27:43 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2019.4846467971802 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Wed Mar 27 23:36:32 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Wed Mar 27 23:43:09 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Wed Mar 27 23:51:11 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Wed Mar 27 23:56:20 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:01:29 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2026.4333724975586 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 00:10:15 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:16:51 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 00:24:59 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 00:30:06 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 00:35:12 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2023.4318554401398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 00:44:00 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 00:50:37 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 00:58:38 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 01:03:45 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 01:08:52 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2020.662867307663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 01:17:39 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:24:13 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 01:32:15 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 01:37:22 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 01:42:27 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2013.0882649421692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 01:51:10 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 01:57:44 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 02:05:46 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 02:10:53 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:16:01 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2019.6320672035217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 02:24:55 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 02:31:32 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 02:39:34 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 02:44:42 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 02:49:47 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2021.9017651081085 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 02:58:34 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:05:08 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 03:13:13 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 03:18:19 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:23:25 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2016.4418518543243 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:32:09 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 03:38:45 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 03:46:48 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 03:51:53 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 03:57:00 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2016.274004459381 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:05:46 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:12:23 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 04:20:29 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 04:25:35 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 04:30:40 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2022.8732318878174 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:39:35 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 04:46:11 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 04:54:15 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 04:59:22 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:04:29 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2025.5087442398071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:13:14 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:19:47 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 05:27:48 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 05:32:54 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 05:38:02 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2013.2049572467804 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:46:49 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 05:53:22 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 06:01:22 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 06:06:28 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 06:11:35 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2013.8727457523346 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:20:23 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 06:26:57 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 06:34:58 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 06:40:05 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 06:45:11 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2014.630373954773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:53:55 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 07:00:31 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 07:08:30 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 07:13:38 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 07:18:44 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2012.0792076587677 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 07:27:26 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 07:34:00 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 07:42:02 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 07:47:11 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 07:52:17 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2013.8605017662048 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 08:01:02 2024]  Iteration number: 0 with current cost as 0.18039407800631446 and parameters 
[-3.77749064  2.23743464 -2.12427958 -0.11653103  0.55388708 -2.77010903
  3.06858498  2.18960151  1.18552015 -1.06648314  0.6027151   1.14432439
  1.3102991  -1.87354663  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279212 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Mar 28 08:07:36 2024]  Iteration number: 0 with current cost as 0.20244008062880609 and parameters 
[-3.79579146  2.23743453 -2.12427943 -0.11653113  0.55388718 -2.77010908
  3.06858488  2.18960156  1.18552019 -1.06648319  0.60271521  1.14432445
  1.31029899 -1.87354659  0.7296507   2.88578419 -0.54534335 -0.47522475
 -2.02654251  0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136115
 -2.39279218 -2.27309774  3.13337145  2.54856948 -0.67550787 -2.69002212]. 
Working on 0.6 fold... 
[Thu Mar 28 08:15:37 2024]  Iteration number: 0 with current cost as 0.18506972236256508 and parameters 
[-3.74431286  2.2374347  -2.1242795  -0.11653103  0.55388715 -2.77010904
  3.06858498  2.18960145  1.18552012 -1.06648308  0.6027151   1.14432445
  1.31029905 -1.87354667  0.7296508   2.88578426 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077094 -1.26456703 -0.25136105
 -2.39279211 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002208]. 
Working on 0.8 fold... 
[Thu Mar 28 08:20:46 2024]  Iteration number: 0 with current cost as 0.1792224406080255 and parameters 
[-2.87751895  2.23743464 -2.12427956 -0.11653095  0.55388712 -2.77010901
  3.06858498  2.18960145  1.18552006 -1.06648308  0.60271514  1.14432445
  1.31029906 -1.87354672  0.7296508   2.88578423 -0.54534335 -0.47522481
 -2.0265424   0.7289737   1.60512664  2.83077103 -1.26456706 -0.25136105
 -2.39279214 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Mar 28 08:25:51 2024]  Iteration number: 0 with current cost as 0.19754162003151945 and parameters 
[-2.8142967   2.23743465 -2.12427962 -0.11653102  0.5538871  -2.77010897
  3.06858498  2.18960147  1.18552001 -1.06648308  0.60271511  1.14432447
  1.310299   -1.87354678  0.7296508   2.88578421 -0.54534333 -0.47522484
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.26456709 -0.25136105
 -2.39279217 -2.27309774  3.13337155  2.54856959 -0.67550787 -2.69002202]. 
Training complete taking 2014.2991881370544 seconds. 
Discarding model... 

Training complete taking 50471.57335782051 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.4884355068206787 seconds. 
Saved predicted values as M-A1-CZ_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (4.623524790523353,), 'R2_train': 0.3651552413420669, 'MAE_train': 1.5177881591217415, 'MSE_test': 11.981639061387453, 'R2_test': -0.19445748504529248, 'MAE_test': 2.47062093903899}. 
Saved model results as M-A1-CZ_Full-CRZ_results.json. 
