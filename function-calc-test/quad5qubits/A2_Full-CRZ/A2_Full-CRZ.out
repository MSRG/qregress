/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:44:10 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:46:59 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 15:50:56 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 15:55:59 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:00:32 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:04:25 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1317.2309501171112 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:08:53 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:12:51 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 16:17:52 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:22:20 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:26:13 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1306.3240525722504 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:30:39 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:34:32 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 16:39:33 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:43:59 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:47:53 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1299.6647763252258 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:52:20 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 16:56:15 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 17:01:14 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:05:41 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:09:35 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1301.4577271938324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:14:07 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:18:03 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 17:23:02 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:27:29 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:31:22 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1307.8263261318207 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:35:54 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 17:39:47 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 17:44:45 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:49:11 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:53:04 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1303.3867642879486 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:57:32 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:01:26 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 18:06:27 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:10:55 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:14:51 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1305.071044921875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 18:19:17 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:23:11 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 18:28:12 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:32:39 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:36:33 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1303.1012682914734 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:41:01 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 18:44:56 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 18:49:55 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:54:23 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:58:16 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1301.2139284610748 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:02:43 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:06:36 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 19:11:39 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:16:08 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:20:04 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1310.2623739242554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:24:35 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:28:29 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 19:33:31 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:38:00 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:41:58 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1312.8819499015808 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:46:26 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 19:50:22 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 19:55:25 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:59:55 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:03:54 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1317.3568542003632 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:08:30 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:12:26 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 20:17:35 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:22:05 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:26:01 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1325.2748930454254 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:30:30 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:34:28 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 20:39:34 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:44:06 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:48:06 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1327.2582972049713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:52:37 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 20:56:32 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 21:01:41 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:06:14 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:10:09 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1322.6090886592865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:14:43 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:18:41 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 21:23:51 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:28:21 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:32:16 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1325.8704950809479 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:36:47 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 21:40:43 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 21:45:44 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:50:11 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:54:09 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1310.4889166355133 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:58:34 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:02:28 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 22:07:28 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:11:55 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:15:48 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1300.1144006252289 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:20:15 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:24:11 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 22:29:10 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:33:37 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:37:30 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1301.9383478164673 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:41:59 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 22:45:52 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 22:50:53 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:55:20 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:59:14 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1304.074688911438 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:03:41 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:07:35 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 23:12:34 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:17:01 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:20:55 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1300.4338808059692 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:25:22 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:29:15 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 23:34:15 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:38:42 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:42:39 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1305.475018978119 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:47:10 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Apr  1 23:51:06 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Mon Apr  1 23:56:09 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:00:42 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:04:40 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1321.3000679016113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:09:14 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:13:14 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 00:18:21 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:22:54 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:26:52 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1331.9997580051422 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:31:26 2024]  Iteration number: 0 with current cost as 0.1016963867611862 and parameters 
[-4.08085323  2.23743464 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858496  2.1896015   1.18552004 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354678  0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654245  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Tue Apr  2 00:35:24 2024]  Iteration number: 0 with current cost as 0.099159776411553 and parameters 
[-4.08193639  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552001 -1.06648313  0.60271508  1.14432443
  1.31029899 -1.8735468   0.72965076  2.88578415 -0.54534339 -0.47522487
 -2.02654245  0.72897365  1.60512661  2.83077103 -1.26456712 -0.25136105
 -2.3927922  -2.27309779  3.13337153  2.54856956 -0.6755079  -2.69002204]. 
Working on 0.6 fold... 
[Tue Apr  2 00:40:28 2024]  Iteration number: 0 with current cost as 0.10581478340434428 and parameters 
[-4.07412584  2.23743464 -2.12427961 -0.116531    0.55388708 -2.770109
  3.06858498  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534337 -0.47522485
 -2.02654243  0.72897365  1.60512664  2.83077102 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:45:01 2024]  Iteration number: 0 with current cost as 0.08752176144873061 and parameters 
[-4.05783283  2.23743464 -2.12427958 -0.11653103  0.55388711 -2.77010897
  3.06858495  2.18960145  1.18552001 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.87354677  0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654243  0.7289737   1.60512661  2.83077101 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:48:58 2024]  Iteration number: 0 with current cost as 0.08855552951630631 and parameters 
[-4.09013748  2.23743466 -2.12427959 -0.11653103  0.55388713 -2.77010897
  3.06858498  2.18960148  1.18552004 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522483
 -2.02654243  0.7289737   1.60512664  2.83077105 -1.2645671  -0.25136102
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1326.4548966884613 seconds. 
Discarding model... 

Training complete taking 32789.07149243355 total seconds. 
Now scoring model... 
Scoring complete taking 1.8455359935760498 seconds. 
Saved predicted values as A2_Full-CRZ_predicted_values.csv
Model scores: {'MSE_train': (2.122479239786497,), 'R2_train': 0.7085676228014319, 'MAE_train': 1.2181502339611083, 'MSE_test': 3.0395138354014204, 'R2_test': 0.6969888649630598, 'MAE_test': 1.5096786791947594}. 
Saved model results as A2_Full-CRZ_results.json. 
