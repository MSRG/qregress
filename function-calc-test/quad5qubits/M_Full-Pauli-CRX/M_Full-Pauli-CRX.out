/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:47 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:02:08 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 16:07:29 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 16:13:13 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 16:18:47 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 16:24:58 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1743.6780223846436 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:31:12 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 16:36:36 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 16:42:21 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 16:47:53 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 16:54:00 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1748.8310487270355 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:00:19 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 17:05:43 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 17:11:28 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 17:17:05 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 17:23:12 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1746.0226073265076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:29:25 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 17:35:08 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 17:40:50 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 17:46:24 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 17:52:29 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1757.2824947834015 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:58:42 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 18:04:15 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 18:10:03 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 18:15:35 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 18:21:36 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1746.0435457229614 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:27:49 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 18:33:11 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 18:38:52 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 18:44:24 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 18:50:26 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1729.0027027130127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:56:38 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 19:01:58 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 19:07:39 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 19:13:09 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 19:19:19 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1732.7227902412415 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:25:30 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 19:30:51 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 19:36:33 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 19:42:03 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 19:48:05 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1726.6172180175781 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:54:18 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 19:59:39 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 20:05:21 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 20:10:51 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 20:16:52 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1741.2170116901398 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:23:19 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 20:28:39 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 20:34:22 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 20:39:54 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 20:45:58 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1730.987312555313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:52:10 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 20:57:32 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 21:03:14 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 21:08:48 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 21:14:50 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1732.1328039169312 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:21:01 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 21:26:23 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 21:32:01 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 21:37:35 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 21:44:02 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1751.4227278232574 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:50:14 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 21:55:38 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 22:01:21 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 22:06:56 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 22:12:56 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1738.1840081214905 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:19:11 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 22:24:36 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 22:30:21 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 22:36:38 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 22:42:39 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1778.94637799263 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:48:51 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 22:54:17 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 22:59:59 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 23:05:30 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 23:11:31 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1732.6360757350922 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 23:17:42 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 23:23:03 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 23:28:45 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Mon Apr  1 23:34:18 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Mon Apr  1 23:40:33 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1762.120003938675 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:47:14 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Mon Apr  1 23:52:37 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Mon Apr  1 23:58:17 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 00:03:50 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 00:09:51 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1753.3330564498901 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 00:16:18 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 00:21:40 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 00:27:25 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 00:32:57 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 00:39:03 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1738.948240995407 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:45:17 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 00:50:41 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 00:56:33 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 01:02:07 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 01:08:10 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1745.153540611267 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 01:14:22 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 01:19:44 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 01:25:27 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 01:31:12 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 01:37:15 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1751.4709327220917 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:43:35 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 01:48:56 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 01:54:39 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 02:00:09 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 02:06:11 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1745.012990474701 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 02:12:40 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 02:18:02 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 02:24:04 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 02:29:39 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 02:35:41 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1759.918378829956 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:42:07 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 02:47:33 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 02:53:21 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 02:58:53 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 03:05:00 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1754.8075625896454 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 03:11:13 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 03:16:38 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 03:22:17 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 03:27:48 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 03:33:53 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1732.8425302505493 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:40:06 2024]  Iteration number: 0 with current cost as 0.2776027232757723 and parameters 
[-2.46810768  2.26771119 -1.97872239 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.02944577  1.14432445
  0.61742976 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136103]. 
Working on 0.4 fold... 
[Tue Apr  2 03:45:26 2024]  Iteration number: 0 with current cost as 0.321778846549698 and parameters 
[-2.506816    2.26400035 -1.99665226 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551999 -1.06648309  0.07780918  1.14432446
  0.67710987 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.26456708 -0.25136102]. 
Working on 0.6 fold... 
[Tue Apr  2 03:51:07 2024]  Iteration number: 0 with current cost as 0.3043821986041623 and parameters 
[-2.46819589  2.29879743 -1.97590872 -0.11653103  0.55388709 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648309  0.03489444  1.14432446
  0.61554277 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 0.8 fold... 
[Tue Apr  2 03:56:39 2024]  Iteration number: 0 with current cost as 0.27844379221861737 and parameters 
[-2.46988992  2.2707539  -1.97762255 -0.11653102  0.55388709 -2.77010898
  3.06858498  2.18960145  1.18551999 -1.06648309  0.03011531  1.14432445
  0.61613206 -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456709 -0.25136103]. 
Working on 1.0 fold... 
[Tue Apr  2 04:02:40 2024]  Iteration number: 0 with current cost as 0.33518281157169855 and parameters 
[-2.51964391  2.24190541 -2.00374358 -0.11653103  0.55388709 -2.77010897
  3.06858498  2.18960145  1.18551999 -1.06648308  0.07929068  1.14432445
  0.6906899  -1.87354679  0.7296508   2.88578419 -0.54534334 -0.47522484
 -2.0265424   0.7289737   1.60512664  2.83077106 -1.26456708 -0.25136102]. 
Training complete taking 1723.7995564937592 seconds. 
Discarding model... 

Training complete taking 43603.13549351692 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.1412997245788574 seconds. 
Saved predicted values as M_Full-Pauli-CRX_predicted_values.csv
Model scores: {'MSE_train': (2.4033192540387764,), 'R2_train': 0.6700061747402228, 'MAE_train': 1.1777526322130154, 'MSE_test': 11.984919926044467, 'R2_test': -0.1947845566026074, 'MAE_test': 2.310562719021964}. 
Saved model results as M_Full-Pauli-CRX_results.json. 
