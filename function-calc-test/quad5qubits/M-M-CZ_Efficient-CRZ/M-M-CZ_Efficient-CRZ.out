/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:46:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:48:45 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 15:52:20 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 15:55:57 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 15:59:36 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:02:52 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1039.2477881908417 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:06:02 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 16:09:46 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:13:24 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:16:51 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:20:14 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1047.2078897953033 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:23:31 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 16:27:04 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:30:41 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:34:19 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:37:33 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1043.7199840545654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:40:56 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 16:44:37 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:48:16 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:52:10 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 16:55:23 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1068.7399458885193 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 16:58:40 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 17:02:25 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:06:04 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:09:57 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:13:13 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1063.2028439044952 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:16:33 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 17:20:17 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:24:03 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:27:59 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:31:25 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1097.9002659320831 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:34:47 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 17:38:27 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:42:20 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:46:03 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 17:49:21 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1072.9978539943695 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:52:40 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 17:56:21 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:59:59 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:03:42 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:06:58 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1064.878099679947 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:10:24 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 18:14:10 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:17:55 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:21:42 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:24:58 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1071.930844783783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:28:14 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 18:31:54 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:35:27 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:39:11 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 18:42:35 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1055.8680453300476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:45:52 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 18:49:39 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:53:31 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:57:18 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:00:45 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1099.9149816036224 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 19:04:14 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 19:07:57 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:11:46 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:15:29 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:18:52 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1082.1429452896118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:22:18 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 19:26:03 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:29:44 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:33:36 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:37:10 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1094.8808977603912 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:40:30 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 19:44:08 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:47:45 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:51:29 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 19:54:50 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1061.544667005539 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:58:09 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 20:01:55 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:05:47 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:09:35 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:13:08 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1094.0014278888702 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:16:27 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 20:20:09 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:24:00 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:27:42 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:31:10 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1084.4079639911652 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:34:28 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 20:38:02 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:41:45 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:45:21 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 20:48:41 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1047.8547999858856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:51:55 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 20:55:38 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:59:18 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:03:03 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:06:26 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1067.7259666919708 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:09:45 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 21:13:27 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:17:08 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:20:46 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:24:12 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1065.4760715961456 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 21:27:30 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 21:31:12 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:34:52 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:38:44 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:42:06 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1080.5570459365845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:45:26 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 21:49:06 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:52:57 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:56:41 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 21:59:59 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1068.0289273262024 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 22:03:20 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 22:07:10 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:10:55 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:14:50 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:18:21 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1099.5011155605316 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 22:21:39 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 22:25:23 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:29:03 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:32:47 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:36:16 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1077.2662060260773 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 22:39:37 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 22:43:29 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:47:15 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:50:52 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 22:54:19 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1082.127722978592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:57:47 2024]  Iteration number: 0 with current cost as 0.16568026472792347 and parameters 
[-2.63993171  2.23743464 -2.12427959 -0.11653103  0.55388708 -2.770109
  3.06858496  2.18960145  1.18551998 -1.06648308  0.60271513  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Mon Apr  1 23:01:26 2024]  Iteration number: 0 with current cost as 0.1731327017207373 and parameters 
[-2.62073801  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648308  0.60271512  1.14432447
  1.310299   -1.8735468 ]. 
Working on 0.6 fold... 
[Mon Apr  1 23:05:14 2024]  Iteration number: 0 with current cost as 0.16753750372287413 and parameters 
[-2.62635377  2.23743461 -2.12427961 -0.11653103  0.55388703 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648311  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Mon Apr  1 23:08:54 2024]  Iteration number: 0 with current cost as 0.13156835156940394 and parameters 
[-2.57114679  2.23743464 -2.12427962 -0.11653103  0.55388706 -2.77010899
  3.06858498  2.18960145  1.18552    -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354678]. 
Working on 1.0 fold... 
[Mon Apr  1 23:12:13 2024]  Iteration number: 0 with current cost as 0.14829497438805206 and parameters 
[-2.4977156   2.23743464 -2.12427964 -0.11653105  0.55388706 -2.77010901
  3.06858498  2.18960147  1.18551998 -1.06648312  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Training complete taking 1073.9259622097015 seconds. 
Discarding model... 

Training complete taking 26805.050904750824 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 2.3770294189453125 seconds. 
Saved predicted values as M-M-CZ_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.461421917044631,), 'R2_train': 0.5247207139359276, 'MAE_train': 1.4055384758009386, 'MSE_test': 15.279955846408637, 'R2_test': -0.5232688564890646, 'MAE_test': 2.6170934833303225}. 
Saved model results as M-M-CZ_Efficient-CRZ_results.json. 
