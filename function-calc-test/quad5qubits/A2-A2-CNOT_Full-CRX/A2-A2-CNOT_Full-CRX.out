/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 15:47:54 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 15:50:51 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 15:56:17 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:01:40 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:07:11 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:12:43 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1636.9806640148163 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:18:16 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 16:23:55 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:29:31 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 16:35:07 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 16:40:40 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1678.508392572403 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:46:13 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 16:51:50 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 16:57:23 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:03:02 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:08:36 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1671.4755544662476 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 17:14:10 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 17:19:36 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:25:05 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:30:36 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 17:36:09 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1660.6534345149994 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:41:47 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 17:47:19 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 17:52:44 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 17:58:15 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:03:46 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1651.5278584957123 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:09:15 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:14:44 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:20:16 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:25:47 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:31:19 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1652.9890532493591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:36:50 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 18:42:21 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 18:47:52 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 18:53:23 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 18:58:53 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1655.9103000164032 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:04:26 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:10:00 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:15:31 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:21:01 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:26:30 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1655.5607814788818 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:32:00 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 19:37:32 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 19:43:01 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 19:48:35 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 19:54:07 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1658.69784283638 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:59:41 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 20:05:13 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:10:52 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:16:24 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:21:54 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1665.4916207790375 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 20:27:25 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 20:33:00 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 20:38:37 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 20:44:11 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 20:49:51 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1681.2423558235168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:55:30 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:01:11 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:06:48 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:12:24 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:18:03 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1687.4170327186584 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:23:42 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:29:18 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 21:34:54 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 21:40:28 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 21:46:07 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1684.6280670166016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:51:46 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 21:57:30 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:03:12 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:08:54 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:14:34 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1711.6615624427795 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:20:16 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:25:58 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 22:31:36 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 22:37:18 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 22:42:55 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1702.297375679016 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 22:48:40 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 22:54:25 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:00:12 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:05:58 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:11:52 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1737.6162693500519 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 23:17:36 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 23:23:20 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:29:05 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Apr  1 23:34:48 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Apr  1 23:40:34 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1721.7663359642029 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 23:46:23 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Mon Apr  1 23:52:02 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Apr  1 23:57:55 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:03:37 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:09:24 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1730.2639455795288 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 00:15:12 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:21:02 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:26:41 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 00:32:28 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 00:38:16 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1732.1664311885834 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 00:44:05 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 00:49:53 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 00:55:43 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:01:25 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:07:12 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1732.5920586585999 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Tue Apr  2 01:12:53 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 01:18:40 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:24:24 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:30:09 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 01:35:52 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1722.6103739738464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Tue Apr  2 01:41:41 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 01:47:31 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 01:53:16 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 01:58:52 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:04:38 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1729.039614200592 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Tue Apr  2 02:10:21 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:16:09 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:21:53 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:27:51 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 02:33:34 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1730.6131584644318 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Tue Apr  2 02:39:10 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 02:44:52 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 02:50:32 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 02:56:11 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:01:53 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1699.9357149600983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Tue Apr  2 03:07:31 2024]  Iteration number: 0 with current cost as 0.20493595311760798 and parameters 
[-2.44392558  2.23743464 -2.12427964 -0.11653103  0.55388706 -2.77010901
  3.06858497  2.18960144  1.18551997 -1.0664831   0.6027151   1.14432442
  1.31029897 -1.87354678  0.72965077  2.88578418 -0.54534337 -0.47522483
 -2.02654242  0.7289737   1.60512664  2.83077105 -1.26456711 -0.25136106
 -2.39279218 -2.27309774  3.13337151  2.54856957 -0.67550787 -2.69002203]. 
Working on 0.4 fold... 
[Tue Apr  2 03:13:05 2024]  Iteration number: 0 with current cost as 0.20650690642412478 and parameters 
[-2.42433007  2.23743464 -2.12427961 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965078  2.88578419 -0.5453434  -0.47522481
 -2.0265424   0.7289737   1.60512666  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856961 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Tue Apr  2 03:18:40 2024]  Iteration number: 0 with current cost as 0.20117842517346374 and parameters 
[-2.49794521  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010904
  3.06858494  2.18960143  1.18551998 -1.06648308  0.6027151   1.14432441
  1.31029896 -1.87354676  0.7296508   2.88578417 -0.54534337 -0.47522485
 -2.02654243  0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Tue Apr  2 03:24:17 2024]  Iteration number: 0 with current cost as 0.15329324815789513 and parameters 
[-2.62784633  2.23743468 -2.12427961 -0.11653103  0.55388708 -2.770109
  3.06858498  2.18960145  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029901 -1.87354675  0.7296508   2.88578417 -0.54534335 -0.47522483
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Tue Apr  2 03:29:59 2024]  Iteration number: 0 with current cost as 0.16867997894322423 and parameters 
[-2.57314914  2.2374346  -2.12427967 -0.1165311   0.55388704 -2.77010909
  3.0685849   2.18960141  1.18551995 -1.0664832   0.60271502  1.14432437
  1.31029891 -1.8735468   0.72965073  2.88578415 -0.54534339 -0.47522489
 -2.02654244  0.7289737   1.6051266   2.83077103 -1.2645671  -0.25136108
 -2.39279222 -2.27309774  3.13337147  2.5485695  -0.67550787 -2.69002209]. 
Training complete taking 1686.7389161586761 seconds. 
Discarding model... 

Training complete taking 42278.38530802727 total seconds. 
Now scoring model... 
Scoring complete taking 1.8847787380218506 seconds. 
Saved predicted values as A2-A2-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (4.13034833887576,), 'R2_train': 0.43287208068155114, 'MAE_train': 1.3214689645294884, 'MSE_test': 6.030030169581079, 'R2_test': 0.39886232307593783, 'MAE_test': 1.843984063573942}. 
Saved model results as A2-A2-CNOT_Full-CRX_results.json. 
