/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:03:38 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 16:06:44 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 16:09:44 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 16:12:43 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 16:15:40 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 900.1077589988708 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:18:37 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 16:21:33 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 16:24:31 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 16:27:30 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 16:30:29 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 891.1880257129669 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:33:27 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 16:36:29 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 16:39:30 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 16:42:26 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 16:45:23 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 893.9256947040558 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:48:23 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 16:51:19 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 16:54:17 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 16:57:17 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 17:00:13 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 888.3142342567444 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:03:13 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 17:06:11 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 17:09:05 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 17:12:02 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 17:14:59 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 887.1979722976685 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:17:55 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 17:20:52 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 17:23:50 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 17:26:47 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 17:29:51 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 889.9301595687866 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:32:50 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 17:35:52 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 17:39:13 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 17:42:12 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 17:45:17 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 925.463342666626 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:48:12 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 17:51:07 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 17:54:04 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 17:57:00 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 17:59:56 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 884.5146608352661 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:02:56 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 18:06:00 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 18:08:57 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 18:11:55 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 18:14:53 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 891.4737355709076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:17:49 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 18:20:45 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 18:23:39 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 18:26:35 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 18:29:30 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 879.910965681076 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:32:27 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 18:35:29 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 18:38:25 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 18:41:31 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 18:44:29 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 897.9361147880554 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:47:25 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 18:50:21 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 18:53:17 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 18:56:12 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 18:59:09 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 884.225988149643 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:02:10 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 19:05:11 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 19:08:07 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 19:11:06 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 19:14:10 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 898.0758457183838 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:17:08 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 19:20:09 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 19:23:05 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 19:26:07 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 19:29:04 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 893.4520137310028 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:32:09 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 19:35:34 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 19:38:29 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 19:41:25 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 19:44:26 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 919.7631549835205 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:47:55 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 19:50:51 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 19:53:48 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 19:56:43 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 19:59:39 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 915.1935300827026 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:02:36 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 20:05:32 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 20:08:30 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 20:11:27 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 20:14:25 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 883.9259767532349 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:17:21 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 20:20:15 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 20:23:13 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 20:26:12 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 20:29:08 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 884.3603649139404 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:32:09 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 20:35:05 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 20:38:09 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 20:41:05 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 20:44:02 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 901.6678922176361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:47:05 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 20:50:02 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 20:52:59 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 20:55:56 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 20:58:54 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 882.6069571971893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:01:52 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 21:04:53 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 21:07:48 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 21:10:45 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 21:13:41 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 888.9366579055786 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:16:39 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 21:19:57 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 21:22:55 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 21:25:49 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 21:28:47 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 910.4053092002869 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:31:49 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 21:34:49 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 21:37:45 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 21:40:41 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 21:43:40 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 893.9431691169739 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:46:42 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 21:49:39 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 21:52:37 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 21:55:33 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 21:58:30 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 884.5188827514648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:01:31 2024]  Iteration number: 0 with current cost as 0.3937300771102623 and parameters 
[-3.17184382  2.23743448 -2.12427979 -0.11653103  0.55388692 -2.77010897
  3.0685849   2.18960145  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354696  0.72965065  2.88578388]. 
Working on 0.4 fold... 
[Mon Apr  1 22:04:30 2024]  Iteration number: 0 with current cost as 0.36882291685437424 and parameters 
[-3.15705313  2.23743432 -2.12427995 -0.1165311   0.55388692 -2.77010905
  3.06858483  2.18960122  1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029899 -1.87354711  0.72965049  2.88578404]. 
Working on 0.6 fold... 
[Mon Apr  1 22:07:34 2024]  Iteration number: 0 with current cost as 0.4147865771668918 and parameters 
[-3.22338825  2.2374344  -2.12427964 -0.11653095  0.55388708 -2.77010889
  3.06858498  2.18960145  1.18551998 -1.06648301  0.6027151   1.14432445
  1.31029914 -1.8735468   0.72965057  2.88578404]. 
Working on 0.8 fold... 
[Mon Apr  1 22:10:34 2024]  Iteration number: 0 with current cost as 0.38591603682869985 and parameters 
[-3.1998008   2.23743456 -2.12427971 -0.11653079  0.55388708 -2.77010882
  3.06858514  2.18960153  1.18551998 -1.06648308  0.6027151   1.14432453
  1.31029914 -1.87354696  0.72965065  2.88578404]. 
Working on 1.0 fold... 
[Mon Apr  1 22:13:39 2024]  Iteration number: 0 with current cost as 0.34737232030090814 and parameters 
[-3.18302562  2.23743448 -2.12427979 -0.11653103  0.553887   -2.77010897
  3.06858498  2.1896013   1.18551998 -1.06648324  0.60271495  1.14432445
  1.31029906 -1.87354704  0.72965049  2.88578419]. 
Training complete taking 907.8192827701569 seconds. 
Discarding model... 

Training complete taking 22378.858387947083 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 4.086016893386841 seconds. 
Saved predicted values as M-M-CNOT_Hadamard_predicted_values.csv
Model scores: {'MSE_train': (8.338335216546245,), 'R2_train': -0.14491619445995219, 'MAE_train': 2.5597054024147154, 'MSE_test': 9.635530976030246, 'R2_test': 0.039427574329521486, 'MAE_test': 2.6871858758577685}. 
Saved model results as M-M-CNOT_Hadamard_results.json. 
