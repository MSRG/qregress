/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin... 
Successfully loaded /scratch/j/jacobsen/gjones/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /scratch/j/jacobsen/gjones/quad5qubits/quadratic_train.bin 
 at time Mon Apr  1 16:01:55 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 16:02:03 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 16:04:48 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:07:26 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:10:33 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 16:13:50 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 897.28160572052 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 16:17:00 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 16:19:43 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:22:24 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:25:30 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 16:28:45 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 915.4935064315796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 16:32:15 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 16:35:03 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:37:41 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:40:53 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 16:44:08 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 903.583226442337 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 16:47:22 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 16:50:05 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 16:52:45 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 16:55:55 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 16:59:09 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 901.360401391983 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 17:02:33 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 17:05:15 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:07:55 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:11:08 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 17:14:27 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 918.2615077495575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 17:17:39 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 17:20:23 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:23:02 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:26:10 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 17:29:27 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 901.0867018699646 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 17:32:40 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 17:35:24 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:38:02 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:41:09 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 17:44:26 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 896.02756524086 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 17:47:36 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 17:50:27 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 17:53:25 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 17:56:37 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 17:59:51 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 929.0336782932281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 18:03:05 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 18:05:46 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:08:25 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:11:31 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 18:14:47 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 892.5451774597168 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 18:17:59 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 18:20:48 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:23:27 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:26:33 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 18:29:46 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 899.6024270057678 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 18:32:59 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 18:35:41 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:38:19 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:41:27 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 18:44:42 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 894.2323951721191 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 18:47:51 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 18:50:48 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 18:53:27 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 18:56:34 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 18:59:47 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 904.6187024116516 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 19:02:58 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 19:05:39 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:08:16 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:11:22 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 19:14:38 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 892.2112483978271 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 19:17:48 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 19:20:31 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:23:08 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:26:15 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 19:29:30 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 892.8416655063629 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 19:32:41 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 19:35:21 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:38:00 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:41:06 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 19:44:20 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 888.6327950954437 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 19:47:31 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 19:50:12 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 19:52:50 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 19:55:57 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 19:59:12 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 892.9231395721436 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 20:02:22 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 20:05:05 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:07:42 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:10:46 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 20:14:02 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 889.3080759048462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 20:17:12 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 20:19:52 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:22:30 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:25:38 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 20:28:51 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 889.9091458320618 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 20:32:03 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 20:34:45 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:37:23 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:40:28 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 20:43:45 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 896.7705125808716 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 20:46:58 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 20:49:42 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 20:52:23 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 20:55:29 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 20:58:49 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 905.0496153831482 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Apr  1 21:02:05 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 21:04:50 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:07:36 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:11:07 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 21:14:21 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 928.2194681167603 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Apr  1 21:17:32 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 21:20:18 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:22:56 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:26:01 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 21:29:17 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 896.3041965961456 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Apr  1 21:32:28 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 21:35:12 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:37:50 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:40:56 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 21:44:13 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 895.4676978588104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Apr  1 21:47:23 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 21:50:05 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 21:52:46 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 21:55:51 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 21:59:05 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 893.8695421218872 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Apr  1 22:02:17 2024]  Iteration number: 0 with current cost as 0.3923565710387492 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10091611  0.5602125  -2.74170703
  2.93445673  2.21298555  1.40343355 -1.06622427  0.63993404  1.1562754
  1.46880396 -1.71877639  0.75208729]. 
Working on 0.4 fold... 
[Mon Apr  1 22:04:59 2024]  Iteration number: 0 with current cost as 0.3677264563221747 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10103815  0.5611993  -2.74028392
  2.95534416  2.20661597  1.37105209 -1.06626496  0.63663992  1.15519607
  1.45324377 -1.73433154  0.7485903 ]. 
Working on 0.6 fold... 
[Mon Apr  1 22:07:37 2024]  Iteration number: 0 with current cost as 0.40976757503733663 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09899345  0.56110958 -2.73802221
  2.92338476  2.21760567  1.41988996 -1.06642777  0.64051706  1.15621066
  1.46873778 -1.72064127  0.7456331 ]. 
Working on 0.8 fold... 
[Mon Apr  1 22:10:44 2024]  Iteration number: 0 with current cost as 0.38197491024486613 and parameters 
[-2.90318345  2.23743464 -2.12427964 -0.09806517  0.55742361 -2.74278241
  2.92688433  2.21830719  1.41345737 -1.06568745  0.63683639  1.15595852
  1.47390883 -1.72070272  0.72806911]. 
Working on 1.0 fold... 
[Mon Apr  1 22:13:59 2024]  Iteration number: 0 with current cost as 0.34177005268690397 and parameters 
[-2.90318345  2.23743464 -2.12427963 -0.10216586  0.56044832 -2.7428021
  2.94629891  2.21729381  1.38056465 -1.06631887  0.63150112  1.15352395
  1.44180936 -1.75022951  0.73002794]. 
Training complete taking 905.7174386978149 seconds. 
Discarding model... 

Training complete taking 22520.35362291336 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 0.7823851108551025 seconds. 
Saved predicted values as M_HWE-CNOT_predicted_values.csv
Model scores: {'MSE_train': (2.4251105016535703,), 'R2_train': 0.6670140724027975, 'MAE_train': 1.0330475940847137, 'MSE_test': 14.575259817120706, 'R2_test': -0.4530172454572101, 'MAE_test': 2.350840775975331}. 
Saved model results as M_HWE-CNOT_results.json. 
