/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Thu Mar 28 03:45:36 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 03:46:11 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:47:38 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:49:06 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:50:33 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:52:00 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 436.511034488678 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 03:53:27 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:54:54 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 03:56:20 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:57:47 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:59:14 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.5202350616455 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:00:41 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:02:08 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:03:35 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:05:03 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:06:30 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 435.6227469444275 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:07:57 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:09:24 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:10:50 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:12:17 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:13:46 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 436.65860176086426 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:15:14 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:16:41 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:18:07 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:19:35 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:21:04 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 436.9936332702637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:22:30 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:23:57 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:25:24 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:26:51 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:28:18 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.10751271247864 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 04:29:45 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:31:11 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:32:38 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:34:05 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:35:31 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 433.7949366569519 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:36:58 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:38:25 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:39:52 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:41:19 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:42:46 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 433.9169075489044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:44:12 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:45:39 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:47:06 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:48:33 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:50:00 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.73796558380127 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:51:27 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:52:55 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 04:54:21 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:55:48 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:57:15 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.9108200073242 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:58:42 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:00:10 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:01:37 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:03:03 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:04:31 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 435.2246081829071 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:05:57 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:07:24 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:08:50 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:10:17 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:11:44 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.0620095729828 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 05:13:11 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:14:38 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:16:05 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:17:32 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:18:59 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.64963483810425 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:20:26 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:21:52 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:23:20 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:24:46 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:26:13 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.12113785743713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:27:40 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:29:07 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:30:33 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:32:00 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:33:27 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 433.69065165519714 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 05:34:54 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:36:21 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:37:49 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:39:16 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:40:44 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 436.83039569854736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:42:11 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:43:38 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:45:05 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:46:32 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:47:59 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.86415934562683 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 05:49:25 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:50:52 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:52:18 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:53:45 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:55:12 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.46129631996155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:56:40 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:58:06 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 05:59:33 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:00:59 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:02:26 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 432.987756729126 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 06:03:53 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:05:20 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:06:47 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:08:13 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:09:41 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.9676971435547 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 06:11:08 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:12:35 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:14:05 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:15:32 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:16:59 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 437.56990361213684 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:18:25 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:19:51 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:21:18 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:22:44 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:24:11 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 433.05422496795654 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:25:38 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:27:05 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:28:32 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:29:58 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:31:25 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 433.0923731327057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 06:32:52 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:34:19 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:35:45 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:37:12 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:38:39 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 434.6068603992462 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 06:40:07 2024]  Iteration number: 0 with current cost as 0.4107398887419871 and parameters 
[-3.16517157  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960144  1.18551998 -1.06648309  0.6027151   1.14432445
  1.31029898 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:41:34 2024]  Iteration number: 0 with current cost as 0.38180867728430595 and parameters 
[-3.13598876  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432446
  1.31029899 -1.8735468 ]. 
Working on 0.6 fold... 
[Thu Mar 28 06:43:00 2024]  Iteration number: 0 with current cost as 0.43691940662264367 and parameters 
[-3.09378384  2.23743463 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858498  2.18960144  1.18551998 -1.06648308  0.6027151   1.14432445
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:44:26 2024]  Iteration number: 0 with current cost as 0.40720349442605236 and parameters 
[-3.07437254  2.23743464 -2.12427964 -0.11653103  0.55388708 -2.77010898
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:45:53 2024]  Iteration number: 0 with current cost as 0.36310551600501895 and parameters 
[-3.05187504  2.23743464 -2.12427963 -0.11653102  0.55388708 -2.77010897
  3.06858499  2.18960145  1.18551998 -1.06648308  0.60271511  1.14432445
  1.31029899 -1.87354679]. 
Training complete taking 433.56081652641296 seconds. 
Discarding model... 

Training complete taking 10869.518778085709 total seconds. 
Now scoring model... 
Scoring complete taking 2.0416131019592285 seconds. 
Saved predicted values as IQP_Efficient-CRX_predicted_values.csv
Model scores: {'MSE_train': (8.713806878183169,), 'R2_train': -0.1964712800741475, 'MAE_train': 2.627965953785317, 'MSE_test': 10.186406672312858, 'R2_test': -0.015489586451493231, 'MAE_test': 2.7994175365841607}. 
Saved model results as IQP_Efficient-CRX_results.json. 
