/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/home/gjones/scratch/quad5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_train.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_train.bin into X and y data. 
Loading dataset from /home/gjones/scratch/quad5qubits/quadratic_test.bin... 
Successfully loaded /home/gjones/scratch/quad5qubits/quadratic_test.bin into X and y data. 
Training model with dataset /home/gjones/scratch/quad5qubits/quadratic_train.bin 
 at time Thu Mar 28 03:45:36 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 03:47:29 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 03:50:10 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 03:52:52 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 03:55:33 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 03:58:29 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 806.0676062107086 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 04:00:54 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:03:35 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 04:06:16 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:08:57 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:11:54 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 804.0688672065735 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 04:14:18 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:16:58 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 04:19:38 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:22:18 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:25:15 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 801.6314878463745 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 04:27:39 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:30:20 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 04:33:00 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:35:40 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:38:40 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 804.7095999717712 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 04:41:04 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:43:44 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 04:46:24 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 04:49:04 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 04:52:01 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 801.4095351696014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 04:54:25 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 04:57:06 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 04:59:46 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:02:26 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:05:22 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 800.9737260341644 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 05:07:47 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:10:27 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 05:13:07 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:15:47 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:18:43 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 800.8604385852814 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 05:21:07 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:23:49 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 05:26:29 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:29:10 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:32:08 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 804.8118629455566 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 05:34:32 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:37:13 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 05:39:53 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:42:34 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:45:30 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 802.4648554325104 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 05:47:55 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 05:50:35 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 05:53:16 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 05:55:57 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 05:58:54 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 804.7777881622314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 06:01:21 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:04:01 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 06:06:40 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:09:21 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:12:18 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 802.5808928012848 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 06:14:42 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:17:22 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 06:20:02 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:22:42 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:25:39 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 801.1006779670715 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 06:28:03 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:30:43 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 06:33:24 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:36:04 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:39:03 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 804.33154296875 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 06:41:28 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:44:08 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 06:46:48 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 06:49:28 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 06:52:24 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 800.871506690979 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 06:54:48 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 06:57:28 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 07:00:09 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:02:50 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:05:46 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 801.6480820178986 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 07:08:10 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 07:10:50 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 07:13:31 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:16:12 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:19:08 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 802.321398973465 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 07:21:32 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 07:24:12 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 07:26:53 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:29:33 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:32:30 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 801.932942867279 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 07:34:54 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 07:37:34 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 07:40:14 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:42:54 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:45:51 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 800.9809198379517 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 07:48:15 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 07:50:55 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 07:53:35 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 07:56:15 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 07:59:11 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 800.3104906082153 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 08:01:36 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:04:16 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 08:06:56 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:09:37 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:12:33 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 802.1327359676361 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Mar 28 08:14:57 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:17:38 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 08:20:18 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:22:58 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:25:54 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 800.9218752384186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Mar 28 08:28:18 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:30:58 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 08:33:38 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:36:18 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:39:17 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 803.0979743003845 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Mar 28 08:41:41 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:44:21 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 08:47:01 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 08:49:41 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 08:52:37 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 799.7210624217987 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Mar 28 08:55:01 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 08:57:41 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 09:00:23 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 09:03:04 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 09:06:00 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 802.8566341400146 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Mar 28 09:08:25 2024]  Iteration number: 0 with current cost as 0.1880284959900821 and parameters 
[-3.94895968  2.23743437 -2.12427964 -0.11653103  0.55388681 -2.77010924
  3.06858472  2.18960145  1.18551998 -1.06648335  0.60271484  1.14432418
  1.31029899 -1.8735468 ]. 
Working on 0.4 fold... 
[Thu Mar 28 09:11:06 2024]  Iteration number: 0 with current cost as 0.18521975093278442 and parameters 
[-3.94485432  2.23743363 -2.12428014 -0.11653204  0.55388607 -2.77010998
  3.06858397  2.18960044  1.18551898 -1.06648409  0.60271409  1.14432344
  1.31029899 -1.87354731]. 
Working on 0.6 fold... 
[Thu Mar 28 09:13:46 2024]  Iteration number: 0 with current cost as 0.17727545916768325 and parameters 
[-3.87572647  2.23743464 -2.12427945 -0.11653121  0.55388671 -2.77010916
  3.06858498  2.18960145  1.18552017 -1.06648346  0.60271492  1.14432464
  1.31029899 -1.8735468 ]. 
Working on 0.8 fold... 
[Thu Mar 28 09:16:26 2024]  Iteration number: 0 with current cost as 0.13847785957721492 and parameters 
[-3.79725616  2.23743444 -2.12427964 -0.11653103  0.55388669 -2.77010897
  3.06858459  2.18960126  1.18551979 -1.06648347  0.60271491  1.14432426
  1.31029899 -1.8735468 ]. 
Working on 1.0 fold... 
[Thu Mar 28 09:19:22 2024]  Iteration number: 0 with current cost as 0.15198260333365488 and parameters 
[-3.80861449  2.23743292 -2.12428049 -0.11653188  0.55388451 -2.77010983
  3.06858327  2.18959974  1.18551913 -1.06648394  0.60271425  1.14432359
  1.31029899 -1.87354766]. 
Training complete taking 802.5052330493927 seconds. 
Discarding model... 

Training complete taking 20059.090456962585 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0412371134020617. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Ignoring NaN found at index: 0. With feature: -1.0206185567010309. 
Scoring complete taking 1.9442687034606934 seconds. 
Saved predicted values as M-A1-CNOT_Efficient-CRZ_predicted_values.csv
Model scores: {'MSE_train': (3.7364466555488023,), 'R2_train': 0.4869577470111027, 'MAE_train': 1.3235909488721214, 'MSE_test': 6.288999675053495, 'R2_test': 0.37304548260651826, 'MAE_test': 1.8802913978259546}. 
Saved model results as M-A1-CNOT_Efficient-CRZ_results.json. 
