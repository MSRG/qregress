/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:53:21 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:54:15 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 17:55:36 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:54 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:24 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:44 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 408.914847612381 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:01:08 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:30 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:49 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:09 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:06:29 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 404.4061415195465 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:07:48 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:09:08 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:10:27 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:11:47 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:13:08 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 399.9917857646942 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:14:27 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:15:47 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:17:10 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:18:31 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:19:49 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.22990226745605 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:21:09 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:22:30 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:23:50 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:25:10 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:26:31 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 400.2245624065399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:27:49 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:29:09 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:30:29 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:31:50 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:33:11 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.4376790523529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:34:34 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:35:53 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:37:13 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:38:36 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:39:56 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 404.96809101104736 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:41:16 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:37 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:43:56 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:45:23 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:46:44 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 408.92758226394653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:09 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:49:29 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:50:50 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:52:12 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:53:34 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 408.15215706825256 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:53 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:14 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:42 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 18:59:04 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:23 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 410.11591601371765 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:01:43 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:05 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:04:25 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:05:53 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:07:14 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 410.9295997619629 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:08:35 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:09:54 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:11:16 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:37 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:13:57 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 403.05989623069763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:15:17 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:16:38 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:57 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:19:19 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:20:39 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 402.6288626194 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:21:58 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:23:19 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:24:38 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:25:56 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:27:17 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 397.19848442077637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:28:37 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:29:56 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:31:17 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:32:43 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:34:01 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 403.65375423431396 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:35:20 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:36:40 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:37:59 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:39:19 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:40:38 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 397.7325556278229 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:57 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:43:18 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:39 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:57 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:47:17 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 398.6383533477783 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:36 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:49:54 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:14 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:35 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:53:56 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 398.8437912464142 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:15 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 19:56:36 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 19:57:54 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 19:59:14 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:00:33 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 397.84038758277893 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:01:53 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:03:13 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:04:32 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:05:52 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:07:11 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 397.8055510520935 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:08:30 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:09:51 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:11:10 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:12:28 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:13:48 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 396.784343957901 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:15:08 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:16:26 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:17:47 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:19:08 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:20:26 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 398.07436966896057 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:21:46 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:23:08 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:24:27 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:47 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:27:05 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 398.88219261169434 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:28:25 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:29:46 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:31:14 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:32:33 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:34:05 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 419.4789021015167 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:24 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Sun Mar 24 20:36:44 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Sun Mar 24 20:38:02 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Sun Mar 24 20:39:22 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:40:42 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.8406472206116 seconds. 
Discarding model... 

Training complete taking 10071.761867284775 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.747027635574341 seconds. 
Saved predicted values as M-A1-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (58.35083788417887,), 'R2_train': 0.717309773921202, 'MAE_train': 5.013475891445163, 'MSE_test': 77.63284338127835, 'R2_test': 0.5320436667903543, 'MAE_test': 5.137892215017407}. 
Saved model results as M-A1-CZ_Modified-Pauli-CRZ_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:29:07 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:30:00 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:31:21 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:32:41 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:34:01 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:35:21 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.38594341278076 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:36:42 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:38:01 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:39:23 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:43 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:42:03 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 402.2295649051666 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:43:24 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:44:44 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:46:03 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:23 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:48:44 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 400.8061068058014 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:03 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:51:24 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:52:45 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:04 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:55:27 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 402.4983654022217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:56:47 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 12:58:07 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 12:59:28 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:00:48 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:02:08 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.4340307712555 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:03:30 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:04:52 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:06:13 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:07:32 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:08:53 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 405.1481235027313 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:10:14 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:11:33 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:12:54 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:14:15 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:15:34 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.12529945373535 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:16:54 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:18:15 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:19:34 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:20:54 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:22:18 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 403.76147985458374 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:23:37 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:24:57 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:26:18 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:27:37 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:28:58 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 399.3123483657837 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:30:18 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:31:37 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:32:59 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:34:19 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:35:39 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.51640248298645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:36:59 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:38:21 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:39:40 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:41:00 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:42:21 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 403.04557180404663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:43:45 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:45:04 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:46:24 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:47:45 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:49:03 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 401.1861181259155 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:50:27 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:51:47 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:53:19 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 13:54:39 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:59 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 415.9880087375641 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:57:19 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 13:58:38 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:59 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:01:23 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:02:43 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 402.7653033733368 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:04:02 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:22 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:45 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:08:04 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:24 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 402.36885142326355 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:10:45 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:09 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:13:30 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:14:52 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:16:11 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 406.7630624771118 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:17:32 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:18:52 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:20:11 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:21:31 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:22:56 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 404.70992946624756 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:24:17 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:25:37 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:26:58 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:28:19 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:29:40 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 426.26271963119507 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:31:23 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:33:05 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:34:25 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:35:54 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:37:19 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 437.2875578403473 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:38:38 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:40:01 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:41:23 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:42:48 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:44:07 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 407.6308596134186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:45:27 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:46:47 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:48:18 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:49:37 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:50:58 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 411.88102626800537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:52:20 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 14:53:40 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 14:55:00 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 14:56:24 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:57:44 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 405.1773474216461 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:59:05 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:00:29 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:01:50 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:03:09 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:04:29 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 405.2353754043579 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 15:05:50 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:07:09 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:08:30 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:09:51 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:11:10 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 400.0046281814575 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:12:30 2024]  Iteration number: 0 with current cost as 0.10822095880898935 and parameters 
[-4.6631934   2.23743464 -2.12427957 -0.11653103  0.55388708 -2.77010904
  3.06858498  2.18960145  1.18551998 -1.06648316  0.6027151   1.14432445
  1.31029899 -1.87354673  0.72965066  2.88578412 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.4 fold... 
[Thu Apr  4 15:13:51 2024]  Iteration number: 0 with current cost as 0.09061534864854894 and parameters 
[-4.69009509  2.23743455 -2.12427955 -0.11653111  0.55388699 -2.77010906
  3.06858481  2.18960154  1.18552007 -1.06648326  0.6027151   1.14432445
  1.31029907 -1.8735468   0.72965063  2.88578411 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.6 fold... 
[Thu Apr  4 15:15:10 2024]  Iteration number: 0 with current cost as 0.08455690796768241 and parameters 
[-4.66452267  2.23743448 -2.12427964 -0.1165311   0.553887   -2.77010912
  3.06858483  2.18960153  1.18552006 -1.06648316  0.60271503  1.14432445
  1.31029899 -1.8735468   0.72965065  2.88578404 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 0.8 fold... 
[Thu Apr  4 15:16:31 2024]  Iteration number: 0 with current cost as 0.08874339114764358 and parameters 
[-4.68039069  2.23743464 -2.12427949 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.1896016   1.18552013 -1.06648316  0.6027151   1.14432445
  1.31029906 -1.87354666  0.72965066  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Working on 1.0 fold... 
[Thu Apr  4 15:17:56 2024]  Iteration number: 0 with current cost as 0.09587137137390866 and parameters 
[-4.67463181  2.23743455 -2.12427947 -0.11653103  0.55388708 -2.77010897
  3.06858498  2.18960154  1.18552015 -1.06648317  0.6027151   1.14432462
  1.31029915 -1.87354672  0.72965072  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737 ]. 
Training complete taking 406.80924940109253 seconds. 
Discarding model... 

Training complete taking 10156.33480477333 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 1.0292367935180664 seconds. 
Saved predicted values as M-A1-CZ_Modified-Pauli-CRZ_predicted_values.csv
Model scores: {'MSE_train': (58.35083788417887,), 'R2_train': 0.717309773921202, 'MAE_train': 5.013475891445163, 'MSE_test': 77.63284338127835, 'R2_test': 0.5320436667903543, 'MAE_test': 5.137892215017407}. 
Saved model results as M-A1-CZ_Modified-Pauli-CRZ_results.json. 
