/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:35:02 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:39:14 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 17:44:03 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:49:33 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:33 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:02:11 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1773.1547257900238 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:46 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 18:13:34 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:18:58 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:27:59 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:31:34 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1762.9390153884888 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:38:11 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 18:42:59 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:48:23 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:57:25 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:01:00 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1766.4325776100159 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:07:34 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 19:12:19 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:17:42 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:26:43 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:30:18 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1756.9789290428162 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:36:54 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 19:41:40 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:47:04 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:56:03 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:59:40 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1761.8328199386597 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:06:15 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 20:11:04 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:16:25 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:25:27 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:29:02 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1762.3592538833618 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:35:35 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 20:40:22 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:45:43 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:54:44 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:58:19 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1756.478768825531 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:04:50 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 21:09:38 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:15:00 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:23:59 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:27:34 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1755.7198390960693 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 21:34:10 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 21:38:57 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:44:21 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:53:24 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:57:03 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1767.9096956253052 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 22:03:35 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 22:08:23 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:13:46 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:41 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:26:15 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1754.1783344745636 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 22:32:53 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 22:37:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:42:59 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:51:57 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:55:31 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1756.72074508667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 23:02:10 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 23:06:58 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:12:21 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:21:19 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:24:53 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1760.8613135814667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 23:31:34 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Sun Mar 24 23:36:22 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:41:44 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:50:42 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:54:18 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1763.2400114536285 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 00:00:52 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 00:05:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:11:16 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:20:29 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:24:09 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1795.6068441867828 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 00:30:57 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 00:35:49 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:41:24 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:50:38 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:54:21 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1818.7764196395874 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 01:01:19 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 01:06:20 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:12:01 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:21:22 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:25:05 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1839.841028213501 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 01:31:58 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 01:36:58 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:42:31 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:52:03 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:55:44 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1836.9847333431244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 02:02:35 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 02:07:37 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:13:13 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:22:32 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:26:19 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1837.707656621933 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 02:33:21 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 02:38:22 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:43:53 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:53:11 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:56:57 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1832.2852766513824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 03:03:38 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 03:08:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:14:17 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:23:40 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:27:24 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1832.0149343013763 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 03:34:15 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 03:39:11 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:44:48 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:54:26 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:58:12 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1849.9611105918884 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 04:05:04 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 04:10:06 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:15:43 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:25:00 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:28:49 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1834.0044250488281 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:35:43 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 04:40:49 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:46:23 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:55:41 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:59:25 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1839.1006298065186 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 05:06:22 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 05:11:30 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:17:07 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:26:47 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:30:37 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1872.1734838485718 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:37:33 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Mon Mar 25 05:42:37 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:48:22 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:57:42 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:01:29 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 1848.4851019382477 seconds. 
Discarding model... 

Training complete taking 44935.748304605484 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.2036244869232178 seconds. 
Saved predicted values as M-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (256.3537191753004,), 'R2_train': -0.24194773301542183, 'MAE_train': 14.396656386710248, 'MSE_test': 224.07254103387086, 'R2_test': -0.3506675797020311, 'MAE_test': 12.79502700130196}. 
Saved model results as M-A1-CNOT_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 12:09:49 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:16:17 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 12:23:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:32:01 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:45:55 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:51:21 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2708.309906244278 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:01:38 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 13:09:15 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:17:48 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:52 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:37:38 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2789.405444383621 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:48:08 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 13:55:56 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:04:55 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:19:23 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:25:06 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2835.800987958908 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:35:03 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 14:42:36 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:50:59 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:04:49 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:10:15 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2710.9460682868958 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 15:20:31 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 15:28:18 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:36:53 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:50:29 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:56:02 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2746.242154121399 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 16:06:07 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 16:13:41 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:22:00 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:35:58 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:41:31 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2731.73015332222 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 16:51:43 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 16:59:01 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:07:12 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:20:51 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:26:18 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2687.154601097107 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 17:36:20 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 17:43:39 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:51:40 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:05:07 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:10:38 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2660.481957912445 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 18:20:39 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 18:27:52 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:35:59 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:50:00 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:55:37 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2697.1249809265137 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 19:05:42 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 19:13:04 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:21:05 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:34:39 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:40:09 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2674.4455144405365 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 19:50:23 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 19:57:50 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:06:14 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:20:09 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:25:48 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2747.1054446697235 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 20:36:18 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 20:43:48 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:52:18 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:06:37 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:12:16 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2785.8119852542877 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 21:22:49 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 21:30:21 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:38:34 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:52:14 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:57:45 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2716.862086057663 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 22:07:47 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 22:15:12 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:23:27 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:37:11 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:42:48 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2712.0178878307343 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 22:52:54 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 23:00:13 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:08:26 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:22:05 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:27:35 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2683.3459837436676 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 23:37:36 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Thu Apr  4 23:44:57 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:53:18 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:07:11 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:12:39 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2703.115973472595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:22:46 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 00:30:05 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:38:12 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:52:01 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:57:28 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2684.059948682785 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:07:27 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 01:14:45 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:23:08 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:36:47 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:42:12 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2690.733973503113 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 01:52:22 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 01:59:33 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:07:42 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:21:41 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:27:12 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2696.861988544464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:37:21 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 02:44:37 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:53:02 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:06:36 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:12:11 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2698.6000366210938 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 03:22:16 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 03:29:35 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:37:44 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:51:20 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:56:41 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2674.4409277439117 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 04:06:42 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 04:13:58 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 04:22:05 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 04:35:45 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 04:41:10 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2661.175554037094 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 04:51:00 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 04:58:18 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:06:27 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 05:20:10 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 05:25:34 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2665.415674686432 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 05:35:34 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 05:42:46 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 05:50:56 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:04:40 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:10:04 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2672.3967983722687 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 06:20:01 2024]  Iteration number: 0 with current cost as 0.4607451290534206 and parameters 
[-1.08295669  2.23743464 -2.12427914 -0.11653103  0.55388708 -2.77010947
  3.06858398  2.18960145  1.18551998 -1.06648408  0.6027146   1.14432445
  1.31029899 -1.8735463   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136154
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002252]. 
Working on 0.4 fold... 
[Fri Apr  5 06:27:16 2024]  Iteration number: 0 with current cost as 0.3762389887700398 and parameters 
[-1.15776266  2.23743476 -2.12427926 -0.11653103  0.5538872  -2.7701091
  3.06858486  2.18960158  1.18552011 -1.06648321  0.60271523  1.14432458
  1.31029886 -1.87354668  0.7296508   2.88578432 -0.54534322 -0.47522473
 -2.02654228  0.72897382  1.60512664  2.83077107 -1.26456697 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856971 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 06:35:22 2024]  Iteration number: 0 with current cost as 0.5001342677203618 and parameters 
[-0.07997934  2.23743464 -2.12427877 -0.11653074  0.55388737 -2.77010926
  3.06858469  2.18960145  1.18551998 -1.06648337  0.60271539  1.14432474
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522456
 -2.02654269  0.7289737   1.60512664  2.83077078 -1.2645671  -0.25136105
 -2.39279247 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 06:49:02 2024]  Iteration number: 0 with current cost as 0.3985886623309338 and parameters 
[-1.12518701  2.23743499 -2.12427911 -0.11653067  0.55388708 -2.77010897
  3.06858481  2.18960145  1.18551998 -1.06648308  0.60271528  1.14432463
  1.31029899 -1.87354645  0.7296508   2.88578437 -0.54534335 -0.4752245
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337172  2.54856958 -0.6755077  -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 06:54:33 2024]  Iteration number: 0 with current cost as 0.40254990535945984 and parameters 
[-1.11690623  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432461
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.02654272  0.7289737   1.60512648  2.83077076 -1.2645671  -0.2513612
 -2.39279234 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2692.459016084671 seconds. 
Discarding model... 

Training complete taking 67726.0457804203 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.5175437927246094 seconds. 
Saved predicted values as M-A1-CNOT_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (256.3537191753004,), 'R2_train': -0.24194773301542183, 'MAE_train': 14.396656386710248, 'MSE_test': 224.07254103387086, 'R2_test': -0.3506675797020311, 'MAE_test': 12.79502700130196}. 
Saved model results as M-A1-CNOT_Full-CRX_results.json. 
