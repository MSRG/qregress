/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:34:27 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 17:41:21 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 17:50:58 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 17:58:01 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:04:06 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2238.2461490631104 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:11:39 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:18:18 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 18:27:39 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 18:34:43 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 18:40:45 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2205.479268550873 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:48:37 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 18:55:30 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:17 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:12:22 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:18:28 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2268.9986555576324 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:26:32 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 19:33:31 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 19:43:16 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 19:50:36 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 19:56:39 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2289.9097485542297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:04:45 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:12:07 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 20:21:50 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 20:29:06 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 20:35:14 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2319.6862320899963 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:43:36 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 20:50:48 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:00:29 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:07:36 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:13:41 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2309.4519057273865 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 21:21:59 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 21:28:56 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 21:38:30 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 21:45:36 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 21:51:42 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2270.805173635483 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 21:59:25 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:06:05 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:15:18 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:22:01 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 22:27:53 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2158.798686027527 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 22:35:21 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 22:41:55 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 22:51:36 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 22:58:29 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:04:35 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2219.8557229042053 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 23:12:22 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:19:11 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Sun Mar 24 23:28:38 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Sun Mar 24 23:35:27 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Sun Mar 24 23:41:31 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2208.120080471039 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 23:49:15 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Sun Mar 24 23:56:25 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:06:27 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:13:28 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:19:45 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2302.1971027851105 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 00:27:40 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 00:34:44 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 00:44:53 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 00:52:04 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 00:58:18 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2307.3393223285675 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 01:06:21 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:13:27 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:23:00 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 01:29:43 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 01:35:29 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2217.8380081653595 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 01:42:46 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 01:49:22 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 01:58:27 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:05:00 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:10:49 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2123.6294593811035 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 02:18:18 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 02:24:57 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 02:34:01 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 02:40:34 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 02:46:32 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2155.8860120773315 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 02:54:17 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:01:07 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:10:24 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:17:14 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 03:23:08 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2190.872602701187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 03:30:47 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 03:37:47 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 03:47:14 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 03:54:02 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:00:01 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2216.7104198932648 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 04:07:48 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:14:45 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 04:24:46 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 04:32:11 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 04:38:30 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2313.8149445056915 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 04:46:41 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 04:53:53 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:03:54 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:11:22 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:17:50 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2358.3291852474213 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 05:25:51 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 05:33:03 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 05:43:17 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 05:50:33 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 05:56:16 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2288.974735021591 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Mon Mar 25 06:03:41 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:10:19 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 06:20:40 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 06:28:26 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 06:35:06 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2358.280059337616 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Mon Mar 25 06:43:36 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 06:51:19 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:01:26 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:08:43 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:14:41 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2374.137843608856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Mon Mar 25 07:22:51 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 07:30:08 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 07:40:12 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 07:47:12 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 07:53:37 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2333.9330661296844 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Mon Mar 25 08:02:04 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:09:25 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:19:40 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 08:26:52 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 08:33:15 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2360.9773499965668 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Mon Mar 25 08:40:45 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Mon Mar 25 08:47:18 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Mon Mar 25 08:56:40 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Mon Mar 25 09:04:33 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Mon Mar 25 09:10:28 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2223.0095331668854 seconds. 
Discarding model... 

Training complete taking 56615.2824678421 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 2.661052703857422 seconds. 
Saved predicted values as M_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (623.8500267913502,), 'R2_train': -2.022344005804376, 'MAE_train': 22.449042842560807, 'MSE_test': 528.832647555197, 'R2_test': -2.187704789016629, 'MAE_test': 20.860294891734206}. 
Saved model results as M_Full-CRX_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/circuits/Encoders.py:12: RuntimeWarning: invalid value encountered in arcsin
  if np.isnan(np.arcsin(features[feature_idx])) or np.isnan(np.arccos(features[feature_idx])):
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:38:56 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:43:12 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 11:50:05 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 11:59:36 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:06:52 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:13:06 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2254.200954914093 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:20:52 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 12:27:43 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 12:37:11 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 12:44:12 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 12:50:34 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2261.6738488674164 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:58:28 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:05:25 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:14:47 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:21:46 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 13:27:52 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2228.8915309906006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:35:38 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 13:42:47 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:39 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 13:59:42 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:06:00 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2280.5200724601746 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:13:40 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:20:30 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 14:29:51 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 14:36:40 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 14:42:34 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2195.881556034088 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:50:11 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 14:56:59 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:06:15 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:12:58 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:18:52 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2179.0362226963043 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 15:26:36 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 15:33:30 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 15:42:59 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 15:49:59 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 15:55:59 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2225.2050075531006 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 16:03:37 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:10:21 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:19:43 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 16:26:36 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 16:32:34 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2198.8189702033997 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 16:40:21 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 16:47:14 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 16:56:37 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:03:18 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:09:17 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2201.326009273529 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 17:17:08 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 17:23:56 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 17:33:47 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 17:40:47 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 17:47:15 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2291.1099886894226 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 17:55:27 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:02:34 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:12:11 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:19:05 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 18:25:02 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2251.1135799884796 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 18:32:34 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 18:39:26 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 18:48:51 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 18:55:39 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:01:42 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2206.814402103424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 19:09:26 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:16:15 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 19:26:04 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 19:32:51 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 19:38:47 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2217.7592232227325 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 19:46:24 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 19:53:19 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:02:37 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:09:45 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:15:53 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2248.9367311000824 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 20:24:14 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 20:31:43 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 20:41:26 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 20:48:29 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 20:54:43 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2321.990867137909 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 21:02:52 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:10:32 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 21:20:36 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 21:28:03 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 21:34:24 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2379.8522186279297 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 21:42:46 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 21:50:21 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:00:52 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:08:10 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:14:50 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2437.2117652893066 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 22:23:24 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 22:30:52 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 22:41:30 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 22:48:16 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 22:54:09 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2336.611679315567 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 23:01:48 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:08:34 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:17:56 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Thu Apr  4 23:24:43 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Thu Apr  4 23:30:40 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2193.617823600769 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 23:38:17 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Thu Apr  4 23:45:03 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Thu Apr  4 23:54:38 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:01:32 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:07:31 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2209.284470796585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Fri Apr  5 00:15:09 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:22:02 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 00:31:35 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 00:38:34 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 00:44:34 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2218.653727054596 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Fri Apr  5 00:52:09 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 00:59:36 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:10:14 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:17:33 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 01:23:55 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2392.4571380615234 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Fri Apr  5 01:32:15 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 01:39:36 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 01:49:27 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 01:56:51 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:03:09 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2344.997261285782 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Fri Apr  5 02:11:27 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:19:06 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 02:29:19 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 02:36:27 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 02:43:13 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2417.1068708896637 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Fri Apr  5 02:51:41 2024]  Iteration number: 0 with current cost as 0.07976173868448619 and parameters 
[-4.85400887  2.23743452 -2.12427958 -0.11653109  0.55388708 -2.77010909
  3.06858486  2.18960145  1.18551998 -1.0664832   0.6027151   1.14432439
  1.31029893 -1.87354686  0.72965069  2.88578419 -0.54534335 -0.47522485
 -2.02654246  0.7289737   1.60512664  2.83077095 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.4 fold... 
[Fri Apr  5 02:59:25 2024]  Iteration number: 0 with current cost as 0.9516194551563306 and parameters 
[-7.9528092   2.23743432 -2.12427964 -0.11653103  0.55388708 -2.7701096
  3.06858436  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077044 -1.26456741 -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.6 fold... 
[Fri Apr  5 03:09:43 2024]  Iteration number: 0 with current cost as 0.9892562590268811 and parameters 
[-7.91864412  2.23743464 -2.12427932 -0.11653103  0.55388739 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.7296508   2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077107 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 0.8 fold... 
[Fri Apr  5 03:17:20 2024]  Iteration number: 0 with current cost as 1.0342591347049663 and parameters 
[-8.28308243  2.23743464 -2.12427932 -0.11653103  0.55388708 -2.77010929
  3.06858467  2.18960145  1.18551998 -1.06648371  0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Working on 1.0 fold... 
[Fri Apr  5 03:23:41 2024]  Iteration number: 0 with current cost as 0.9832558896945491 and parameters 
[-7.72371319  2.23743464 -2.12427901 -0.11653103  0.55388708 -2.77010929
  3.06858498  2.18960145  1.18551998 -1.0664834   0.6027151   1.14432445
  1.31029899 -1.8735468   0.72965049  2.88578419 -0.54534335 -0.47522485
 -2.0265424   0.7289737   1.60512664  2.83077076 -1.2645671  -0.25136105
 -2.39279218 -2.27309774  3.13337155  2.54856958 -0.67550787 -2.69002202]. 
Training complete taking 2415.2268540859222 seconds. 
Discarding model... 

Training complete taking 56908.29955792427 total seconds. 
Now scoring model... 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Ignoring NaN found at index: 0. With feature: 1.020408163265306. 
Scoring complete taking 3.1891472339630127 seconds. 
Saved predicted values as M_Full-CRX_predicted_values.csv
Model scores: {'MSE_train': (623.8500267913502,), 'R2_train': -2.022344005804376, 'MAE_train': 22.449042842560807, 'MSE_test': 528.832647555197, 'R2_test': -2.187704789016629, 'MAE_test': 20.860294891734206}. 
Saved model results as M_Full-CRX_results.json. 
