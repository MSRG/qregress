/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Sun Mar 24 17:30:08 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 17:30:55 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:32:34 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 17:33:53 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 17:35:24 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:36:55 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 450.7716746330261 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 17:38:21 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:39:57 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 17:41:13 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 17:42:41 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:44:07 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 434.98140048980713 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 17:45:38 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:47:23 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 17:48:44 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 17:50:13 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:51:46 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 457.59196400642395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 17:53:15 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 17:54:52 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 17:56:12 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 17:57:40 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 17:59:12 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 446.4968523979187 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:00:43 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:02:28 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:03:51 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:05:23 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:06:55 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 463.66574907302856 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:08:34 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:10:17 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:11:36 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:13:10 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:14:40 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 464.96973514556885 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:16:11 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:17:54 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:19:11 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:20:41 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:22:09 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 446.2020902633667 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 18:23:38 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:25:20 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:26:41 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:28:13 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:29:52 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 468.3162899017334 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 18:31:27 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:33:09 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:34:29 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:36:01 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:37:34 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 458.6477143764496 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 18:39:06 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:40:52 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:42:13 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:43:44 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:45:15 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 463.19157314300537 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 18:46:46 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:48:31 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:49:49 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:51:20 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 18:52:48 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 454.0159378051758 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 18:54:22 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 18:56:06 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 18:57:26 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 18:58:58 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:00:34 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 461.4017689228058 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:02:06 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:03:45 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:05:06 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:06:35 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:08:14 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 467.86354780197144 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:09:59 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:11:57 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:13:19 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:14:48 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:16:23 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 483.6757218837738 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:17:56 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:19:40 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:20:57 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:22:31 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:24:04 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 462.99370074272156 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 19:25:35 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:27:16 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:28:39 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:30:12 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:31:46 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 460.8896107673645 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 19:33:16 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:34:58 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:36:23 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:38:02 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:39:34 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 466.08294200897217 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 19:41:06 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:42:44 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:44:00 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:45:32 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:46:59 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 441.9446506500244 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 19:48:26 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:50:06 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:51:25 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 19:52:51 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 19:54:27 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 449.2960798740387 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 19:55:54 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 19:57:32 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 19:58:46 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 20:00:11 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:01:38 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 429.1734812259674 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Sun Mar 24 20:03:03 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:04:40 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 20:05:58 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 20:07:28 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:08:55 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 437.5498080253601 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Sun Mar 24 20:10:19 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:11:58 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 20:13:17 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 20:14:44 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:16:15 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 443.73309421539307 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Sun Mar 24 20:17:43 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:19:21 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 20:20:35 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 20:22:06 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:23:32 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 432.8186385631561 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Sun Mar 24 20:24:57 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:26:36 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 20:27:50 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 20:29:18 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:30:45 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 435.81610918045044 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Sun Mar 24 20:32:16 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Sun Mar 24 20:33:55 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Sun Mar 24 20:35:15 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Sun Mar 24 20:36:45 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Sun Mar 24 20:38:13 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 451.7488203048706 seconds. 
Discarding model... 

Training complete taking 11333.83991408348 total seconds. 
Now scoring model... 
Scoring complete taking 1.783731460571289 seconds. 
Saved predicted values as A2_ESU2_predicted_values.csv
Model scores: {'MSE_train': (27.708903467706307,), 'R2_train': 0.8657596622480495, 'MAE_train': 3.714968878299922, 'MSE_test': 29.353778752859398, 'R2_test': 0.8230608841212702, 'MAE_test': 3.5003653005580384}. 
Saved model results as A2_ESU2_results.json. 
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
/scratch/j/jacobsen/gjones/lin5qubits/quantum/Quantum.py:302: OptimizeWarning: Unknown solver options: tol
  opt_result = minimize(self._cost_wrapper, x0=params, method=self.optimizer, callback=self._callback,
Loading dataset from /linear_train.bin... 
Successfully loaded /linear_train.bin into X and y data. 
Loading dataset from /linear_test.bin... 
Successfully loaded /linear_test.bin into X and y data. 
Training model with dataset /linear_train.bin 
 at time Thu Apr  4 11:36:38 2024... 
Training using 5-fold cross-validation. 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 11:37:23 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 11:39:02 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 11:40:17 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 11:41:44 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:43:12 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 436.90186285972595 seconds. 
Saving model as new best... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 11:44:37 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 11:46:15 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 11:47:33 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 11:49:00 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:50:32 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 441.42827582359314 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 11:52:00 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 11:53:36 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 11:54:56 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 11:56:30 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 11:57:58 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 446.7907979488373 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 11:59:26 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:01:03 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:02:19 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:03:50 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:05:17 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 436.67332220077515 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.001, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:06:42 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:08:20 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:09:35 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:11:04 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:12:29 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 433.8134310245514 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:13:55 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:15:32 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:16:50 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:18:16 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:19:43 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 432.9206602573395 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:21:09 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:22:52 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:24:12 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:25:39 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:27:06 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 445.5941698551178 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 12:28:34 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:30:13 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:31:29 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:32:55 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:34:21 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 431.1668975353241 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 12:35:48 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:37:26 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:38:40 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:40:09 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:41:35 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 438.10694193840027 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.01, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 12:43:03 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:44:41 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:45:57 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:47:23 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:48:53 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 435.93119525909424 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 12:50:22 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:52:02 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 12:53:19 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 12:54:47 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 12:56:16 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 443.82847142219543 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 12:57:47 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 12:59:25 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:00:41 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:02:10 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:03:39 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 441.88515067100525 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:05:06 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:06:45 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:08:02 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:09:32 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:11:03 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 447.54382491111755 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:12:35 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:14:14 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:15:30 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:17:01 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:18:29 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 443.58798360824585 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 0.1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:19:57 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:21:39 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:22:55 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:24:27 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:25:56 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 445.224892616272 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 13:27:26 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:29:06 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:30:25 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:31:54 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:33:24 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 447.1351113319397 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 13:34:49 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:36:29 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:37:45 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:39:12 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:40:40 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 440.2207860946655 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 13:42:11 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:43:50 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:45:06 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:46:33 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:47:59 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 435.16787791252136 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 13:49:25 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:51:00 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:52:14 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 13:53:40 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 13:55:05 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 430.47470927238464 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 1, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 13:56:35 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 13:58:11 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 13:59:26 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 14:00:51 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:02:16 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 428.32594203948975 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.001}...

Working on 0.2 fold... 
[Thu Apr  4 14:03:44 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:05:28 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 14:06:43 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 14:08:08 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:09:33 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 437.9382028579712 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.01}...

Working on 0.2 fold... 
[Thu Apr  4 14:11:02 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:12:38 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 14:13:53 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 14:15:18 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:16:44 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 430.0679020881653 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 0.1}...

Working on 0.2 fold... 
[Thu Apr  4 14:18:12 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:19:48 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 14:21:09 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 14:22:37 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:24:03 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 437.92454767227173 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 1}...

Working on 0.2 fold... 
[Thu Apr  4 14:25:32 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:27:10 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 14:28:28 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 14:29:58 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:31:26 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 441.32492661476135 seconds. 
Discarding model... 

Beginning training with hyperparameters {'alpha': 10, 'beta': 10}...

Working on 0.2 fold... 
[Thu Apr  4 14:32:52 2024]  Iteration number: 0 with current cost as 0.08254126135184216 and parameters 
[-1.8061569   2.23743466 -2.12427959 -0.11653101  0.55388708 -2.77010897
  3.06858498  2.18960148  1.18552    -1.06648308]. 
Working on 0.4 fold... 
[Thu Apr  4 14:34:31 2024]  Iteration number: 0 with current cost as 0.04565948306169125 and parameters 
[-1.63842078  2.23743464 -2.12427962 -0.11653101  0.55388708 -2.77010899
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.6 fold... 
[Thu Apr  4 14:35:46 2024]  Iteration number: 0 with current cost as 0.0636736913151231 and parameters 
[-1.75675894  2.23743464 -2.12427962 -0.11653103  0.55388708 -2.77010897
  3.06858496  2.18960145  1.18551998 -1.06648311]. 
Working on 0.8 fold... 
[Thu Apr  4 14:37:11 2024]  Iteration number: 0 with current cost as 0.06242059885217759 and parameters 
[-1.74666116  2.23743464 -2.12427961 -0.11653101  0.55388708 -2.77010897
  3.06858496  2.18960147  1.18552    -1.0664831 ]. 
Working on 1.0 fold... 
[Thu Apr  4 14:38:37 2024]  Iteration number: 0 with current cost as 0.06501695523109059 and parameters 
[-1.74143584  2.23743466 -2.12427958 -0.116531    0.55388709 -2.77010897
  3.06858498  2.18960148  1.18552001 -1.06648308]. 
Training complete taking 431.58872294425964 seconds. 
Discarding model... 

Training complete taking 10961.567975759506 total seconds. 
Now scoring model... 
Scoring complete taking 2.3415582180023193 seconds. 
Saved predicted values as A2_ESU2_predicted_values.csv
Model scores: {'MSE_train': (27.708903467706307,), 'R2_train': 0.8657596622480495, 'MAE_train': 3.714968878299922, 'MSE_test': 29.353778752859398, 'R2_test': 0.8230608841212702, 'MAE_test': 3.5003653005580384}. 
Saved model results as A2_ESU2_results.json. 
